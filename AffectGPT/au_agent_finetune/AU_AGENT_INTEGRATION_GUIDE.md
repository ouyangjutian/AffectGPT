# AU Agenté›†æˆä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

å°†AU Agenté›†æˆåˆ°AffectGPTï¼Œå®ç°ç«¯åˆ°ç«¯çš„AUæ¨¡æ€å¤„ç†ï¼š
```
è§†é¢‘ â†’ OpenFace AUæ£€æµ‹ â†’ AU Agent (Qwen2.5 + LoRA) â†’ AUæè¿° â†’ CLIPç¼–ç  â†’ AUç‰¹å¾
```

---

## ğŸ¯ å®Œæ•´æµç¨‹

### **æ­¥éª¤1: å‡†å¤‡AUæŒ‡ä»¤æ•°æ®é›†**

ä½¿ç”¨MER-Factoryå·²ç”Ÿæˆçš„AUæè¿°æ•°æ®ï¼š

```bash
cd /home/project/AffectGPT/AffectGPT

# ä»MER-Factoryè¾“å‡ºæ„å»ºæŒ‡ä»¤æ•°æ®é›†
python prepare_au_instruction_dataset.py
```

**ç”Ÿæˆæ–‡ä»¶**ï¼š
- `au_instruction_dataset.json` - å®Œæ•´æ•°æ®é›†
- `au_instruction_dataset.jsonl` - LLaMA-Factoryæ ¼å¼

**æ•°æ®æ ¼å¼ç¤ºä¾‹**ï¼š
```json
{
  "instruction": "Based on the following Action Unit detections, describe the facial expression:",
  "input": "AU01: 0.98, AU05: 0.98, AU07: 2.35, AU25: 1.76",
  "output": "The facial expression exhibits subtle brow lowering, neutral ocular engagement with mild lid tightening, and slight lip parting, consistent with a prototypical neutral state."
}
```

---

### **æ­¥éª¤2: å¾®è°ƒAU Agent**

ä½¿ç”¨LLaMA-Factoryå¾®è°ƒQwen2.5-7Bï¼š

```bash
# å®‰è£…LLaMA-Factoryï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e .

# è¿”å›AffectGPTç›®å½•
cd /home/project/AffectGPT/AffectGPT

# å¼€å§‹è®­ç»ƒ
bash train_au_agent.sh
```

**è®­ç»ƒå‚æ•°**ï¼š
- **åŸºç¡€æ¨¡å‹**: Qwen2.5-7B-Instruct
- **æ–¹æ³•**: LoRA (rank=64, alpha=128)
- **Epochs**: 3
- **Batch Size**: 4 Ã— 4 (gradient accumulation)
- **Learning Rate**: 5e-5
- **é¢„è®¡æ—¶é—´**: 8-12å°æ—¶ï¼ˆå•GPUï¼Œ100Kæ ·æœ¬ï¼‰

**è¾“å‡º**ï¼š
```
./output/au_agent_qwen2.5_7b_lora/
â”œâ”€â”€ checkpoint-500/
â”œâ”€â”€ checkpoint-1000/
â”œâ”€â”€ checkpoint-best/  â† ä½¿ç”¨è¿™ä¸ª
â””â”€â”€ ...
```

---

### **æ­¥éª¤3: æµ‹è¯•AU Agent**

éªŒè¯å¾®è°ƒæ•ˆæœï¼š

```bash
python test_au_agent.py
```

**æµ‹è¯•è¾“å‡ºç¤ºä¾‹**ï¼š
```
Test Case 1: Neutral Expression
AU Values: {'AU01': 0.98, 'AU05': 0.98, 'AU07': 2.35, 'AU25': 1.76'}

Generated Description:
  The facial expression exhibits subtle brow lowering, neutral ocular 
  engagement with mild lid tightening, and slight lip parting, 
  consistent with a prototypical neutral state.
```

---

### **æ­¥éª¤4: ä¿®æ”¹base_dataset.pyé›†æˆAU Agent**

åœ¨`base_dataset.py`çš„`__init__`ä¸­æ·»åŠ AU Agentåˆå§‹åŒ–ï¼š

```python
# my_affectgpt/datasets/datasets/base_dataset.py

from my_affectgpt.models.au_agent import create_au_agent

class BaseDataset():
    def __init__(self, ..., model_cfg=None, dataset_cfg=None, ...):
        # ... ç°æœ‰ä»£ç  ...
        
        # AU Agenté…ç½®
        self.use_au_agent = getattr(dataset_cfg, 'use_au_agent', False)
        if self.use_au_agent:
            self.au_agent = create_au_agent(dataset_cfg)
            print(f"[Dataset] AU Agent enabled")
        else:
            self.au_agent = None
```

ä¿®æ”¹`_extract_au_features_realtime`ä½¿ç”¨AU Agentï¼š

```python
def _extract_au_features_realtime(self, video_name):
    """å®æ—¶ä»OpenFaceæ£€æµ‹ + AU Agentç”Ÿæˆæè¿° + CLIPç¼–ç """
    
    if self.use_au_agent:
        # æ–¹æ¡ˆA: ä½¿ç”¨AU Agentç”Ÿæˆæè¿°ï¼ˆæ–°æ–¹æ¡ˆï¼‰
        return self._extract_au_with_agent(video_name)
    else:
        # æ–¹æ¡ˆB: è¯»å–MER-Factoryé¢„ç”Ÿæˆçš„æè¿°ï¼ˆåŸæ–¹æ¡ˆï¼‰
        return self._extract_au_from_json(video_name)

def _extract_au_with_agent(self, video_name):
    """ä½¿ç”¨AU Agentå®æ—¶ç”ŸæˆAUæè¿°"""
    import pandas as pd
    
    # 1. è¯»å–OpenFace CSV
    openface_csv = os.path.join(self.openface_output_dir, f"{video_name}.csv")
    if not os.path.exists(openface_csv):
        print(f"âš ï¸ OpenFace output not found: {openface_csv}")
        return None
    
    df = pd.read_csv(openface_csv)
    
    # 2. ä¸ºæ¯ä¸€å¸§ç”ŸæˆAUæè¿°
    descriptions = []
    for idx, row in df.iterrows():
        au_values = self.au_agent.parse_openface_csv(row.to_dict())
        description = self.au_agent.generate_description(au_values)
        descriptions.append(description)
    
    # 3. ä½¿ç”¨CLIPç¼–ç æè¿°
    clip_model = self._load_clip_for_au()
    if clip_model is None:
        return None
    
    import clip
    device = next(clip_model.parameters()).device
    text_tokens = clip.tokenize(descriptions, truncate=True).to(device)
    
    with torch.no_grad():
        text_features = clip_model.encode_text(text_tokens)
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
    
    return text_features
```

---

### **æ­¥éª¤5: é…ç½®è®­ç»ƒä½¿ç”¨AU Agent**

ä¿®æ”¹è®­ç»ƒé…ç½®æ–‡ä»¶ï¼š

```yaml
# train_configs/config_with_au_agent.yaml

datasets:
  mercaptionplus:
    # ... å…¶ä»–é…ç½® ...
    
    # AU Agenté…ç½®
    use_au_agent: true  # å¯ç”¨AU Agent
    au_agent_base_model: /home/project/Dataset/Emotion/tools/transformer/LLM/Qwen2.5-7B-Instruct
    au_agent_lora_weights: /home/project/AffectGPT/AffectGPT/output/au_agent_qwen2.5_7b_lora/checkpoint-best
    au_agent_use_lora: true
    
    # OpenFaceè¾“å‡ºè·¯å¾„ï¼ˆå¦‚æœä½¿ç”¨AU Agentå®æ—¶ç”Ÿæˆï¼‰
    openface_output_dir: /home/project/openface_outputs
```

---

### **æ­¥éª¤6: è®­ç»ƒAffectGPT with AU Agent**

```bash
# è®­ç»ƒ
python train.py --cfg-path train_configs/config_with_au_agent.yaml
```

**è®­ç»ƒæµç¨‹**ï¼š
```
è§†é¢‘ â†’ OpenFaceæ£€æµ‹AU
         â†“
    AU Agentç”Ÿæˆæè¿°
         â†“
    CLIPç¼–ç ä¸ºç‰¹å¾ [T, 512]
         â†“
    Q-Formerå¤„ç†
         â†“
    æŠ•å½±åˆ°LLMç©ºé—´
         â†“
    ä¸å…¶ä»–æ¨¡æ€èåˆ
         â†“
    LLMç”Ÿæˆå›å¤
```

---

### **æ­¥éª¤7: æ¨ç†ä½¿ç”¨AU Agent**

ä¿®æ”¹`inference_hybird.py`ï¼š

```python
# æ¨ç†é…ç½®
if use_au:
    dataset_cls.use_au_agent = True
    dataset_cls.au_agent_base_model = "/home/project/Dataset/Emotion/tools/transformer/LLM/Qwen2.5-7B-Instruct"
    dataset_cls.au_agent_lora_weights = "./output/au_agent_qwen2.5_7b_lora/checkpoint-best"
    dataset_cls.openface_output_dir = "/home/project/openface_outputs"
```

è¿è¡Œæ¨ç†ï¼š

```bash
python inference_hybird.py --cfg-path inference_config_au_agent.yaml
```

---

## ğŸ“Š æ–¹æ¡ˆå¯¹æ¯”

### **åŸæ–¹æ¡ˆ vs AU Agentæ–¹æ¡ˆ**

| æ–¹æ¡ˆ | AUæ£€æµ‹ | AUæè¿°ç”Ÿæˆ | CLIPç¼–ç  | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|--------|-----------|---------|------|------|
| **åŸæ–¹æ¡ˆï¼ˆMER-Factoryï¼‰** | OpenFace | GPT-4o/Gemini API | âœ… | æè¿°è´¨é‡é«˜ | éœ€è¦APIè´¹ç”¨ |
| **AU Agentæ–¹æ¡ˆï¼ˆæ–°ï¼‰** | OpenFace | Qwen2.5 + LoRA | âœ… | å®Œå…¨å…è´¹ï¼Œå¯å®šåˆ¶ | éœ€è¦å¾®è°ƒ |

---

## ğŸ¯ ä¼˜åŠ¿

### **1. å®Œå…¨å…è´¹**
- âœ… æ— éœ€GPT-4o APIè´¹ç”¨ï¼ˆèŠ‚çœ$768ï¼‰
- âœ… æ— éœ€Gemini APIè´¹ç”¨ï¼ˆèŠ‚çœ$23ï¼‰
- âœ… æœ¬åœ°æ¨ç†ï¼Œæ— ç½‘ç»œé™åˆ¶

### **2. å¯å®šåˆ¶**
- âœ… é’ˆå¯¹æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡å¾®è°ƒ
- âœ… å¯ä»¥æ·»åŠ é¢†åŸŸçŸ¥è¯†
- âœ… å¯ä»¥è°ƒæ•´æè¿°é£æ ¼

### **3. æ€§èƒ½**
- âœ… Qwen2.5-7Bæ€§èƒ½æ¥è¿‘GPT-4o
- âœ… LoRAå¾®è°ƒåæ›´é€‚åˆAUä»»åŠ¡
- âœ… æ¨ç†é€Ÿåº¦å¿«ï¼ˆæœ¬åœ°GPUï¼‰

### **4. æ•°æ®éšç§**
- âœ… æ•°æ®ä¸ç¦»å¼€æœ¬åœ°
- âœ… é€‚åˆæ•æ„Ÿæ•°æ®

---

## â±ï¸ æ—¶é—´æˆæœ¬

| é˜¶æ®µ | æ—¶é—´ | è¯´æ˜ |
|------|------|------|
| æ•°æ®å‡†å¤‡ | 1-2å°æ—¶ | ä»MER-Factoryæå–100Kæ ·æœ¬ |
| AU Agentå¾®è°ƒ | 8-12å°æ—¶ | å•GPUï¼ŒQwen2.5-7B + LoRA |
| æµ‹è¯•éªŒè¯ | 30åˆ†é’Ÿ | æµ‹è¯•ç”Ÿæˆè´¨é‡ |
| é›†æˆä»£ç  | 2-3å°æ—¶ | ä¿®æ”¹base_dataset.pyç­‰ |
| **æ€»è®¡** | **~15å°æ—¶** | ä¸€æ¬¡æ€§å·¥ä½œ |

---

## ğŸ’¾ æ˜¾å­˜éœ€æ±‚

| æ“ä½œ | æ˜¾å­˜ | é…ç½® |
|------|------|------|
| **è®­ç»ƒAU Agent** | 24GB | LoRA, bf16, gradient checkpointing |
| **æ¨ç†AU Agent** | 8GB | ä»…æ¨ç†ï¼Œbf16 |
| **AffectGPTè®­ç»ƒï¼ˆå«AU Agentï¼‰** | 40GB | å»ºè®®A100 |

**ä¼˜åŒ–**ï¼š
- ä½¿ç”¨int8é‡åŒ–ï¼šæ˜¾å­˜å‡åŠ
- ä½¿ç”¨DeepSpeed ZeROï¼šåˆ†å¸ƒå¼è®­ç»ƒ

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### **é—®é¢˜1: AU Agentç”Ÿæˆè´¨é‡å·®**

**åŸå› **ï¼šå¾®è°ƒä¸å……åˆ†

**è§£å†³**ï¼š
```bash
# å¢åŠ è®­ç»ƒè½®æ•°
num_train_epochs: 5

# æˆ–å¢åŠ æ•°æ®é‡
max_samples: 200000
```

---

### **é—®é¢˜2: æ˜¾å­˜ä¸è¶³**

**è§£å†³**ï¼š
```bash
# å‡å°batch size
per_device_train_batch_size: 2

# æˆ–ä½¿ç”¨int8é‡åŒ–
load_in_8bit: true
```

---

### **é—®é¢˜3: æ¨ç†é€Ÿåº¦æ…¢**

**è§£å†³**ï¼š
```python
# 1. æ‰¹é‡ç”Ÿæˆ
batch_size = 16
descriptions = au_agent.batch_generate_descriptions(au_values_list, batch_size)

# 2. ä½¿ç”¨vLLMåŠ é€Ÿ
from vllm import LLM
llm = LLM(model=au_agent_path)
```

---

## ğŸ“ é…ç½®ç¤ºä¾‹

### **å®Œæ•´è®­ç»ƒé…ç½®**

```yaml
# train_configs/emercoarse_au_agent.yaml

model:
  face_or_frame: multiface_audio_face_frame_au_text
  # ... å…¶ä»–æ¨¡å‹é…ç½® ...

datasets:
  mercaptionplus:
    face_or_frame: multiface_audio_face_frame_au_text
    
    # AU Agenté…ç½®
    use_au_agent: true
    au_agent_base_model: /home/project/Dataset/Emotion/tools/transformer/LLM/Qwen2.5-7B-Instruct
    au_agent_lora_weights: ./output/au_agent_qwen2.5_7b_lora/checkpoint-best
    au_agent_use_lora: true
    openface_output_dir: /home/project/openface_outputs
    
    # æˆ–è€…ä½¿ç”¨MER-Factoryé¢„ç”Ÿæˆçš„æè¿°ï¼ˆåŸæ–¹æ¡ˆï¼‰
    # use_au_agent: false
    # mer_factory_output: /home/project/MER-Factory/output
```

---

## ğŸ‰ æ€»ç»“

**AU Agentæ–¹æ¡ˆä¼˜åŠ¿**ï¼š
1. âœ… **å®Œå…¨å…è´¹**ï¼ˆæ— APIè´¹ç”¨ï¼‰
2. âœ… **å¯å®šåˆ¶**ï¼ˆé’ˆå¯¹ä»»åŠ¡å¾®è°ƒï¼‰
3. âœ… **é«˜æ€§èƒ½**ï¼ˆQwen2.5-7B + LoRAï¼‰
4. âœ… **ç«¯åˆ°ç«¯**ï¼ˆä¸AffectGPTå®Œç¾é›†æˆï¼‰

**é€‚ç”¨åœºæ™¯**ï¼š
- éœ€è¦å¤§è§„æ¨¡AUå¤„ç†ï¼ˆAPIè´¹ç”¨å¤ªé«˜ï¼‰
- éœ€è¦å®šåˆ¶AUæè¿°é£æ ¼
- æ•°æ®éšç§æ•æ„Ÿ
- ç¦»çº¿ç¯å¢ƒ

ç°åœ¨ä½ å¯ä»¥åƒè®ºæ–‡ä¸€æ ·ï¼Œä½¿ç”¨AU Agentå®ç°å®Œæ•´çš„AUæ¨¡æ€å¤„ç†ï¼ğŸŠ
