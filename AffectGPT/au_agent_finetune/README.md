# AU Agent å¾®è°ƒå·¥å…·åŒ…

æœ¬æ–‡ä»¶å¤¹åŒ…å«AU Agentçš„å®Œæ•´å¾®è°ƒæµç¨‹ï¼Œç”¨äºè®­ç»ƒQwen2.5-7B-Instructæ¨¡å‹ï¼Œå®ç°ä»AUæ£€æµ‹å€¼åˆ°è‡ªç„¶è¯­è¨€æè¿°çš„è½¬æ¢ã€‚

---

## ğŸ“ æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | ç”¨é€” |
|------|------|
| **prepare_au_instruction_dataset.py** | å‡†å¤‡è®­ç»ƒæ•°æ®é›†ï¼ˆä»MER-Factoryæå–ï¼‰ |
| **train_au_agent.sh** | AU Agentå¾®è°ƒè„šæœ¬ï¼ˆä½¿ç”¨LLaMA-Factoryï¼‰ |
| **test_au_agent.py** | æµ‹è¯•AU Agentç”Ÿæˆè´¨é‡ |
| **au_agent_lora_config.yaml** | LoRAå¾®è°ƒé…ç½®æ–‡ä»¶ |
| **setup_au_agent.sh** | ä¸€é”®è®¾ç½®è„šæœ¬ï¼ˆè‡ªåŠ¨åŒ–å…¨æµç¨‹ï¼‰ |
| **AU_AGENT_INTEGRATION_GUIDE.md** | è¯¦ç»†é›†æˆæŒ‡å— |
| **AU_AGENT_SUMMARY.md** | æ–¹æ¡ˆæ€»ç»“ |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### **æ–¹å¼1: ä¸€é”®å®Œæˆï¼ˆæ¨èï¼‰**

```bash
cd /home/project/AffectGPT/AffectGPT/au_agent_finetune

# æ‰§è¡Œä¸€é”®è®¾ç½®ï¼ˆæ•°æ®å‡†å¤‡ + è®­ç»ƒ + æµ‹è¯•ï¼‰
bash setup_au_agent.sh
```

---

### **æ–¹å¼2: åˆ†æ­¥æ‰§è¡Œ**

#### **æ­¥éª¤1: å‡†å¤‡æ•°æ®é›†**

```bash
cd /home/project/AffectGPT/AffectGPT/au_agent_finetune

python prepare_au_instruction_dataset.py
```

**è¾“å‡º**ï¼š
- `au_instruction_dataset.json` - å®Œæ•´æ•°æ®é›†
- `au_instruction_dataset.jsonl` - LLaMA-Factoryæ ¼å¼

---

#### **æ­¥éª¤2: å¾®è°ƒAU Agent**

```bash
bash train_au_agent.sh
```

**è®­ç»ƒå‚æ•°**ï¼š
- åŸºç¡€æ¨¡å‹ï¼šQwen2.5-7B-Instruct
- æ–¹æ³•ï¼šLoRA (rank=64, alpha=128)
- è®­ç»ƒè½®æ•°ï¼š3 epochs
- é¢„è®¡æ—¶é—´ï¼š8-12å°æ—¶ï¼ˆå•GPUï¼‰

**è¾“å‡º**ï¼š
```
../output/au_agent_qwen2.5_7b_lora/
â”œâ”€â”€ checkpoint-500/
â”œâ”€â”€ checkpoint-1000/
â”œâ”€â”€ checkpoint-best/  â† ä½¿ç”¨è¿™ä¸ª
â””â”€â”€ ...
```

---

#### **æ­¥éª¤3: æµ‹è¯•AU Agent**

```bash
python test_au_agent.py
```

**åŠŸèƒ½**ï¼šéªŒè¯ç”Ÿæˆè´¨é‡ï¼Œæµ‹è¯•ä¸åŒAUç»„åˆ

---

## ğŸ“Š ä½¿ç”¨åœºæ™¯

### **åœºæ™¯1: è®­ç»ƒæ—¶å®æ—¶ç”ŸæˆAUæè¿°ï¼ˆæ¨èï¼‰**

åœ¨è®­ç»ƒé…ç½®ä¸­å¯ç”¨AU Agentï¼š

```yaml
# train_configs/config_with_au_agent.yaml

datasets:
  mercaptionplus:
    face_or_frame: multiface_audio_face_frame_au_text
    
    # å¯ç”¨AU Agent
    use_au_agent: true
    au_agent_base_model: /home/project/Dataset/Emotion/tools/transformer/LLM/Qwen2.5-7B-Instruct
    au_agent_lora_weights: ../output/au_agent_qwen2.5_7b_lora/checkpoint-best
    openface_output_dir: /home/project/openface_outputs
```

**è®­ç»ƒæµç¨‹**ï¼š
```
è§†é¢‘ â†’ OpenFace â†’ AU Agent â†’ CLIP â†’ AffectGPTè®­ç»ƒ
```

---

### **åœºæ™¯2: æ¨ç†æ—¶å®æ—¶ç”ŸæˆAUæè¿°**

åœ¨æ¨ç†é…ç½®ä¸­å¯ç”¨AU Agentï¼š

```yaml
# inference_config_au_agent.yaml

inference:
  use_au_agent: true
  au_agent_base_model: /home/project/Dataset/Emotion/tools/transformer/LLM/Qwen2.5-7B-Instruct
  au_agent_lora_weights: ../output/au_agent_qwen2.5_7b_lora/checkpoint-best
  openface_output_dir: /home/project/openface_outputs
```

**æ¨ç†æµç¨‹**ï¼š
```
æµ‹è¯•è§†é¢‘ â†’ OpenFace â†’ AU Agent â†’ CLIP â†’ AffectGPTæ¨ç†
```

---

## ğŸ¯ ä¸è®ºæ–‡å¯¹æ¯”

| é¡¹ç›® | è®ºæ–‡EmoChat | æœ¬å®ç° |
|------|------------|--------|
| åŸºç¡€æ¨¡å‹ | LLaMA-3.2-1B | **Qwen2.5-7B** âœ… |
| å¾®è°ƒæ–¹æ³• | LoRA | LoRA âœ… |
| æ•°æ®æ¥æº | GPT-4oç”Ÿæˆ | MER-Factory/æœ¬åœ° âœ… |
| æˆæœ¬ | APIè´¹ç”¨ | **å®Œå…¨å…è´¹** âœ… |
| é›†æˆåº¦ | ç‹¬ç«‹æ¨¡å— | **AffectGPTé›†æˆ** âœ… |

---

## ğŸ’° æˆæœ¬å¯¹æ¯”

| æ–¹æ¡ˆ | è®¾ç½®æˆæœ¬ | æ¨ç†æˆæœ¬ï¼ˆ64Kè§†é¢‘ï¼‰ |
|------|---------|-------------------|
| GPT-4o API | $0 | $768 |
| Gemini API | $0 | $23 |
| **AU Agent** | **15å°æ—¶** | **$0** |

---

## ğŸ“– è¯¦ç»†æ–‡æ¡£

- **é›†æˆæŒ‡å—**ï¼šæŸ¥çœ‹ `AU_AGENT_INTEGRATION_GUIDE.md`
- **æ–¹æ¡ˆæ€»ç»“**ï¼šæŸ¥çœ‹ `AU_AGENT_SUMMARY.md`
- **é…ç½®æ–‡ä»¶**ï¼šå‚è€ƒ `au_agent_lora_config.yaml`

---

## âš™ï¸ é…ç½®è¯´æ˜

### **å…³é”®è·¯å¾„é…ç½®**

ç¼–è¾‘ `prepare_au_instruction_dataset.py`ï¼š

```python
# ç¬¬180-182è¡Œ
MER_FACTORY_OUTPUT = '/home/project/MER-Factory/output'  # â† ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„
OUTPUT_JSON = './au_instruction_dataset.json'
OUTPUT_JSONL = './au_instruction_dataset.jsonl'
```

ç¼–è¾‘ `train_au_agent.sh`ï¼š

```bash
# ç¬¬6-8è¡Œ
BASE_MODEL="/home/project/Dataset/Emotion/tools/transformer/LLM/Qwen2.5-7B-Instruct"  # â† ä¿®æ”¹
DATASET_PATH="./au_instruction_dataset.jsonl"
OUTPUT_DIR="../output/au_agent_qwen2.5_7b_lora"  # â† ä¿®æ”¹
```

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### **é—®é¢˜1: æ‰¾ä¸åˆ°MER-Factoryè¾“å‡º**

**é”™è¯¯**ï¼š`MER-Factory output not found`

**è§£å†³**ï¼š
```bash
# æ£€æŸ¥è·¯å¾„
ls /home/project/MER-Factory/output

# ä¿®æ”¹é…ç½®
vim prepare_au_instruction_dataset.py  # æ›´æ–° MER_FACTORY_OUTPUT
```

---

### **é—®é¢˜2: æ˜¾å­˜ä¸è¶³**

**é”™è¯¯**ï¼š`CUDA out of memory`

**è§£å†³**ï¼š
```yaml
# ç¼–è¾‘ au_agent_lora_config.yaml
per_device_train_batch_size: 2  # ä»4å‡åˆ°2
gradient_accumulation_steps: 8  # ä»4å¢åˆ°8
```

---

### **é—®é¢˜3: LLaMA-Factoryæœªå®‰è£…**

**é”™è¯¯**ï¼š`LLaMA-Factory not found`

**è§£å†³**ï¼š
```bash
git clone https://github.com/hiyouga/LLaMA-Factory.git /home/project/LLaMA-Factory
cd /home/project/LLaMA-Factory
pip install -e .
```

---

## ğŸ“Š é¢„æœŸè¾“å‡º

### **æ•°æ®é›†ç»Ÿè®¡**

```
Total samples collected: 100,000+
Train: 95,000
Val: 5,000
```

### **è®­ç»ƒæ—¥å¿—**

```
Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [2:30:00<00:00]
Loss: 0.45
Eval Loss: 0.32
```

### **æµ‹è¯•ç»“æœ**

```
Test Case 1: Neutral Expression
AU Values: {'AU01': 0.98, 'AU05': 0.98, 'AU07': 2.35}
Generated: "The facial expression exhibits subtle brow lowering..."
```

---

## ğŸ‰ ä½¿ç”¨æµç¨‹

```bash
# 1. è¿›å…¥å¾®è°ƒæ–‡ä»¶å¤¹
cd /home/project/AffectGPT/AffectGPT/au_agent_finetune

# 2. ä¸€é”®è®¾ç½®
bash setup_au_agent.sh

# 3. è¿”å›AffectGPTæ ¹ç›®å½•
cd ..

# 4. é…ç½®è®­ç»ƒ
vim train_configs/config_with_au_agent.yaml

# 5. å¼€å§‹è®­ç»ƒ
python train.py --cfg-path train_configs/config_with_au_agent.yaml
```

---

## ğŸ“ æ”¯æŒ

é‡åˆ°é—®é¢˜ï¼Ÿ
1. æŸ¥çœ‹ `AU_AGENT_INTEGRATION_GUIDE.md`
2. æ£€æŸ¥è·¯å¾„é…ç½®
3. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼š`../output/au_agent_qwen2.5_7b_lora/`

---

## ğŸ“ ç‰ˆæœ¬ä¿¡æ¯

- **åˆ›å»ºæ—¥æœŸ**ï¼š2025-11-17
- **åŸºç¡€æ¨¡å‹**ï¼šQwen2.5-7B-Instruct
- **å¾®è°ƒæ–¹æ³•**ï¼šLoRA (rank=64, alpha=128)
- **å‚è€ƒè®ºæ–‡**ï¼šEmoChat (AAAI 2025)

---

ç¥å¾®è°ƒé¡ºåˆ©ï¼ğŸš€
