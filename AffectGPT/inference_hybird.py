import os
import time
import glob
import argparse
import numpy as np
import pandas as pd
from datetime import datetime
from pathlib import Path

import torch
import torch.backends.cudnn as cudnn

import decord
decord.bridge.set_bridge('torch')

from my_affectgpt.tasks import *
from my_affectgpt.models import *
from my_affectgpt.runners import *
from my_affectgpt.processors import *
from my_affectgpt.datasets.builders import *
from my_affectgpt.common.config import Config
from my_affectgpt.common.dist_utils import get_rank
from my_affectgpt.common.registry import registry
from my_affectgpt.conversation.conversation_video import Chat
from my_affectgpt.datasets.builders.image_text_pair_builder import * # åŠ è½½æ‰€æœ‰dataset cls

import config
from toolkit.utils.read_files import *


# é‡‡ç”¨çš„æ˜¯è¿™ä¸ªæ–‡ä»¶ä¸‹å­˜å‚¨æ•°é‡æœ€å¤šçš„ root
def search_for_ckpt_root(root_candidates):
    if len(root_candidates) == 0:
        return ''
    
    # æ‰¾åˆ° files æœ€å¤šçš„ root
    maxcount = 0
    targetroot = ''
    for root in root_candidates:
        count = len([path for path in os.listdir(root) if path.startswith('checkpoint_')])
        print (root, '==>', count)
        if count > maxcount:
            maxcount = count
            targetroot = root
    print ('================================================')
    print (f'Targetroot: epoch range: 0-{maxcount-1}')
    
    # æ‰“å°æœ€åŽä¸€ä¸ªæ–‡ä»¶çš„åˆ›å»ºæ—¶é—´ for targetroot
    last_file = sorted(glob.glob(targetroot + '/checkpoint*'))[-1]
    file_stat = Path(last_file).stat()
    creation_time = file_stat.st_ctime
    print("Targetroot: Last ckpt creation time:", datetime.fromtimestamp(creation_time))
    print ('================================================')
    return targetroot


# case1: é»˜è®¤ => last epoch
# case2: æŒ‡å®š inference_cfg.test_epoch == a; é‚£å°±åªè·‘è¿™ä¸ª epoch ä¸‹çš„ç»“æžœ
# case3: æŒ‡å®š inference_cfg.test_epochs == a-b; è·‘æœ€åŽä¸€ä¸ª
def get_ckpt3_candidates(ckpt3_root, inference_cfg):
    
    if inference_cfg.test_epoch != 'xxx':
        cur_epoch = inference_cfg.test_epoch
        ckpts = glob.glob("%s/*%06d*.pth" %(ckpt3_root, int(cur_epoch)))
        assert len(ckpts) == 1, 'Error: (ckpt, epoch) combination is not exists or contain multiple candidates!'
        return [ckpts[0]]
    
    elif inference_cfg.test_epochs == 'xxx-xxx':
        last_ckpt = sorted(glob.glob("%s/*.pth" %(ckpt3_root)))[-1]
        last_epoch=  int(last_ckpt.split('_')[-3])
        assert last_epoch > 10, f'Error: too less training time to conduct automatic inference!'
        return [last_ckpt]
    
    else:
        start_epoch, end_epoch = inference_cfg.test_epochs.split('-')
        skip_epoch = int(inference_cfg.skip_epoch) 
        whole_ckpts = []
        for cur_epoch in range(int(start_epoch), int(end_epoch)+1):
            if cur_epoch % skip_epoch == 0:
                ckpts = glob.glob("%s/*%06d*.pth" %(ckpt3_root, int(cur_epoch)))
                assert len(ckpts) == 1, 'Error: (ckpt, epoch) combination is not exists or contain multiple candidates!'
                whole_ckpts.append(ckpts[0])
        return whole_ckpts


# å› ä¸ºæˆ‘ä»¬ç›®å‰åªå¤„ç† merbenchï¼Œè¿™äº›æ˜¯ video çš„ï¼Œéœ€è¦å’ŒåŽŸå§‹è®­ç»ƒæ•°æ®ä¸­çš„ video æ•°æ®å¯¹åº”çš„ face_or_frame ä¸€è‡´
def get_face_or_frame(datasets_cfg, outside_face_or_frame):
    if outside_face_or_frame is not None:
        return outside_face_or_frame
    
    face_or_frame_candidates = []
    if 'mercaptionplus' in datasets_cfg:
        face_or_frame_candidates.append(datasets_cfg['mercaptionplus'].face_or_frame)
    if 'ovmerd' in datasets_cfg:
        face_or_frame_candidates.append(datasets_cfg['ovmerd'].face_or_frame)
    assert len(set(face_or_frame_candidates)) == 1, f'must has the unified face_or_frame type'
    face_or_frame = list(set(face_or_frame_candidates))[0]
    return face_or_frame


def get_name2cls(dataset):
    if dataset == 'MER2023':          return MER2023_Dataset()
    if dataset == 'MER2024':          return MER2024_Dataset()
    if dataset == 'MELD':             return MELD_Dataset()
    if dataset == 'IEMOCAPFour':      return IEMOCAPFour_Dataset()
    if dataset == 'CMUMOSI':          return CMUMOSI_Dataset()
    if dataset == 'CMUMOSEI':         return CMUMOSEI_Dataset()
    if dataset == 'SIMS':             return SIMS_Dataset()
    if dataset == 'SIMSv2':           return SIMSv2_Dataset()
    if dataset == 'MER2025OV':        return MER2025OV_Dataset()
    if dataset == 'OVMERDPlus':       return OVMERDPlus_Dataset()
    print ('dataset cls not provided!')
    return None


# ä¼˜å…ˆçº§ï¼šoutside_user_message > zeroshot > use_reasoning > dataset specific
def get_user_message(dataset_cls, zeroshot, outside_user_message, use_reasoning=True):
    if outside_user_message is not None:
        user_message = outside_user_message
    elif zeroshot:
        # ðŸŽ¯ zeroshotä¼˜å…ˆï¼šåªè¦æ±‚åˆ†ç±»ï¼Œä¸è¦æ±‚reasoning
        user_message = dataset_cls.func_get_qa_ovlabel(sample=None, question_only=True)
    elif use_reasoning:
        # ä½¿ç”¨reasoningæ¨¡å¼ï¼šè¦æ±‚æ¨¡åž‹ç»™å‡ºæŽ¨ç†è¿‡ç¨‹
        user_message = dataset_cls.func_get_qa_description(sample=None, question_only=True)
    else:
        # é»˜è®¤ä½¿ç”¨reasoning
        user_message = dataset_cls.func_get_qa_description(sample=None, question_only=True)
    return user_message


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AffectGPT Inference Process")
    parser.add_argument("--cfg-path", default='xxx', help="path to configuration file.")
    parser.add_argument("--options",  nargs="+", help="override some settings in the used config, format: --option xx=xx yy=yy zz=zz")
    parser.add_argument("--dataset", default='merbench', help="evaluate dataset")
    parser.add_argument('--zeroshot', action='store_true', default=False, help='whether testing on zeroshot performance?')
    parser.add_argument('--no_reasoning', action='store_true', default=False, help='disable reasoning output, only classification')
    parser.add_argument('--outside_user_message',  default=None, help="we use the outside user message, rather than dataset dependent.")
    parser.add_argument('--outside_face_or_frame', default=None, help="we use the outside face_or_frame, rather than dataset dependent.")
    args = parser.parse_args()
    cfg = Config(args)
    model_cfg = cfg.model_cfg
    datasets_cfg = cfg.datasets_cfg
    inference_cfg = cfg.inference_cfg
    device = 'cuda:{}'.format(inference_cfg.gpu)
    inference_datasets = ['MER2023', 'MER2024', 'MELD', 'IEMOCAPFour', 'CMUMOSI', 'CMUMOSEI', 'SIMS', 'SIMSv2', 'OVMERDPlus']
    

    print ('======== Step1: cfg pre-analysis ========')
    # æ”¯æŒ ckpt_root / ckpt_name ä¸¤ç§ç±»åž‹è¾“å…¥ => (ckpt3_root)
    # é»˜è®¤æƒ…å†µæ˜¯ä¾æ® os.path.basename(args.cfg_path) æ‰¾åˆ° => (ckpt3_root)
    if inference_cfg.ckpt_root not in ['', 'xxx']:
        ckpt3_root = inference_cfg.ckpt_root
    elif inference_cfg.ckpt_name not in ['', 'xxx']:
        cfg_name = os.path.basename(args.cfg_path)[:-len('.yaml')]
        ckpt3_root = os.path.join('output', cfg_name, inference_cfg.ckpt_name)
        assert inference_cfg.ckpt_name.startswith(cfg_name) # è¿™å—å’Œ train éƒ¨åˆ†æ˜¯ç›¸äº’é…åˆä¸‹çš„ç»“æžœ
    else:
        print ('strat searching for suitable ckpt_root')
        cfg_name = os.path.basename(args.cfg_path)[:-len('.yaml')]
        root_candidates = glob.glob(os.path.join('output', cfg_name, cfg_name+'*'))
        ckpt3_root = search_for_ckpt_root(root_candidates)
    print ('processed ckpt3 root:')
    print (ckpt3_root)

    # (ckpt3_root) => processed epochs
    print ('processed ckpt3 epochs:')
    whole_ckpt3s = get_ckpt3_candidates(ckpt3_root, inference_cfg)
    for item in whole_ckpt3s: print (os.path.basename(item))

    # => (face_or_frame) (è¿™ä¸ªéœ€è¦ä¸Žè®­ç»ƒæ•°æ®é‡‡ç”¨çš„ face_or_frame ç›¸åŒ)
    face_or_frame = get_face_or_frame(datasets_cfg, args.outside_face_or_frame)
    print (f'Read data type: {face_or_frame}')
    print ('=======================================')


    ## main process for each ckpt3 candidates
    for ii, ckpt_3 in enumerate(whole_ckpt3s):

        ##############################################################
        print (f'======== Step2: initial model; using ckpt_3: {os.path.basename(ckpt_3)} ========')
        model_cfg.ckpt_3 = ckpt_3 # ckpt_3 has the highest priority
        if ii == 0: # first-round: initialize models
            model_cls = registry.get_model_class(model_cfg.arch) # affectgpt
            model = model_cls.from_config(model_cfg)
        if ii > 0:  # second-round: update trainable params (ç”¨æ–°çš„ ckpt_3 å‚æ•°è¦†ç›–)
            ckpt = torch.load(model_cfg.ckpt_3, map_location="cpu", weights_only=True)
            model.load_state_dict(ckpt['model'], strict=False)
        model = model.to(device).eval() # !! reduce randomness during the inference
        chat = Chat(model, model_cfg, device=device)
        ##############################################################


        print ('======== Step3: Inferece ========')
        if args.dataset == 'inferenceData':
            process_datasets = inference_datasets
        else:
            names = args.dataset.split(',')
            process_datasets = names
        print ('process datasets: ', process_datasets)

        ## for each dataset
        for dataset in process_datasets:
            print (f'current dataset: {dataset}')
            ## dataset_cls å†…éƒ¨åœ¨ train / inference å†…éƒ¨çš„æ›´æ–°
            dataset_cls = get_name2cls(dataset)
            dataset_cls.needed_data = dataset_cls.get_needed_data(face_or_frame)
            dataset_cls.vis_processor = BaseProcessor()
            dataset_cls.img_processor = BaseProcessor()
            vis_processor_cfg = inference_cfg.get("vis_processor") # read vis processor
            img_processor_cfg = inference_cfg.get("img_processor") # read img processor
            if vis_processor_cfg is not None:
                dataset_cls.vis_processor = registry.get_processor_class(vis_processor_cfg.train.name).from_config(vis_processor_cfg.train)
            if img_processor_cfg is not None:
                dataset_cls.img_processor = registry.get_processor_class(img_processor_cfg.train.name).from_config(img_processor_cfg.train)
            dataset_cls.n_frms = model_cfg.vis_processor.train.n_frms
            
            # æ·»åŠ Frameé‡‡æ ·é…ç½®æ”¯æŒ - ä»Žinference_cfgä¸­è¯»å–
            dataset_cls.frame_n_frms = getattr(inference_cfg, 'frame_n_frms', dataset_cls.n_frms)  # Frameå¸§æ•°ï¼Œé»˜è®¤ä¸Žn_frmsç›¸åŒ
            dataset_cls.frame_sampling = getattr(inference_cfg, 'frame_sampling', 'uniform')  # Frameé‡‡æ ·ç­–ç•¥ï¼Œé»˜è®¤uniform
            
            # æŽ¨ç†æ¨¡å¼é…ç½® - æ”¯æŒAUå®žæ—¶å¤„ç†å’ŒFrameé¢„æå–
            dataset_cls.use_realtime_extraction = False  # ä¸ä½¿ç”¨åˆ†å¸ƒå¼å®žæ—¶æå–
            
            # ðŸŽ¯ ä»Žé…ç½®æ–‡ä»¶è¯»å–æ¯ä¸ªæ¨¡æ€ç‹¬ç«‹çš„é¢„æå–é…ç½®
            # Frameç‰¹å¾ï¼šæ ¹æ®é‡‡æ ·ç­–ç•¥åŠ¨æ€å†³å®š
            if dataset_cls.frame_sampling == 'emotion_peak':
                dataset_cls.use_preextracted_frame = True   # emotion_peak â†’ é¢„æå–
                print(f"ðŸ“¥ [Frame] emotion_peaké‡‡æ · â†’ ä½¿ç”¨é¢„æå–ç‰¹å¾")
            else:
                dataset_cls.use_preextracted_frame = False  # uniformç­‰ â†’ å®žæ—¶å¤„ç†
                print(f"ðŸŽ¬ [Frame] {dataset_cls.frame_sampling}é‡‡æ · â†’ ä½¿ç”¨å®žæ—¶å¤„ç†")
            
            dataset_cls.use_preextracted_face = getattr(inference_cfg, 'use_preextracted_face', False)
            dataset_cls.use_preextracted_audio = getattr(inference_cfg, 'use_preextracted_audio', False)
            dataset_cls.use_preextracted_au = getattr(inference_cfg, 'use_preextracted_au', False)
            
            dataset_cls.preextracted_root = getattr(inference_cfg, 'preextracted_root', './preextracted_features')
            dataset_cls.visual_encoder = getattr(inference_cfg, 'visual_encoder', 'CLIP_VIT_LARGE')
            dataset_cls.acoustic_encoder = getattr(inference_cfg, 'acoustic_encoder', 'HUBERT_LARGE')
            
            # æ£€æµ‹æ˜¯å¦éœ€è¦AUæ¨¡æ€ï¼ˆæ³¨æ„ï¼šä¸èƒ½ç”¨'au' in stringï¼Œä¼šåŒ¹é…åˆ°audioï¼‰
            # ä½¿ç”¨å•è¯åˆ†å‰²æ¥å‡†ç¡®æ£€æµ‹AUæ¨¡æ€
            tokens = face_or_frame.lower().replace('_', ' ').split()
            use_au = 'au' in tokens
            if use_au:
                # ðŸŽ¯ Nonverbalæ–‡æœ¬æ¨¡å¼ï¼šç›´æŽ¥ä»ŽJSONåŠ è½½æ–‡æœ¬åµŒå…¥prompt
                dataset_cls.nonverbal_json = getattr(inference_cfg, 'nonverbal_json', None)
                dataset_cls.use_nonverbal_text = getattr(inference_cfg, 'use_nonverbal_text', False)
                dataset_cls._nonverbal_data = None  # æ‡’åŠ è½½
                
                if dataset_cls.use_nonverbal_text and dataset_cls.nonverbal_json:
                    print(f'âœ… [INFERENCE] Nonverbalæ¨¡å¼: æ–‡æœ¬ç›´æŽ¥åµŒå…¥prompt')
                    print(f'   Nonverbal JSON: {dataset_cls.nonverbal_json}')
                else:
                    print(f'âš ï¸ [INFERENCE] Nonverbalæœªé…ç½®ï¼ŒNonverbalä¿¡æ¯å°†ä¸å¯ç”¨')
            
            print(f'====== Inference Frame Sampling Config ======')
            print(f'Frame frames: {dataset_cls.frame_n_frms}, Frame sampling: {dataset_cls.frame_sampling}')
            print(f'Face frames: {dataset_cls.n_frms}, Face sampling: uniform')
            
            # æ˜¾ç¤ºå„æ¨¡æ€é¢„æå–é…ç½®çŠ¶æ€
            print(f'====== Preextracted Features Config ======')
            print(f'Frame: {"âœ… Preextracted" if dataset_cls.use_preextracted_frame else "âŒ Realtime"}')
            print(f'Face:  {"âœ… Preextracted" if dataset_cls.use_preextracted_face else "âŒ Realtime"}')
            print(f'Audio: {"âœ… Preextracted" if dataset_cls.use_preextracted_audio else "âŒ Realtime"}')
            print(f'Nonverbal: {"âœ… Text" if (use_au and getattr(dataset_cls, "use_nonverbal_text", False)) else "N/A"}')


            ## è¯»å–æ¯ä¸ªæ•°æ®é›†çš„å†…å®¹
            test_names = dataset_cls.read_test_names()
            name2subtitle = dataset_cls.name2subtitle

            ## å®šä¹‰ç»“æžœå­˜å‚¨ä½ç½®ï¼Œå¦‚æžœå­˜åœ¨ç›¸åº”è·¯å¾„ç›´æŽ¥è·³è¿‡
            save_root = os.path.join(inference_cfg.base_root + f'-{dataset.lower()}', # output/results-{dataset}/ckpt3_name
                                    os.path.basename(ckpt3_root)) 
            if not os.path.exists(save_root): os.makedirs(save_root)
            epoch = os.path.basename(cfg.model_cfg.ckpt_3)[:-4]
            save_path = '%s/%s.npz' %(save_root, epoch) # output/result-{dataset}/ckpt3_name/epochname
            if os.path.exists(save_path): continue

            ## ä¸»è¦å¤„ç†å‡½æ•° ã€è´¹æ—¶çš„ä¸»è¦åœ¨è¿™ä¸ªéƒ¨åˆ†ã€‘
            name2reason = {}
            for ii, name in enumerate(test_names):
                subtitle = name2subtitle[name]
                print (f'process on {ii}|{len(test_names)}: {name} | {subtitle}')

                # è½¬æˆ cls é‡Œé¢çš„æ”¯æŒç±»åž‹è¿›è¡Œ path è¯»å–
                sample = {'name': name}
                video_path, image_path, audio_path, face_npy = None, None, None, None
                if hasattr(dataset_cls, '_get_video_path'): video_path = dataset_cls._get_video_path(sample)
                if hasattr(dataset_cls, '_get_audio_path'): audio_path = dataset_cls._get_audio_path(sample)
                if hasattr(dataset_cls, '_get_face_path'):  face_npy   = dataset_cls._get_face_path(sample)
                if hasattr(dataset_cls, '_get_image_path'): image_path = dataset_cls._get_image_path(sample)
                sample_data = dataset_cls.read_frame_face_audio_text(video_path, face_npy, audio_path, image_path, sample_name=name)
                
                # æ£€æŸ¥sample_dataæ˜¯å¦æœ‰æ•ˆ
                if sample_data is None:
                    print(f"âš ï¸ æ ·æœ¬æ•°æ®åŠ è½½å¤±è´¥ï¼Œè·³è¿‡: {name}")
                    continue
                # print (sample_data['face'].shape)

                # => img_list (ä¸å†åŒ…å«AUï¼ŒAUä½œä¸ºæ–‡æœ¬ç›´æŽ¥åµŒå…¥prompt)
                audio_llms, frame_llms, face_llms, image_llms, multi_llms = None, None, None, None, None
                audio_hiddens, audio_llms = chat.postprocess_audio(sample_data)  
                frame_hiddens, frame_llms = chat.postprocess_frame(sample_data)
                face_hiddens,  face_llms  = chat.postprocess_face(sample_data)
                _,             image_llms = chat.postprocess_image(sample_data)
                if face_or_frame.startswith('multiface'):
                    _, multi_llms = chat.postprocess_multi(face_hiddens, audio_hiddens)
                elif face_or_frame.startswith('multiframe'):
                    _, multi_llms = chat.postprocess_multi(frame_hiddens, audio_hiddens)

                img_list = {}
                img_list['audio'] = audio_llms
                img_list['frame'] = frame_llms
                img_list['face']  = face_llms
                img_list['image'] = image_llms
                img_list['multi'] = multi_llms
                # ðŸŽ¯ AUä¸å†ä½œä¸ºç‰¹å¾ï¼Œæ”¹ä¸ºcaptionæ–‡æœ¬ç›´æŽ¥åµŒå…¥prompt

                # ðŸŽ¯ èŽ·å–Nonverbalæ–‡æœ¬
                nonverbal_text = None
                if getattr(dataset_cls, 'use_nonverbal_text', False):
                    nonverbal_text = dataset_cls.get_nonverbal_text(name)

                # get prompt (use_reasoning=True => reasoning; zeroshot => ov labels; else => dataset specific)
                use_reasoning = not args.no_reasoning  # é»˜è®¤å¯ç”¨reasoning
                user_message = get_user_message(dataset_cls, args.zeroshot, args.outside_user_message, use_reasoning)
                prompt = dataset_cls.get_prompt_for_multimodal(face_or_frame, subtitle, user_message, nonverbal_text=nonverbal_text)
                
                # => call function
                response = chat.answer_sample(prompt=prompt, img_list=img_list,
                                            num_beams=1, temperature=1, do_sample=True, top_p=0.9, 
                                            max_new_tokens=1200, max_length=2000) # llama: max_token_num=2048
                name2reason[name] = response
                print (response)

                # if ii == 0: break # for debug

            print ('save results')
            np.savez_compressed(save_path, name2reason=name2reason)
