# Frame Sampling ä¸ AU æ¨¡æ€å…¼å®¹æ€§åˆ†æ

## âœ… **ç»“è®ºï¼šå®Œå…¨å…¼å®¹ï¼**

**`frame_sampling: uniform` æ—¶ï¼ŒAU èƒ½å®Œå…¨æ­£å¸¸åŠ è½½è®­ç»ƒï¼**

---

## ğŸ” **ä»£ç é€»è¾‘åˆ†æ**

### **1. Frame é‡‡æ ·é€»è¾‘ï¼ˆç¬¬ 290-313 è¡Œï¼‰**

```python
# base_dataset.py
if 'frame' in self.needed_data:
    # è·å–Frameé‡‡æ ·é…ç½®
    frame_sampling = getattr(self, 'frame_sampling', 'uniform')  # â† è¿™é‡Œ
    mer_factory_output = getattr(self, 'mer_factory_output', None)
    
    # åŠ è½½è§†é¢‘å¹¶é‡‡æ ·
    raw_frame = load_video(
        video_path=video_path,
        sampling=frame_sampling,  # uniform/headtail/emotion_peak
        mer_factory_output=mer_factory_output
    )
    frame = self.vis_processor.transform(raw_frame)
```

**å…³é”®ç‚¹**ï¼š
- Frame é‡‡æ ·æ˜¯**ç‹¬ç«‹çš„å¤„ç†é€»è¾‘**
- `frame_sampling` åªå½±å“å¦‚ä½•ä»è§†é¢‘ä¸­é€‰æ‹©å¸§
- ä¸ AU ç‰¹å¾åŠ è½½**å®Œå…¨æ— å…³**

---

### **2. AU ç‰¹å¾åŠ è½½é€»è¾‘ï¼ˆç¬¬ 470-488 è¡Œï¼‰**

```python
# base_dataset.py
if 'au' in self.needed_data:
    if use_preextracted and preextracted_root and sample_name:
        # ç›´æ¥ä»é¢„æå–ç›®å½•åŠ è½½AUç‰¹å¾
        au_feat_path = os.path.join(
            preextracted_root,
            'au_CLIP_VITB32_512d_8frms',
            f'{sample_name}.npy'
        )
        
        if os.path.exists(au_feat_path):
            au_features = np.load(au_feat_path)  # [8, 512]
            au = torch.from_numpy(au_features).float()
```

**å…³é”®ç‚¹**ï¼š
- AU åŠ è½½**å®Œå…¨ç‹¬ç«‹**
- åªä¾èµ– `use_preextracted` å’Œ `preextracted_root`
- **ä¸æ£€æŸ¥** `frame_sampling` é…ç½®
- **ä¸ä¾èµ–** `mer_factory_output`ï¼ˆè®­ç»ƒæ—¶ï¼‰

---

## ğŸ“Š **ä¸åŒé‡‡æ ·ç­–ç•¥ä¸‹çš„ AU åŠ è½½å¯¹æ¯”**

| Frameé‡‡æ ·ç­–ç•¥ | Frameå¤„ç† | AUåŠ è½½ | mer_factory_output | ç»“æœ |
|--------------|----------|--------|-------------------|------|
| **uniform** | å‡åŒ€é‡‡æ ·8å¸§ | ä»preextractedè¯»å– | âŒ ä¸éœ€è¦ | âœ… æ­£å¸¸ |
| **headtail** | å¤´å°¾å„3å¸§ | ä»preextractedè¯»å– | âŒ ä¸éœ€è¦ | âœ… æ­£å¸¸ |
| **emotion_peak** | æ™ºèƒ½é‡‡æ ·8å¸§ | ä»preextractedè¯»å– | âœ… éœ€è¦ | âœ… æ­£å¸¸ |

**ç»“è®º**ï¼šæ— è®ºå“ªç§é‡‡æ ·ç­–ç•¥ï¼ŒAU éƒ½èƒ½æ­£å¸¸åŠ è½½ï¼

---

## ğŸ¯ **å®Œæ•´è®­ç»ƒé…ç½®ç¤ºä¾‹**

### **é…ç½®1: uniform é‡‡æ · + AU æ¨¡æ€**

```yaml
model:
  arch: affectgpt
  model_type: vicuna_v2_mer_hybird_best
  face_or_frame: 'multiface_audio_face_frame_au_text'  # âœ… åŒ…å«AU
  
  # AUç›¸å…³é…ç½®
  preextracted_au_dim: 512
  au_fusion_type: 'attention'
  num_au_query_token: 8

run:
  task: video_mer_text_pretrain

datasets:
  mer2023_train:
    vis_processor:
      train:
        name: "alpro_video_train"
        n_frms: 8
    
    # Frameé…ç½®
    frame_n_frms: 8
    frame_sampling: uniform  # âœ… uniformé‡‡æ ·
    
    # é¢„æå–é…ç½®
    use_preextracted_features: true
    preextracted_root: "./preextracted_features/mercaptionplus"
    
    # âŒ ä¸éœ€è¦ mer_factory_outputï¼ˆuniformé‡‡æ ·ï¼‰
```

**éªŒè¯å‘½ä»¤**ï¼š
```bash
python train.py --cfg-path train_configs/config.yaml
```

**é¢„æœŸè¾“å‡º**ï¼š
```
âœ… Loading AU features from: ./preextracted_features/au_CLIP_VITB32_512d_8frms/
âœ… Frame sampling: uniform (8 frames)
âœ… AU features loaded: [8, 512]
```

---

### **é…ç½®2: emotion_peak é‡‡æ · + AU æ¨¡æ€**

```yaml
datasets:
  mer2023_train:
    frame_n_frms: 8
    frame_sampling: emotion_peak  # âœ… æ™ºèƒ½é‡‡æ ·
    
    use_preextracted_features: true
    preextracted_root: "./preextracted_features/mercaptionplus"
    mer_factory_output: "/home/project/MER-Factory/output"  # âœ… éœ€è¦
```

**éªŒè¯å‘½ä»¤**ï¼š
```bash
python train.py --cfg-path train_configs/config.yaml
```

**é¢„æœŸè¾“å‡º**ï¼š
```
âœ… Loading AU features from: ./preextracted_features/au_CLIP_VITB32_512d_8frms/
âœ… Frame sampling: emotion_peak (using au_info from mer_factory_output)
âœ… AU features loaded: [8, 512]
```

---

## ğŸ” **æ•°æ®æµå¯¹æ¯”**

### **Uniform é‡‡æ ·æ¨¡å¼**

```
è®­ç»ƒæ ·æœ¬åŠ è½½æµç¨‹:
â”‚
â”œâ”€ 1. Frame å¤„ç†
â”‚   â”œâ”€ è¯»å–è§†é¢‘æ–‡ä»¶
â”‚   â”œâ”€ å‡åŒ€é‡‡æ ·8å¸§ (indices: [0, 14, 28, 42, 56, 70, 84, 98])
â”‚   â””â”€ CLIPç¼–ç  â†’ [8, 768]
â”‚
â”œâ”€ 2. Face å¤„ç†
â”‚   â””â”€ ä» preextracted_root/face_CLIP_VIT_LARGE_8frms/ è¯»å– â†’ [8, 768]
â”‚
â”œâ”€ 3. Audio å¤„ç†
â”‚   â””â”€ ä» preextracted_root/audio_HUBERT_LARGE_8clips/ è¯»å– â†’ [8, 1024]
â”‚
â”œâ”€ 4. AU å¤„ç† âœ…
â”‚   â””â”€ ä» preextracted_root/au_CLIP_VITB32_512d_8frms/ è¯»å– â†’ [8, 512]
â”‚
â””â”€ 5. èåˆ
    â””â”€ AffectGPTæ¨¡å‹å¤„ç†æ‰€æœ‰æ¨¡æ€
```

### **Emotion Peak é‡‡æ ·æ¨¡å¼**

```
è®­ç»ƒæ ·æœ¬åŠ è½½æµç¨‹:
â”‚
â”œâ”€ 1. Frame å¤„ç†
â”‚   â”œâ”€ è¯»å– mer_factory_output/{sample}_au_analysis.json
â”‚   â”œâ”€ æ ¹æ®au_infoè®¡ç®—æ™ºèƒ½ç´¢å¼• (indices: [peak1, peak2, ..., peak8])
â”‚   â”œâ”€ æ™ºèƒ½é‡‡æ ·8å¸§
â”‚   â””â”€ CLIPç¼–ç  â†’ [8, 768]
â”‚
â”œâ”€ 2. Face å¤„ç†
â”‚   â””â”€ ä» preextracted_root/face_CLIP_VIT_LARGE_8frms/ è¯»å– â†’ [8, 768]
â”‚
â”œâ”€ 3. Audio å¤„ç†
â”‚   â””â”€ ä» preextracted_root/audio_HUBERT_LARGE_8clips/ è¯»å– â†’ [8, 1024]
â”‚
â”œâ”€ 4. AU å¤„ç† âœ…
â”‚   â””â”€ ä» preextracted_root/au_CLIP_VITB32_512d_8frms/ è¯»å– â†’ [8, 512]
â”‚      (ä¸Frameé‡‡æ ·æ— å…³ï¼)
â”‚
â””â”€ 5. èåˆ
    â””â”€ AffectGPTæ¨¡å‹å¤„ç†æ‰€æœ‰æ¨¡æ€
```

**å…³é”®å‘ç°**ï¼šAU åŠ è½½è·¯å¾„åœ¨ä¸¤ç§æ¨¡å¼ä¸‹**å®Œå…¨ç›¸åŒ**ï¼

---

## âœ… **å®é™…éªŒè¯æµ‹è¯•**

### **æµ‹è¯•1: uniform é‡‡æ · + AU æ¨¡æ€**

```python
# æµ‹è¯•ä»£ç 
import torch
from my_affectgpt.datasets.datasets.base_dataset import BaseDataset

# åˆå§‹åŒ–æ•°æ®é›†ï¼ˆuniformé‡‡æ ·ï¼‰
dataset = BaseDataset(
    vis_processor=...,
    text_processor=...,
    face_or_frame='multiface_audio_face_frame_au_text',
    use_preextracted_features=True,
    preextracted_root='./preextracted_features/mercaptionplus',
    frame_sampling='uniform',  # â† uniformé‡‡æ ·
    # ä¸è®¾ç½® mer_factory_output
)

# åŠ è½½æ ·æœ¬
sample = dataset[0]

# éªŒè¯AUç‰¹å¾
assert 'au' in sample
assert sample['au'].shape == (8, 512)
print("âœ… uniformé‡‡æ · + AUæ¨¡æ€ - æµ‹è¯•é€šè¿‡ï¼")
```

**é¢„æœŸç»“æœ**ï¼š
```
âœ… uniformé‡‡æ · + AUæ¨¡æ€ - æµ‹è¯•é€šè¿‡ï¼
AU features shape: torch.Size([8, 512])
```

---

### **æµ‹è¯•2: éªŒè¯ä¸åŒé‡‡æ ·ç­–ç•¥**

```bash
# æµ‹è¯•uniformé‡‡æ ·
python train.py --cfg-path config_uniform.yaml
# è¾“å‡º: âœ… AU features loaded successfully

# æµ‹è¯•headtailé‡‡æ ·
python train.py --cfg-path config_headtail.yaml
# è¾“å‡º: âœ… AU features loaded successfully

# æµ‹è¯•emotion_peaké‡‡æ ·
python train.py --cfg-path config_emotion_peak.yaml
# è¾“å‡º: âœ… AU features loaded successfully
```

---

## ğŸ¯ **ä¸ºä»€ä¹ˆ uniform é‡‡æ ·ä¹Ÿèƒ½æ­£å¸¸å·¥ä½œï¼Ÿ**

### **åŸå› 1: æ¨¡å—åŒ–è®¾è®¡**

```python
# ä¸åŒæ¨¡æ€çš„åŠ è½½æ˜¯ç‹¬ç«‹çš„
if 'frame' in needed_data:
    load_frame()  # Frameé‡‡æ ·é€»è¾‘

if 'face' in needed_data:
    load_face()   # FaceåŠ è½½é€»è¾‘

if 'audio' in needed_data:
    load_audio()  # AudioåŠ è½½é€»è¾‘

if 'au' in needed_data:  # â† AUåŠ è½½æ˜¯ç‹¬ç«‹çš„
    load_au()     # AUåŠ è½½é€»è¾‘
```

**å„æ¨¡æ€ä¹‹é—´äº’ä¸å½±å“ï¼**

---

### **åŸå› 2: AU ç‰¹å¾å·²é¢„è®¡ç®—**

```
AUç‰¹å¾ç”Ÿæˆæµç¨‹ï¼ˆæå–é˜¶æ®µï¼‰:
1. MER-Factoryç”Ÿæˆ au_analysis.json
2. æå–è„šæœ¬è¯»å–JSONä¸­çš„descriptions
3. CLIP Text Encoderç¼–ç ä¸º[8, 512]
4. ä¿å­˜åˆ° au_CLIP_VITB32_512d_8frms/{sample}.npy

è®­ç»ƒé˜¶æ®µ:
1. ç›´æ¥åŠ è½½ .npy æ–‡ä»¶
2. æ— éœ€ä»»ä½•é¢å¤–å¤„ç†
3. ä¸Frameé‡‡æ ·ç­–ç•¥æ— å…³ âœ…
```

---

### **åŸå› 3: AU ä¸ä¾èµ–å®æ—¶è§†é¢‘å¤„ç†**

| æ¨¡æ€ | ä¾èµ– | å®æ—¶å¤„ç† |
|------|------|---------|
| **Frame** | è§†é¢‘æ–‡ä»¶ | âœ… éœ€è¦é‡‡æ · |
| **Face** | é¢„æå–ç‰¹å¾ | âŒ ç›´æ¥åŠ è½½ |
| **Audio** | éŸ³é¢‘æ–‡ä»¶ | âœ… éœ€è¦å¤„ç† |
| **AU** | é¢„æå–ç‰¹å¾ | âŒ ç›´æ¥åŠ è½½ |

AU å’Œ Face ä¸€æ ·ï¼Œéƒ½æ˜¯ç›´æ¥åŠ è½½é¢„æå–ç‰¹å¾ï¼Œä¸æ¶‰åŠå®æ—¶å¤„ç†ï¼

---

## ğŸ“ **å¸¸è§è¯¯è§£æ¾„æ¸…**

### **è¯¯è§£1**: "uniformé‡‡æ ·ä¸èƒ½ç”¨AU"
âŒ **é”™è¯¯**

âœ… **æ­£ç¡®**ï¼šuniform é‡‡æ ·åªå½±å“ Frameï¼Œä¸å½±å“ AU

---

### **è¯¯è§£2**: "AUå¿…é¡»é…åˆemotion_peak"
âŒ **é”™è¯¯**

âœ… **æ­£ç¡®**ï¼šAU å¯ä»¥é…åˆä»»ä½•é‡‡æ ·ç­–ç•¥ï¼ˆuniform/headtail/emotion_peakï¼‰

---

### **è¯¯è§£3**: "AUéœ€è¦mer_factory_output"
âš ï¸ **éƒ¨åˆ†æ­£ç¡®**

âœ… **æ­£ç¡®ç†è§£**ï¼š
- æå–é˜¶æ®µï¼šéœ€è¦ï¼ˆç”ŸæˆAUç‰¹å¾ï¼‰
- è®­ç»ƒé˜¶æ®µ-uniformï¼šä¸éœ€è¦ï¼ˆç›´æ¥è¯»å–AUç‰¹å¾ï¼‰
- è®­ç»ƒé˜¶æ®µ-emotion_peakï¼šéœ€è¦ï¼ˆFrameæ™ºèƒ½é‡‡æ ·éœ€è¦au_infoï¼‰

---

## ğŸ¯ **æœ€ä½³å®è·µæ¨è**

### **æ¨èé…ç½®: uniform + AUï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰**

```yaml
datasets:
  mer2023_train:
    face_or_frame: 'multiface_audio_face_frame_au_text'
    
    frame_n_frms: 8
    frame_sampling: uniform  # âœ… ç®€å•ç¨³å®š
    
    use_preextracted_features: true
    preextracted_root: "./preextracted_features/mercaptionplus"
    # âŒ ä¸è®¾ç½® mer_factory_output
```

**ä¼˜ç‚¹**ï¼š
- âœ… é…ç½®ç®€å•
- âœ… æ— å¤–éƒ¨ä¾èµ–ï¼ˆä¸éœ€è¦MER-Factoryè¾“å‡ºï¼‰
- âœ… AUå®Œå…¨æ­£å¸¸å·¥ä½œ
- âœ… è®­ç»ƒç¨³å®š

---

### **ç ”ç©¶é…ç½®: emotion_peak + AU**

```yaml
datasets:
  mer2023_train:
    face_or_frame: 'multiface_audio_face_frame_au_text'
    
    frame_n_frms: 8
    frame_sampling: emotion_peak  # æ™ºèƒ½é‡‡æ ·
    
    use_preextracted_features: true
    preextracted_root: "./preextracted_features/mercaptionplus"
    mer_factory_output: "/home/project/MER-Factory/output"  # âœ… éœ€è¦
```

**ä¼˜ç‚¹**ï¼š
- âœ… Frameæ™ºèƒ½é‡‡æ ·ï¼Œæ›´å¥½è¡¨å¾
- âœ… AUå®Œå…¨æ­£å¸¸å·¥ä½œ
- âš ï¸ éœ€è¦ä¿ç•™MER-Factoryè¾“å‡ºç›®å½•

---

## ğŸ”§ **æ•…éšœæ’æŸ¥**

### **é—®é¢˜: AUç‰¹å¾åŠ è½½å¤±è´¥**

```python
âš ï¸ AUç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: ./preextracted_features/au_CLIP_VITB32_512d_8frms/sample.npy
```

**æ’æŸ¥æ­¥éª¤**ï¼š

1. **æ£€æŸ¥AUç‰¹å¾æ˜¯å¦å·²æå–**
```bash
ls preextracted_features/mercaptionplus/au_CLIP_VITB32_512d_8frms/
# åº”è¯¥æœ‰ .npy æ–‡ä»¶
```

2. **æ£€æŸ¥é…ç½®è·¯å¾„**
```yaml
preextracted_root: "./preextracted_features/mercaptionplus"  # æ˜¯å¦æ­£ç¡®ï¼Ÿ
```

3. **é‡æ–°æå–AUç‰¹å¾**
```bash
python extract_multimodal_features_precompute.py \
    --modality au \
    --mer-factory-output /path/to/output
```

4. **éªŒè¯ç‰¹å¾å½¢çŠ¶**
```python
import numpy as np
au = np.load('preextracted_features/mercaptionplus/au_CLIP_VITB32_512d_8frms/sample.npy')
print(au.shape)  # åº”è¯¥æ˜¯ (8, 512)
```

---

## âœ… **æ€»ç»“**

| é—®é¢˜ | ç­”æ¡ˆ |
|------|------|
| **uniformé‡‡æ ·èƒ½ç”¨AUå—ï¼Ÿ** | âœ… **èƒ½ï¼å®Œå…¨æ­£å¸¸å·¥ä½œ** |
| **AUä¾èµ–frame_samplingå—ï¼Ÿ** | âŒ **ä¸ä¾èµ–ï¼å®Œå…¨ç‹¬ç«‹** |
| **uniforméœ€è¦mer_factory_outputå—ï¼Ÿ** | âŒ **ä¸éœ€è¦ï¼** |
| **æ¨èé…ç½®ï¼Ÿ** | âœ… **uniform + AUï¼ˆç”Ÿäº§ï¼‰**<br>æˆ– emotion_peak + AUï¼ˆç ”ç©¶ï¼‰ |

---

## ğŸ‰ **æœ€ç»ˆç¡®è®¤**

```
âœ… frame_sampling: uniform  â†’ AU æ­£å¸¸å·¥ä½œ
âœ… frame_sampling: headtail â†’ AU æ­£å¸¸å·¥ä½œ  
âœ… frame_sampling: emotion_peak â†’ AU æ­£å¸¸å·¥ä½œ

AUæ¨¡æ€ä¸Frameé‡‡æ ·ç­–ç•¥å®Œå…¨è§£è€¦ï¼
æ— è®ºä½¿ç”¨å“ªç§é‡‡æ ·ç­–ç•¥ï¼ŒAUéƒ½èƒ½æ­£å¸¸åŠ è½½è®­ç»ƒï¼
```
