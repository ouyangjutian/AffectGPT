## æ¨èè®­ç»ƒé…ç½® - ä½¿ç”¨é¢„æå–AUç‰¹å¾ï¼ˆä¸ä½¿ç”¨AU Agentï¼‰
## ä¼˜ç‚¹ï¼šè®­ç»ƒå¿«3-5å€ï¼Œæ˜¾å­˜å ç”¨å°ï¼ˆ15GB vs 30GBï¼‰

model:
  arch: affectgpt
  model_type: pretrain_vicuna
  
  # ======================== ç¼–ç å™¨é…ç½® ========================
  # ğŸ¯ å…³é”®é…ç½®ï¼šä¸ä½¿ç”¨AU Agent
  use_au_agent: False  # âŒ è®­ç»ƒæ—¶ä¸åŠ è½½AU Agent
  
  # é¢„æå–æ¨¡å¼ä¼˜åŒ– - èŠ‚çœæ˜¾å­˜
  # skip_encoders: True  # âœ… è·³è¿‡CLIPå’ŒHuBERTåŠ è½½ï¼ˆè¿›ä¸€æ­¥èŠ‚çœ2-3GBï¼‰è®­ç»ƒ
  skip_encoders: False # âŒ æ¨ç†
  preextracted_visual_dim: 768   # CLIP_VIT_LARGEè¾“å‡ºç»´åº¦
  preextracted_acoustic_dim: 1024  # HUBERT_LARGEè¾“å‡ºç»´åº¦
  preextracted_au_dim: 512       # CLIP text encoder AUç‰¹å¾ç»´åº¦

  # ======================== å†»ç»“ç­–ç•¥ ========================
  # Fusioné…ç½®
  frozen_video_proj: False
  frozen_video_Qformer: False
  frozen_audio_Qformer: False
  frozen_audio_proj: False
  frozen_au_Qformer: False     # AU Q-Formerå†»ç»“å‚æ•°
  frozen_au_proj: False        # AUæŠ•å½±å±‚å†»ç»“å‚æ•°
  frozen_multi_Qformer: False
  frozen_multi_llama_proj: False
  frozen_llm: False

  # ======================== èåˆç­–ç•¥ ============================
  multi_fusion_type: attention # attention/qformer
  video_fusion_type: attention # qformer(default)/mean/attention
  audio_fusion_type: attention # qformer(default)/mean/attention
  image_fusion_type: mean       # token(default)/mean
  au_fusion_type: attention     # qformer/mean/attention

  # ============================= Checkpoint ============================
  # AffectGPT checkpoint
  ckpt: ""
  ckpt_2: ""

  # ========================   LLMé…ç½®ä¸ç¼–ç å™¨é…ç½®ï¼ˆä»…ç”¨äºè·¯å¾„æ„å»ºï¼‰========================
  llama_model: "Qwen25"
  acoustic_encoder: "HUBERT_LARGE"
  visual_encoder: "CLIP_VIT_LARGE"

  # Query tokené…ç½®
  num_audio_query_token: 1
  num_video_query_token: 1
  num_multi_query_token: 1
  num_image_query_token: 1
  num_au_query_token: 1

  max_length: 1024
  # ======================== Processoré…ç½® ========================
  vis_processor:
    train:
      name: "alpro_video_train"
      n_frms: 8
      image_size: 224
  text_processor:
    train:
      name: "blip_caption"
  img_processor:
    train:
      name: "blip2_image_train"
      image_size: 224

  # ======================== æ•°æ®é›†é…ç½® ========================
datasets:
  mercaptionplus:
    data_type: video
    face_or_frame: 'multiface_audio_face_frame_text'  # multi, face, audio, frame, au, text
    label_type: 'hybird'
    
    # Frameé‡‡æ ·é…ç½®
    frame_n_frms: 8
    frame_sampling: 'emotion_peak'  # æˆ– 'emotion_peak'ï¼ˆAUæ™ºèƒ½é‡‡æ ·ï¼‰
    
    # MER-Factoryè¾“å‡ºè·¯å¾„ - ç”¨äºAUæ¨¡æ€å’Œemotion_peaké‡‡æ ·
    mer_factory_output: '/home/project/MER-Factory/output'
    
    # ğŸ¯ é¢„æå–ç‰¹å¾é…ç½® - è®­ç»ƒæ—¶å…¨éƒ¨å¯ç”¨
    use_preextracted_frame: True   # âœ… Frameé¢„æå–
    use_preextracted_face: True    # âœ… Faceé¢„æå–
    use_preextracted_audio: True   # âœ… Audioé¢„æå–
    # use_preextracted_au: True      # âœ… AUé¢„æå–
    
    preextracted_root: './preextracted_features/mercaptionplus'
    
    # ç¼–ç å™¨é…ç½®ï¼ˆç”¨äºæ„å»ºç‰¹å¾è·¯å¾„ï¼‰
    visual_encoder: 'CLIP_VIT_LARGE'
    acoustic_encoder: 'HUBERT_LARGE'
    clips_per_video: 8
    
    # ä½¿ç”¨è¯´æ˜:
    # 1. é¢„å…ˆè¿è¡Œ extract_mercaptionplus_features.sh æå–AUç‰¹å¾
    # 2. ç¡®ä¿ preextracted_features/mercaptionplus/au_CLIP_VITB32_8frms/ å­˜åœ¨
    # 3. Frame/Face/Audioä¹Ÿå»ºè®®é¢„æå–ä»¥è·å¾—æœ€ä½³æ€§èƒ½


run:
  task: video_text_pretrain

  # ======================== å­¦ä¹ ç‡é…ç½®ï¼ˆå…³é”®ï¼ï¼‰========================
  # optimizer
  # optimizer: adamw    # AdamWä¼˜åŒ–å™¨
  lr_sched: "linear_warmup_cosine_lr"
  init_lr: 1e-5       # åˆå§‹å­¦ä¹ ç‡ï¼ˆå³°å€¼ï¼‰ æ¨èï¼š1e-4
  min_lr: 1e-5        # æœ€å°å­¦ä¹ ç‡ï¼ˆå…è®¸è¡°å‡100å€ï¼‰1e-6
  warmup_lr: 1e-6     # Warmupèµ·å§‹å­¦ä¹ ç‡
  weight_decay: 0.05  # æƒé‡è¡°å‡ï¼ˆæ­£åˆ™åŒ–ï¼‰

  max_epoch: 60  # 60           # 15  # æ€»epochæ•°
  iters_per_epoch: 5000 # 5000  # 31327                 # æ¯ä¸ªepochå®Œæ•´éå†æ•°æ®é›†
  warmup_steps: 5000 # 5000     # 6265                     # Warmupæ­¥æ•°ï¼ˆ2%çš„æ€»æ­¥æ•°ï¼‰

  batch_size_train: 1
  batch_size_eval: 1
  # æµ‹è¯•æ¢¯åº¦ç´¯ç§¯
  # accum_grad_iters: 8                    # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæ¨¡æ‹Ÿbatch_size=8ï¼‰

  seed: 42
  num_workers: 4

  amp: True
  resume_ckpt_path: null

  evaluate: False
  train_splits: ["train"]

  device: "cuda"
  world_size: 1
  dist_url: "env://"
  distributed: True
    # ===== æ—¥å¿—å’Œä¿å­˜ =====
  # log_freq: 50                          # æ—¥å¿—æ‰“å°é¢‘ç‡ï¼ˆæ¯100æ­¥ï¼‰
  # visualize_training: False    # å¯ç”¨ï¼ˆé»˜è®¤ï¼‰


inference:
  task: video_text_pretrain

  vis_processor:
    train:
      name: "alpro_video_eval"
      n_frms: 8
      image_size: 224
  text_processor:
    train:
      name: "blip_caption"
  img_processor:
    train:
      name: "blip2_image_eval"
      image_size: 224

  face_or_frame: 'xxx'
  
  # æ¨ç†é…ç½®
  frame_n_frms: 8
  frame_sampling: 'emotion_peak'       # ğŸ¯ é‡‡æ ·ç­–ç•¥: 'uniform'(å®æ—¶) æˆ– 'emotion_peak'(é¢„æå–)
  
  # ğŸ¯ é¢„æå–ç‰¹å¾é…ç½® - æ¯ä¸ªæ¨¡æ€ç‹¬ç«‹æ§åˆ¶
  # use_preextracted_frame: æ ¹æ®frame_samplingè‡ªåŠ¨å†³å®šï¼ˆemotion_peakâ†’True, å…¶ä»–â†’Falseï¼‰
  use_preextracted_face: False   # âŒ Faceå®æ—¶åŠ è½½.npy
  use_preextracted_audio: False  # âŒ Audioå®æ—¶åŠ è½½
  # use_preextracted_au: False     # âŒ AUå®æ—¶CLIPç¼–ç 
  
  preextracted_root: './preextracted_features'  # é¢„æå–ç‰¹å¾æ ¹ç›®å½•
  visual_encoder: 'CLIP_VIT_LARGE'  # ç”¨äºæ„å»ºç‰¹å¾è·¯å¾„
  acoustic_encoder: 'HUBERT_LARGE'  # ç”¨äºæ„å»ºç‰¹å¾è·¯å¾„
  
  # ğŸ¯ AUå®æ—¶CLIPç¼–ç é…ç½®
  mer_factory_output: '/home/project/MER-Factory/output'  # AUæ¨¡æ€éœ€è¦
  use_au_clip_realtime: False  # âœ… AUä½¿ç”¨å®æ—¶CLIPç¼–ç 
  base_root: 'output/log_information/emercoarse_highlevelfilter4_outputhybird_bestsetup_bestfusion_lz_face_frame/peak/inference_result/results'
  ckpt_root: xxx
  ckpt_name: xxx
  test_epoch: xxx
  test_epochs: xxx-xxx
  skip_epoch: 1
  gpu: 0
