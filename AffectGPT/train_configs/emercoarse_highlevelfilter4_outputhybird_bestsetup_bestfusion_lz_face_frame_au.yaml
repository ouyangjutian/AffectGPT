## w Pre-fusion + Attention

model:
  arch: affectgpt
  model_type: pretrain_vicuna
  
  # é¢„æå–æ¨¡å¼ä¼˜åŒ– - å®Œå…¨ç§»é™¤ç¼–ç å™¨ä¾èµ–èŠ‚çœ2.3GBæ˜¾å­˜
  skip_encoders: False  # ğŸ¯ å¯ç”¨æ—¶å®Œå…¨è·³è¿‡CLIP_VIT_LARGEå’ŒHUBERT_LARGEåŠ è½½
  # skip_encoders: True # ğŸ¯ å¯ç”¨æ—¶å®Œå…¨è·³è¿‡CLIP_VIT_LARGEå’ŒHUBERT_LARGEã€CLIPåŠ è½½
  # é¢„æå–æ¨¡å¼ä¸‹çš„ç¼–ç å™¨é…ç½®ï¼ˆä»…ç”¨äºç»´åº¦è®¡ç®—ï¼Œä¸å®é™…åŠ è½½ï¼‰
  preextracted_visual_dim: 768   # CLIP_VIT_LARGEè¾“å‡ºç»´åº¦
  preextracted_acoustic_dim: 1024  # HUBERT_LARGEè¾“å‡ºç»´åº¦
  preextracted_au_dim: 512       # CLIP text encoder AUç‰¹å¾ç»´åº¦

  # Audio Q-Former and Video Q-Former
  frozen_video_proj: False
  frozen_video_Qformer: False
  frozen_audio_Qformer: False
  frozen_audio_proj: False
  frozen_au_Qformer: False     # AU Q-Formerå†»ç»“å‚æ•°
  frozen_au_proj: False        # AUæŠ•å½±å±‚å†»ç»“å‚æ•°
  frozen_multi_Qformer: False
  frozen_multi_llama_proj: False
  frozen_llm: False

  multi_fusion_type: attention # attention/qformer
  video_fusion_type: attention # qformer(default)/mean/attention
  audio_fusion_type: attention # qformer(default)/mean/attention
  image_fusion_type: mean       # token(default)/mean
  au_fusion_type: attention     # qformer/mean/attention

  # AffectGPT
  ckpt:   ""
  ckpt_2: ""

  # Other pre-trained models
  llama_model: "Qwen25"            # Vicuna
  acoustic_encoder: "HUBERT_LARGE" # IMAGEBIND
  visual_encoder: "CLIP_VIT_LARGE" # EVA_CLIP_G

  num_audio_query_token: 1
  num_video_query_token: 1
  num_multi_query_token: 1
  num_image_query_token: 1
  num_au_query_token: 1

  max_length: 1024

  vis_processor:
    train:
      name: "alpro_video_train"
      n_frms: 8
      image_size: 224
  text_processor:
    train:
      name: "blip_caption"
  img_processor:
    train:
      name: "blip2_image_train"
      image_size: 224
  

datasets:

  mercaptionplus:
    data_type: video
    face_or_frame: 'multiface_audio_face_frame_au_text' # é»˜è®¤æ˜¯è¯»å– frame or face  multi, face, audio, au, text
    label_type: 'hybird' # æ¨¡å‹æ˜¯é¢„æµ‹ description è¿˜æ˜¯ label å‘¢ï¼Ÿ
    
    # Frameé‡‡æ ·é…ç½® - å¯é€‰æ‹©å‡åŒ€é‡‡æ ·æˆ–å³°å€¼å¸§é‡‡æ ·
    frame_n_frms: 8        # Frameå¸§æ•°: 8(å‡åŒ€é‡‡æ ·) æˆ– 8(æ™ºèƒ½é‡‡æ ·)
    frame_sampling: 'uniform'  # Frameé‡‡æ ·ç­–ç•¥: 'uniform'(å‡åŒ€é‡‡æ ·) æˆ– 'emotion_peak'(AUæ™ºèƒ½é‡‡æ ·)
    mer_factory_output: '/home/project/MER-Factory/output'  # MER-Factoryè¾“å‡ºç›®å½• - ç”¨äºFrameæ¨¡æ€çš„emotion_peakæ¨¡å¼
    
    # é¢„æå–ç‰¹å¾é…ç½® - å‡å°‘æ˜¾å­˜æ¶ˆè€—
    use_preextracted_features: False   # ğŸ¯ ä¸å¯ç”¨é¢„æå–ç‰¹å¾æ¨¡å¼
    # use_preextracted_features: True   # ğŸ¯ å¯ç”¨é¢„æå–ç‰¹å¾æ¨¡å¼
    preextracted_root: './preextracted_features/mercaptionplus'  # é¢„æå–ç‰¹å¾æ ¹ç›®å½•
    visual_encoder: 'CLIP_VIT_LARGE'    # è§†è§‰ç¼–ç å™¨åç§°(ç”¨äºæ„å»ºç‰¹å¾è·¯å¾„)
    acoustic_encoder: 'HUBERT_LARGE'    # å£°å­¦ç¼–ç å™¨åç§°(ç”¨äºæ„å»ºç‰¹å¾è·¯å¾„)
    clips_per_video: 8                  # éŸ³é¢‘ç‰‡æ®µæ•°(ç”¨äºæ„å»ºç‰¹å¾è·¯å¾„)
    
    # é¢„æå–ç‰¹å¾ä½¿ç”¨è¯´æ˜:
    # 1. å®æ—¶å¤„ç†æ¨¡å¼(é»˜è®¤): use_preextracted_features: False
    #    - ä¼˜ç‚¹: çµæ´»æ€§é«˜ï¼Œæ”¯æŒæ•°æ®å¢å¼º
    #    - ç¼ºç‚¹: æ˜¾å­˜æ¶ˆè€—å¤§(~3GB)ï¼Œè®­ç»ƒæ…¢
    # 2. é¢„æå–ç‰¹å¾æ¨¡å¼: use_preextracted_features: True
    #    - ä¼˜ç‚¹: æ˜¾å­˜æ¶ˆè€—å°(~500MB)ï¼Œè®­ç»ƒå¿«3-5å€
    #    - ç¼ºç‚¹: éœ€è¦é¢„å…ˆæå–ç‰¹å¾ï¼Œçµæ´»æ€§è¾ƒä½
    
    # Frameé‡‡æ ·ç­–ç•¥ä½¿ç”¨è¯´æ˜:
    # 1. å‡åŒ€é‡‡æ ·(é»˜è®¤): frame_sampling: 'uniform', frame_n_frms: 8
    #    - ä»è§†é¢‘ä¸­å‡åŒ€é‡‡æ ·8å¸§
    # 2. AUæ™ºèƒ½é‡‡æ ·:    frame_sampling: 'emotion_peak', frame_n_frms: 8
    #    - æ ¹æ®AUå³°å€¼ä¿¡æ¯æ™ºèƒ½é€‰æ‹©8å¸§(éœ€è¦mer_factory_output)
    #    - ä¼šè‡ªåŠ¨è¯»å– mer_factory_output/{video_name}/{video_name}_au_analysis.json
    #    - å¦‚æœæ²¡æœ‰au_infoæ–‡ä»¶ï¼Œä¼šå›é€€åˆ°å‡åŒ€é‡‡æ ·
    # æ³¨æ„: Faceå§‹ç»ˆä¿æŒ8å¸§uniformé‡‡æ ·ä¸å˜


# 1. é‡‡ç”¨å¤§batch + ä¸‰ä¸ª 80g gpu
run:

  task: video_text_pretrain

  # optimizer
  lr_sched: "linear_warmup_cosine_lr"
  init_lr: 1e-5 # 3e-5 loss: nan -> é™ä½ loss åˆ° 2e-5 nan -> é™ä½ loss åˆ° 1e-5
  min_lr: 1e-5
  warmup_lr: 1e-6
  weight_decay: 0.05

  # max_epoch: 60 # 20 -> 100 å¤§æ¦‚å¯ä»¥è®©æ¯ä¸ªæ ·æœ¬è·‘2-3æ¬¡çš„æ ·å­ï¼Œå› ä¸º EMER-Coarse å¤§çº¦æœ‰ 11.5k samples => å¤§çº¦ 26h å°±å¯ä»¥è·‘å®Œï¼Œé€Ÿåº¦è¿˜okçš„
  # iters_per_epoch: 5000 # 1000
  # warmup_steps: 5000    # 1000
# æµ‹è¯•AU
  max_epoch: 30 # 20 -> 100 å¤§æ¦‚å¯ä»¥è®©æ¯ä¸ªæ ·æœ¬è·‘2-3æ¬¡çš„æ ·å­ï¼Œå› ä¸º EMER-Coarse å¤§çº¦æœ‰ 11.5k samples => å¤§çº¦ 26h å°±å¯ä»¥è·‘å®Œï¼Œé€Ÿåº¦è¿˜okçš„
  iters_per_epoch: 1000 # 1000
  warmup_steps: 1000    # 1000

  batch_size_train: 1 # 3 # è¿™æ˜¯å•ä¸€gpuçš„batch_sizeï¼›å®é™…batch_size = gpu_num * batch_size_train
  batch_size_eval:  1 # 3 # è¿™æ˜¯å•ä¸€gpuçš„batch_sizeï¼›å®é™…batch_size = gpu_num * batch_size_train

  seed: 42
  num_workers: 4

  amp: True # auto mixed precision, multiplication (fp16), addition (fp32)
  resume_ckpt_path: null # continue training from resume_ckpt_path

  evaluate: False 
  train_splits: ["train"]

  device: "cuda" 
  world_size: 1
  dist_url: "env://"
  distributed: True


# è¿™éƒ¨åˆ†æ”¾çš„æ˜¯ inference ç›¸å…³çš„
inference:

  task: video_text_pretrain

  vis_processor:
    train:
      name: "alpro_video_eval"
      n_frms: 8
      image_size: 224
  text_processor:
    train:
      name: "blip_caption"
  img_processor:
    train:
      name: "blip2_image_eval"
      image_size: 224

  ######################################################
  # NOTE: ä¸ºäº†é¿å…å‡ºé”™ï¼Œè®¾ç½®ä¸ºä¸ emercoarse æˆ–è€… mer2023 ä¸€è‡´å°±å¯ä»¥
  face_or_frame: 'xxx'
  ######################################################
  
  # Frameé‡‡æ ·é…ç½® - æ¨ç†æ—¶çš„Frameé‡‡æ ·ç­–ç•¥
  frame_n_frms: 8        # Frameå¸§æ•°: 8(å‡åŒ€é‡‡æ ·) æˆ– 1(å³°å€¼å¸§)
  frame_sampling: 'uniform'  # Frameé‡‡æ ·ç­–ç•¥: 'uniform'(å‡åŒ€é‡‡æ ·) æˆ– 'emotion_peak'(å³°å€¼å¸§)
  
  # é¢„æå–ç‰¹å¾é…ç½® - æ¨ç†æ—¶å‡å°‘æ˜¾å­˜æ¶ˆè€—
  use_preextracted_features: False   # ğŸ¯ ä¸å¯ç”¨é¢„æå–ç‰¹å¾æ¨¡å¼
  # use_preextracted_features: True   # ğŸ¯ å¯ç”¨é¢„æå–ç‰¹å¾æ¨¡å¼
  preextracted_root: './preextracted_features/mercaptionplus'  # é¢„æå–ç‰¹å¾æ ¹ç›®å½•
  mer_factory_output: '/home/project/MER-Factory/output'  # MER-Factoryè¾“å‡ºç›®å½•

  visual_encoder: 'CLIP_VIT_LARGE'    # è§†è§‰ç¼–ç å™¨åç§°(ç”¨äºæ„å»ºç‰¹å¾è·¯å¾„)
  acoustic_encoder: 'HUBERT_LARGE'    # å£°å­¦ç¼–ç å™¨åç§°(ç”¨äºæ„å»ºç‰¹å¾è·¯å¾„)
  clips_per_video: 8                  # éŸ³é¢‘ç‰‡æ®µæ•°(ç”¨äºæ„å»ºç‰¹å¾è·¯å¾„)
  
  # æ¨ç†æ—¶Frameé‡‡æ ·é…ç½®è¯´æ˜:
  # 1. å‡åŒ€é‡‡æ ·8å¸§: frame_n_frms: 8, frame_sampling: 'uniform'
  # 2. å³°å€¼å¸§1å¸§:   frame_n_frms: 1, frame_sampling: 'emotion_peak'
  # æ³¨æ„: Faceå§‹ç»ˆä¿æŒ8å¸§uniformé‡‡æ ·ä¸å˜

  base_root: 'output/log_information/emercoarse_highlevelfilter4_outputhybird_bestsetup_bestfusion_lz_face_frame_au/test/inference_result/results'

  # inference ckpt
  ckpt_root: xxx
  ckpt_name: xxx
  test_epoch: xxx
  test_epochs: xxx-xxx
  skip_epoch: 1 # only process on epoch%10=0 çš„éƒ¨åˆ† ckpt_3
  gpu: 0

