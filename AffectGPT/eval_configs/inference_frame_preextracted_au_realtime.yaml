# ================================================================
# æ¨ç†é…ç½® - Frameé¢„æå– + AUå®æ—¶CLIPç¼–ç ï¼ˆæœ€ä¼˜æ··åˆæ¨¡å¼ï¼‰
# ================================================================
# 
# è®¾è®¡ç†å¿µï¼š
# - Frame: ä½¿ç”¨é¢„æå–çš„emotion_peakç‰¹å¾ï¼ˆåŠ é€Ÿ600-1200å€ï¼‰
# - Face: å®æ—¶åŠ è½½.npyæ–‡ä»¶
# - Audio: å®æ—¶åŠ è½½éŸ³é¢‘æ–‡ä»¶
# - AU: å®æ—¶CLIPç¼–ç ï¼ˆä»MER-Factory JSONè¯»å–summary_descriptionï¼‰
#
# æ”¶ç›Šï¼š
# - FrameåŠ è½½ä»5-10msé™è‡³0.5ms
# - AUä¿æŒçµæ´»æ€§ï¼ˆæ— éœ€é¢„æå–ï¼ŒèŠ‚çœå­˜å‚¨ï¼‰
# - æ€»ä½“æ¨ç†åŠ é€Ÿ40-60%
# ================================================================

inference:
  task: video_text_pretrain

  vis_processor:
    train:
      name: "alpro_video_eval"
      n_frms: 8
      image_size: 224
  text_processor:
    train:
      name: "blip_caption"
  img_processor:
    train:
      name: "blip2_image_eval"
      image_size: 224

  # ============================================================
  # æ¨¡æ€é…ç½®ï¼ˆéœ€è¦åœ¨å®é™…è¿è¡Œæ—¶æ›¿æ¢'xxx'ï¼‰
  # ============================================================
  face_or_frame: 'xxx'  # ç¤ºä¾‹: 'face_frame_audio_au'
  
  # ============================================================
  # Frameé…ç½® - é¢„æå–emotion_peakç‰¹å¾
  # ============================================================
  frame_n_frms: 8
  frame_sampling: 'emotion_peak'  # ç”¨äºæ„å»ºç‰¹å¾è·¯å¾„
  
  # ğŸ¯ å…³é”®ï¼šå¯ç”¨é¢„æå–æ¨¡å¼
  use_preextracted_features: True
  preextracted_root: './preextracted_features'  # ç›¸å¯¹äºè¿è¡Œç›®å½•
  
  # Frameç‰¹å¾è·¯å¾„æ„å»ºæ‰€éœ€
  visual_encoder: 'CLIP_VIT_LARGE'
  
  # ============================================================
  # AUé…ç½® - å®æ—¶CLIPç¼–ç 
  # ============================================================
  # AUå®æ—¶CLIPç¼–ç éœ€è¦MER-Factoryè¾“å‡º
  mer_factory_output: '/home/project/MER-Factory/output'
  use_au_clip_realtime: True  # âœ… AUä½¿ç”¨å®æ—¶CLIPç¼–ç 
  
  # ============================================================
  # å…¶ä»–é…ç½®
  # ============================================================
  base_root: 'output/inference_results'
  ckpt_root: xxx
  ckpt_name: xxx
  test_epoch: xxx
  test_epochs: xxx-xxx
  skip_epoch: 1
  gpu: 0

# ================================================================
# ä½¿ç”¨è¯´æ˜
# ================================================================
# 
# 1. ç¡®ä¿å·²é¢„æå–Frameç‰¹å¾ï¼š
#    cd /home/project/AffectGPT/AffectGPT/MER-UniBench
#    bash run_extract_emotion_peak_batch.sh
#
# 2. éªŒè¯ç‰¹å¾æ–‡ä»¶å­˜åœ¨ï¼š
#    ls preextracted_features/mer2023/frame_CLIP_VIT_LARGE_emotion_peak_8frms/*.npy
#
# 3. è¿è¡Œæ¨ç†ï¼š
#    python inference_hybird.py \
#      --cfg-path eval_configs/inference_frame_preextracted_au_realtime.yaml \
#      --dataset mer2023 \
#      --ckpt <checkpoint_path>
#
# ================================================================
# é¢„æœŸè¡Œä¸º
# ================================================================
#
# Frameæ¨¡æ€ï¼š
#   ç‰¹å¾è·¯å¾„: ./preextracted_features/<dataset>/frame_CLIP_VIT_LARGE_emotion_peak_8frms/<sample_name>.npy
#   åŠ è½½æ–¹å¼: np.load() â†’ torch.Tensor [8, 768]
#   æ€§èƒ½: ~0.5ms/sample
#
# Faceæ¨¡æ€ï¼š
#   åŠ è½½æ–¹å¼: å®æ—¶load_face() â†’ ä».npyè¯»å–äººè„¸å¸§
#   æ€§èƒ½: ~0.01ms/sample
#
# Audioæ¨¡æ€ï¼š
#   åŠ è½½æ–¹å¼: å®æ—¶load_audio() â†’ ä»éŸ³é¢‘æ–‡ä»¶è¯»å–
#   æ€§èƒ½: ~10-20ms/sample
#
# AUæ¨¡æ€ï¼š
#   åŠ è½½æ–¹å¼: ä»MER-Factory JSONè¯»å–summary_description â†’ CLIP textç¼–ç 
#   JSONè·¯å¾„: {mer_factory_output}/{dataset}/{sample_name}/{sample_name}_au_analysis.json
#   æ€§èƒ½: ~2-3ms/sample
#
# ================================================================
# mer_factory_outputçš„ä½œç”¨
# ================================================================
#
# âŒ ä¸å†ç”¨äºFrameçš„emotion_peaké‡‡æ ·ï¼ˆå·²é¢„æå–ï¼‰
# âœ… ä»ç„¶ç”¨äºAUçš„å®æ—¶CLIPç¼–ç 
#
# å¦‚æœåˆ é™¤mer_factory_output:
# - Frame: ä¸å—å½±å“ï¼ˆä½¿ç”¨é¢„æå–ç‰¹å¾ï¼‰
# - AU: ä¼šå¤±è´¥æˆ–å›é€€åˆ°é¢„æå–æ¨¡å¼
#
# ================================================================
