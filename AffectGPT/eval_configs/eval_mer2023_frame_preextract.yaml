## MER2023æ¨ç†é…ç½® - Frameé¢„æå–æ··åˆæ¨¡å¼
## ç­–ç•¥ï¼šFrameä½¿ç”¨é¢„æå–çš„emotion_peakç‰¹å¾ï¼ŒFace/Audio/AUå®æ—¶å¤„ç†

model:
  arch: affectgpt
  model_type: pretrain_vicuna
  
  # ======================== ç¼–ç å™¨é…ç½® ========================
  skip_encoders: False  # âŒ ä¸è·³è¿‡ç¼–ç å™¨ï¼ˆéœ€è¦å®æ—¶å¤„ç†Face/Audioï¼‰
  
  # ======================== LLMé…ç½® ========================
  llama_model: "Qwen25"
  visual_encoder: "CLIP_VIT_LARGE"    # ç”¨äºFaceå®æ—¶ç¼–ç 
  acoustic_encoder: "HUBERT_LARGE"    # ç”¨äºAudioå®æ—¶ç¼–ç 
  
  # ======================== Query Tokené…ç½® ========================
  num_audio_query_token: 1
  num_video_query_token: 1  # Faceå’ŒFrameå…±ç”¨
  num_multi_query_token: 1
  num_image_query_token: 1
  num_au_query_token: 1
  
  # ======================== èåˆç­–ç•¥ ========================
  multi_fusion_type: attention
  video_fusion_type: attention
  audio_fusion_type: attention
  image_fusion_type: mean
  au_fusion_type: attention
  
  max_length: 1024
  
  # ======================== Processoré…ç½® ========================
  vis_processor:
    eval:
      name: "alpro_video_eval"
      n_frms: 8
      image_size: 224
  text_processor:
    eval:
      name: "blip_caption"
  img_processor:
    eval:
      name: "blip2_image_eval"
      image_size: 224

# ======================== æ•°æ®é›†é…ç½® ========================
datasets:
  mer2023:
    data_type: video
    face_or_frame: 'face_frame_audio_au'  # ä½¿ç”¨æ‰€æœ‰æ¨¡æ€
    
    # æ•°æ®è·¯å¾„
    video_root: '/home/project/Dataset/Emotion/MER2025/dataset/mer2023-dataset-process/video'
    audio_root: '/home/project/Dataset/Emotion/MER2025/dataset/mer2023-dataset-process/audio'
    face_root: '/home/project/Dataset/Emotion/MER2025/dataset/mer2023-dataset-process/openface_face'
    ann_paths: ['/home/project/Dataset/Emotion/MER2025/dataset/mer2023-dataset-process/label-6way.npz']
    
    # ğŸ¯ å…³é”®é…ç½®ï¼šæ··åˆæ¨¡å¼
    # Frame: ä½¿ç”¨é¢„æå–çš„emotion_peakç‰¹å¾
    frame_n_frms: 8
    frame_sampling: 'emotion_peak'
    use_preextracted_features: True  # â† å¯ç”¨é¢„æå–æ¨¡å¼
    preextracted_root: './preextracted_features/mer2023'  # â† Frameç‰¹å¾è·¯å¾„
    
    # ç¼–ç å™¨é…ç½®ï¼ˆç”¨äºæ„å»ºFrameç‰¹å¾è·¯å¾„ + å®æ—¶ç¼–ç Face/Audioï¼‰
    visual_encoder: 'CLIP_VIT_LARGE'
    acoustic_encoder: 'HUBERT_LARGE'
    
    # Face: å®æ—¶å¤„ç†ï¼ˆç³»ç»Ÿæ£€æµ‹åˆ°æ²¡æœ‰faceé¢„æå–ç‰¹å¾ä¼šè‡ªåŠ¨å›é€€åˆ°å®æ—¶ï¼‰
    # é»˜è®¤8å¸§uniformé‡‡æ ·
    
    # Audio: å®æ—¶å¤„ç†ï¼ˆç³»ç»Ÿæ£€æµ‹åˆ°æ²¡æœ‰audioé¢„æå–ç‰¹å¾ä¼šè‡ªåŠ¨å›é€€åˆ°å®æ—¶ï¼‰
    # é»˜è®¤8 clips
    
    # AU: å®æ—¶CLIPç¼–ç 
    mer_factory_output: '/home/project/MER-Factory/output'
    use_au_clip_realtime: True  # â† AUä½¿ç”¨å®æ—¶CLIPç¼–ç ï¼ˆä»JSONè¯»å–summary_descriptionï¼‰

# ======================== æ¨ç†é…ç½® ========================
inference:
  task: video_text_pretrain
  
  vis_processor:
    eval:
      name: "alpro_video_eval"
      n_frms: 8
      image_size: 224
  text_processor:
    eval:
      name: "blip_caption"
  img_processor:
    eval:
      name: "blip2_image_eval"
      image_size: 224
  
  # æ··åˆæ¨¡å¼é…ç½®
  face_or_frame: 'face_frame_audio_au'
  
  # Frameé…ç½®ï¼ˆé¢„æå–ï¼‰
  frame_n_frms: 8
  frame_sampling: 'emotion_peak'
  
  # MER-Factoryè·¯å¾„ï¼ˆAUå’Œemotion_peakéœ€è¦ï¼‰
  mer_factory_output: '/home/project/MER-Factory/output'
  
  # ğŸ¯ å…³é”®ï¼šå¯ç”¨é¢„æå–ä½†åªå½±å“Frame
  use_preextracted_features: True  # å¯ç”¨é¢„æå–æ¨¡å¼
  use_au_clip_realtime: True       # AUå®æ—¶CLIPç¼–ç 
  
  # è¾“å‡ºé…ç½®
  base_root: 'output/inference_results/mer2023'
  gpu: 0

# ======================== è¯„ä¼°æ•°æ®é›†é…ç½® ========================
evaluation_datasets_cfg:
  mer2023:
    eval_file_path: '/home/project/Dataset/Emotion/MER2025/dataset/mer2023-dataset-process/label-6way.npz'
    video_root: '/home/project/Dataset/Emotion/MER2025/dataset/mer2023-dataset-process/video'
    audio_root: '/home/project/Dataset/Emotion/MER2025/dataset/mer2023-dataset-process/audio'
    face_root: '/home/project/Dataset/Emotion/MER2025/dataset/mer2023-dataset-process/openface_face'
