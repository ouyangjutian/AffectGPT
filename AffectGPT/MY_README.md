# AffectGPT é¡¹ç›®å®Œæ•´æŒ‡å—

> **æœ€åæ›´æ–°**: 2024-11-23 21:30  
> **è¯´æ˜**: æœ¬æ–‡æ¡£æ•´åˆäº†æ‰€æœ‰åŠŸèƒ½æ¨¡å—çš„ä½¿ç”¨è¯´æ˜å’Œé…ç½®æŒ‡å—  
> **ç‰ˆæœ¬**: v2.4 - é›†æˆè®­ç»ƒå¯è§†åŒ–ï¼Œè‡ªåŠ¨ä¿å­˜æ›²çº¿å›¾

---

## ğŸ“š ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [AUå¤„ç†ä¸‰ç§æ¨¡å¼ï¼ˆé‡è¦ï¼‰](#auå¤„ç†ä¸‰ç§æ¨¡å¼é‡è¦)
3. [å¿«é€Ÿå¼€å§‹ - ä¸ä½¿ç”¨AU Agent](#å¿«é€Ÿå¼€å§‹---ä¸ä½¿ç”¨au-agent)
4. [è®­ç»ƒå¯è§†åŒ–ï¼ˆè‡ªåŠ¨ä¿å­˜æ›²çº¿å›¾ï¼‰](#è®­ç»ƒå¯è§†åŒ–è‡ªåŠ¨ä¿å­˜æ›²çº¿å›¾)
5. [Pre-Fusionå†…éƒ¨æœºåˆ¶è¯¦è§£](#pre-fusionå†…éƒ¨æœºåˆ¶è¯¦è§£)
6. [è®­ç»ƒé‡‡æ ·æœºåˆ¶è¯¦è§£](#è®­ç»ƒé‡‡æ ·æœºåˆ¶è¯¦è§£)
7. [AU Agent é›†æˆæŒ‡å—](#au-agent-é›†æˆæŒ‡å—)
8. [AUç‰¹å¾æå–ä¸è®­ç»ƒ](#auç‰¹å¾æå–ä¸è®­ç»ƒ)
9. [ç¼–ç å™¨è·³è¿‡ç­–ç•¥](#ç¼–ç å™¨è·³è¿‡ç­–ç•¥)
10. [Frameé‡‡æ ·ç­–ç•¥](#frameé‡‡æ ·ç­–ç•¥)
11. [é¢„æå–ç‰¹å¾ä¼˜åŒ–](#é¢„æå–ç‰¹å¾ä¼˜åŒ–)
12. [é…ç½®æ–‡ä»¶å¯¹æ¯”](#é…ç½®æ–‡ä»¶å¯¹æ¯”)
13. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## é¡¹ç›®æ¦‚è¿°

AffectGPTæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿï¼Œé›†æˆäº†è§†é¢‘ã€éŸ³é¢‘ã€æ–‡æœ¬å’ŒAUï¼ˆAction Unitï¼‰ç­‰å¤šç§æ¨¡æ€ã€‚

### æ ¸å¿ƒåŠŸèƒ½

- **å¤šæ¨¡æ€èåˆ**: Frame, Face, Audio, Text, AU
- **AU Agent**: ä»AUå€¼ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°
- **æ™ºèƒ½é‡‡æ ·**: åŸºäºæƒ…æ„Ÿå³°å€¼çš„å¸§é€‰æ‹©ç­–ç•¥
- **é¢„æå–ç‰¹å¾**: å‡å°‘è®­ç»ƒæ—¶çš„è®¡ç®—å¼€é”€

### ç³»ç»Ÿæ¶æ„

```
è§†é¢‘/éŸ³é¢‘è¾“å…¥
    â†“
OpenFace AUåˆ†æ
    â†“
AU Agentç”Ÿæˆæè¿°
    â†“
å¤šæ¨¡æ€ç‰¹å¾æå–
    â†“
AffectGPTæ¨ç†
    â†“
æƒ…æ„Ÿè¯†åˆ«ç»“æœ
```

---

## AUå¤„ç†ä¸‰ç§æ¨¡å¼ï¼ˆé‡è¦ï¼‰

> **æ ¸å¿ƒæ€æƒ³**: AU Agentåªåœ¨ç”ŸæˆJSONæ—¶ä½¿ç”¨ä¸€æ¬¡ï¼Œè®­ç»ƒå’Œæ¨ç†ç›´æ¥ä½¿ç”¨CLIPç¼–ç ç”Ÿæˆçš„æè¿°

### ğŸ“Š ä¸‰ç§æ¨¡å¼å¯¹æ¯”

| æ¨¡å¼ | AU Agent | CLIPåŠ è½½ | æ˜¾å­˜å ç”¨ | é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|---------|------|---------|
| **æ¨¡å¼1: é¢„æå–ç‰¹å¾** | âŒ ä¸ä½¿ç”¨ | âŒ ä¸éœ€è¦ | ğŸŸ¢ 15GB | âš¡ æœ€å¿« | âœ… è®­ç»ƒï¼ˆæ¨èï¼‰ |
| **æ¨¡å¼2: å®æ—¶CLIPç¼–ç ** | âŒ ä¸ä½¿ç”¨ | âœ… CLIP ViT-B/32 | ğŸŸ¡ 17GB | ğŸš€ è¾ƒå¿« | âœ… æ¨ç†ï¼ˆæ¨èï¼‰ |
| **æ¨¡å¼3: AU Agent** | âœ… ä½¿ç”¨ | âœ… CLIP + AU Agent | ğŸ”´ 30GB | ğŸŒ æ…¢ | âš ï¸ ä¸æ¨è |

### æ¨èæ–¹æ¡ˆ â­

**è®­ç»ƒ**: é¢„æå–ç‰¹å¾æ¨¡å¼  
**æ¨ç†**: å®æ—¶CLIPç¼–ç æ¨¡å¼

```yaml
# è®­ç»ƒé…ç½®
model:
  use_au_agent: False  # âŒ ä¸ä½¿ç”¨AU Agent
  skip_encoders: True  # âœ… è·³è¿‡ç¼–ç å™¨

datasets:
  mercaptionplus:
    use_preextracted_features: True  # âœ… ä½¿ç”¨é¢„æå–ç‰¹å¾
    preextracted_root: './preextracted_features/mercaptionplus'

# æ¨ç†é…ç½®
model:
  use_au_agent: False  # âŒ ä¸ä½¿ç”¨AU Agent
  skip_encoders: False  # âŒ ä¸è·³è¿‡ï¼ˆéœ€è¦å®æ—¶ç¼–ç ï¼‰

inference:
  use_au_clip_realtime: True  # âœ… å®æ—¶CLIPç¼–ç 
  mer_factory_output: '/home/project/MER-Factory/output'
```

**ä¼˜ç‚¹**:
- âœ… è®­ç»ƒæœ€å¿«ï¼ˆé¢„æå–ï¼‰
- âœ… æ¨ç†çµæ´»ï¼ˆå®æ—¶ç¼–ç ï¼‰
- âœ… æ˜¾å­˜å ç”¨å°ï¼ˆ15-17GB vs 30GBï¼‰
- âœ… ä¸éœ€è¦AU Agentæ¨¡å‹

---

## å¿«é€Ÿå¼€å§‹ - ä¸ä½¿ç”¨AU Agent

### ğŸ¯ å®Œæ•´å·¥ä½œæµç¨‹

```
æ­¥éª¤1: ç”ŸæˆAUæè¿°ï¼ˆä½¿ç”¨AU Agentï¼Œåªè¿è¡Œä¸€æ¬¡ï¼‰
MER-Factory â†’ AU Agent â†’ JSONæ–‡ä»¶ï¼ˆå«summary_descriptionï¼‰

æ­¥éª¤2: æå–è®­ç»ƒç‰¹å¾ï¼ˆåªè¿è¡Œä¸€æ¬¡ï¼‰
JSON â†’ CLIPç¼–ç  â†’ .npyæ–‡ä»¶

æ­¥éª¤3: è®­ç»ƒï¼ˆä¸ä½¿ç”¨AU Agentï¼‰
.npyæ–‡ä»¶ â†’ ç›´æ¥åŠ è½½ â†’ AffectGPTè®­ç»ƒ

æ­¥éª¤4: æ¨ç†ï¼ˆä¸ä½¿ç”¨AU Agentï¼‰
JSON â†’ summary_description â†’ CLIPå®æ—¶ç¼–ç  â†’ AffectGPTæ¨ç†
```

### æ­¥éª¤1: ç”ŸæˆAUæè¿°

```bash
cd /home/project/MER-Factory

# æ‰¹é‡å¤„ç†æ‰€æœ‰æ•°æ®é›†
python batch_extract_au_multi_datasets.py \
    --mode 2 \              # æµ‹è¯•é›†æ¨¡å¼
    --gen-method 1 \        # AU Agentç”Ÿæˆ
    --datasets 1            # å…¨éƒ¨10ä¸ªæ•°æ®é›†

# æˆ–åå°è¿è¡Œ
nohup python batch_extract_au_multi_datasets.py --mode 2 --gen-method 1 --datasets 1 > batch.log 2>&1 &
```

### æ­¥éª¤2: æå–CLIPç‰¹å¾ï¼ˆè®­ç»ƒç”¨ï¼‰

```bash
cd /home/project/AffectGPT/AffectGPT

# ä½¿ç”¨æä¾›çš„è„šæœ¬
bash extract_mercaptionplus_features.sh

# æˆ–æ‰‹åŠ¨è¿è¡Œ
python extract_multimodal_features_precompute.py \
    --dataset mercaptionplus \
    --modality au \
    --device cuda:0 \
    --mer-factory-output /home/project/MER-Factory/output/MERCaptionPlus \
    --csv_path /path/to/train.csv \
    --csv_column name \
    --save_root ./preextracted_features
```

### æ­¥éª¤3: è®­ç»ƒï¼ˆä¸ä½¿ç”¨AU Agentï¼‰

```bash
cd /home/project/AffectGPT/AffectGPT

# ä½¿ç”¨æ¨èé…ç½®æ–‡ä»¶
python train.py \
    --cfg-path train_configs/recommended_train_with_preextracted_au.yaml
```

**é…ç½®è¦ç‚¹**:
```yaml
model:
  use_au_agent: False  # âŒ ä¸ä½¿ç”¨AU Agent
  skip_encoders: True  # âœ… è·³è¿‡ç¼–ç å™¨åŠ è½½

datasets:
  mercaptionplus:
    use_preextracted_features: True  # âœ… ä½¿ç”¨é¢„æå–ç‰¹å¾
    preextracted_root: './preextracted_features/mercaptionplus'
```

### æ­¥éª¤4: æ¨ç†ï¼ˆä¸ä½¿ç”¨AU Agentï¼‰

```bash
# ä½¿ç”¨æ¨èé…ç½®æ–‡ä»¶
python inference.py \
    --cfg-path eval_configs/recommended_inference_with_clip_realtime.yaml
```

**é…ç½®è¦ç‚¹**:
```yaml
model:
  use_au_agent: False  # âŒ ä¸ä½¿ç”¨AU Agent
  skip_encoders: False  # âŒ æ¨ç†ä¸è·³è¿‡

inference:
  use_au_clip_realtime: True  # âœ… å®æ—¶CLIPç¼–ç 
  mer_factory_output: '/home/project/MER-Factory/output'
  # æ³¨æ„ï¼šè·¯å¾„ä¼šè‡ªåŠ¨è¡¥å……æ•°æ®é›†åç§°ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å®š
  # å®é™…è¯»å–è·¯å¾„: {mer_factory_output}/{dataset}/{video_name}/{video_name}_au_analysis.json
  # ä¾‹å¦‚: /home/project/MER-Factory/output/MER2023/sample_00000905/sample_00000905_au_analysis.json
```

### âš ï¸ è·¯å¾„é…ç½®é‡è¦è¯´æ˜

**MER-Factoryè¾“å‡ºè·¯å¾„ç»“æ„**:
```
/home/project/MER-Factory/output/
â”œâ”€â”€ MER2023/              # æ•°æ®é›†åç§°ï¼ˆè‡ªåŠ¨ä»datasetç±»è·å–ï¼‰
â”‚   â””â”€â”€ sample_00000905/  # è§†é¢‘åç§°
â”‚       â””â”€â”€ sample_00000905_au_analysis.json
â”œâ”€â”€ MER2024/
â”‚   â””â”€â”€ sample_xxx/
â”‚       â””â”€â”€ sample_xxx_au_analysis.json
â””â”€â”€ MERCaptionPlus/
    â””â”€â”€ samplenew3_00000120/
        â””â”€â”€ samplenew3_00000120_au_analysis.json
```

**é…ç½®æ–¹å¼**:
```yaml
# âœ… æ­£ç¡®ï¼šåªé…ç½®æ ¹è·¯å¾„
inference:
  mer_factory_output: '/home/project/MER-Factory/output'

# âŒ é”™è¯¯ï¼šä¸è¦æ‰‹åŠ¨æ·»åŠ æ•°æ®é›†åç§°
inference:
  mer_factory_output: '/home/project/MER-Factory/output/MER2023'  # ä¼šå¯¼è‡´è·¯å¾„é‡å¤
```

**è‡ªåŠ¨è·¯å¾„æ„å»º**:
- æ¨ç†æ—¶ï¼Œä»£ç ä¼šè‡ªåŠ¨ä»æ•°æ®é›†ç±»è·å– `self.dataset` å±æ€§ï¼ˆå¦‚ `'MER2023'`ï¼‰
- è‡ªåŠ¨æ„å»ºå®Œæ•´è·¯å¾„: `{mer_factory_output}/{dataset}/{video_name}/{video_name}_au_analysis.json`
- æ”¯æŒå¤šä¸ªæ•°æ®é›†æ¨ç†ï¼Œæ¯ä¸ªæ•°æ®é›†ä½¿ç”¨ç›¸åŒçš„ `mer_factory_output` æ ¹è·¯å¾„

### ğŸ“Š æ€§èƒ½ä¼˜åŠ¿

| æŒ‡æ ‡ | æ–°æ–¹æ¡ˆ | æ—§æ–¹æ¡ˆï¼ˆAU Agentï¼‰ | æå‡ |
|------|--------|-------------------|------|
| **æ˜¾å­˜å ç”¨** | 15-17GB | 30GB | èŠ‚çœ43% |
| **è®­ç»ƒé€Ÿåº¦** | åŸºå‡† | æ…¢3å€ | æå‡3å€ |
| **æ¨ç†é€Ÿåº¦** | åŸºå‡† | æ…¢3å€ | æå‡3å€ |
| **GPUè¦æ±‚** | 1x 20GB | 2x 20GB | èŠ‚çœ1å¼ å¡ |

---

## AU Agent é›†æˆæŒ‡å—

### âœ… AU AgentåŠŸèƒ½

- âœ… ä»AUå€¼ç”Ÿæˆå®¢è§‚çš„è‚Œè‚‰è¿åŠ¨æè¿°ï¼ˆæ— æƒ…æ„Ÿè¯ï¼‰
- âœ… æ”¯æŒä»MER-Factory JSONåŠ è½½AU result
- âœ… è®­ç»ƒå’Œæ¨ç†ç»Ÿä¸€ä½¿ç”¨AU Agent
- âœ… æ˜¾å­˜ä¼˜åŒ–ï¼šæ”¯æŒå•ç‹¬GPUè¿è¡ŒAU Agent

### ğŸ“Š å®Œæ•´æ•°æ®æµ

#### **è®­ç»ƒé˜¶æ®µ**
```
1. MER-Factoryç”ŸæˆAU result (OpenFace only)
   â””â”€â”€ {sample_name}_au_analysis.json

2. base_dataset.pyåŠ è½½
   â””â”€â”€ _load_au_result_from_mer_factory()
   â””â”€â”€ è¿”å›: {'active_aus': {...}, 'au_description': "..."}

3. conversation_video.pyå¤„ç†
   â””â”€â”€ postprocess_au() ä½¿ç”¨AU Agent
   â””â”€â”€ AU Agentç”ŸæˆFacial Contentæè¿°
       è¾“å…¥: AU values + AU descriptions (åªæœ‰AU result)
       è¾“å‡º: å®¢è§‚çš„è‚Œè‚‰è¿åŠ¨æè¿°ï¼ˆæ— æƒ…æ„Ÿè¯ï¼‰
   â””â”€â”€ è½¬æ¢ä¸ºtext tokens â†’ è¾“å…¥AffectGPTè®­ç»ƒ
```

#### **æ¨ç†é˜¶æ®µ**
```
1. MER-Factoryç”ŸæˆAU result (åŒè®­ç»ƒ)
2. base_dataset.pyåŠ è½½ (åŒè®­ç»ƒ)
3. conversation_video.pyå¤„ç† (åŒè®­ç»ƒ)
   â””â”€â”€ AU Agentç”Ÿæˆæè¿° â†’ AffectGPTæ¨ç† â†’ æƒ…æ„Ÿè¯†åˆ«ç»“æœ
```

### ğŸš€ ä½¿ç”¨æ­¥éª¤

#### æ­¥éª¤1: ç”ŸæˆAUåˆ†æç»“æœ

```bash
cd /home/project/MER-Factory

# ä½¿ç”¨æ‰¹å¤„ç†è„šæœ¬ï¼ˆæ¨èï¼‰
python batch_extract_au_multi_datasets.py \
    --mode 2 \              # æµ‹è¯•é›†æ¨¡å¼
    --gen-method 1 \        # AU Agentç”Ÿæˆ
    --datasets 1            # å…¨éƒ¨æ•°æ®é›†

# åå°è¿è¡Œ
nohup python batch_extract_au_multi_datasets.py > batch_run.log 2>&1 &
```

**è¾“å‡ºç›®å½•ç»“æ„**:
```
/home/project/MER-Factory/output/
â”œâ”€â”€ MERCaptionPlus/
â”‚   â”œâ”€â”€ samplenew3_00000120/
â”‚   â”‚   â””â”€â”€ samplenew3_00000120_au_analysis.json
â”œâ”€â”€ MER2023/
â”œâ”€â”€ MER2024/
â””â”€â”€ ... (å…¶ä»–8ä¸ªæ•°æ®é›†)
```

**JSONæ–‡ä»¶ç»“æ„**:
```json
{
  "per_frame_au_descriptions": [
    {
      "frame": 1,
      "timestamp": 0.0,
      "au_description": "Upper lip raiser (intensity: 1.06), ...",
      "active_aus": {"AU10_r": 1.06, "AU12_r": 1.14, ...},
      "is_peak_frame": false,
      "fine_grained_description": "system\n...\nuser\n...\nassistant\nå®Œæ•´æè¿°"
    }
  ],
  "au_info": {
    "total_frames": 44,
    "peak_frames": [{"peak_index": 42, ...}]
  },
  "summary_description": {
    "1": "çº¯å‡€æè¿°ï¼ˆä»…assistantéƒ¨åˆ†ï¼Œç”¨äºCLIPç‰¹å¾æå–ï¼‰",
    "35": "The expression features moderate brow lowering...",
    "69": "The facial expression demonstrates..."
  }
}
```

#### æ­¥éª¤2: é…ç½®è®­ç»ƒ/æ¨ç†

**è®­ç»ƒé…ç½®** (`train_configs/*.yaml`):
```yaml
model:
  arch: affectgpt
  
  # AU Agenté…ç½®
  use_au_agent: True  # è®­ç»ƒæ—¶ä½¿ç”¨AU Agent
  au_agent_base_model: "/home/project/Dataset/Emotion/tools/transformer/LLM/Qwen2.5-7B-Instruct"
  au_agent_lora_weights: "/home/project/AffectGPT/AffectGPT/output/au_agent_qwen2.5_7b_lora"
  au_agent_device: "cuda:1"  # AU Agentç‹¬å GPU 1

datasets:
  mercaptionplus:
    # MER-Factoryè¾“å‡ºè·¯å¾„
    mer_factory_output: '/home/project/MER-Factory/output'
    
    # Frameé‡‡æ ·é…ç½®
    frame_n_frms: 8
    frame_sampling: 'uniform'  # æˆ– 'emotion_peak'
```

**æ¨ç†é…ç½®** (`eval_configs/*.yaml`):
```yaml
model:
  use_au_agent: True  # æ¨ç†æ—¶ä¹Ÿä½¿ç”¨AU Agent
  au_agent_base_model: "..."
  au_agent_lora_weights: "..."

datasets:
  mer2023:  # æˆ–å…¶ä»–æ•°æ®é›†
    mer_factory_output: '/home/project/MER-Factory/output'
```

#### æ­¥éª¤3: è®­ç»ƒ/æ¨ç†

```bash
# è®­ç»ƒ
python train.py --cfg-path train_configs/xxx.yaml

# æ¨ç†
python inference.py --cfg-path eval_configs/xxx.yaml
```

### âš ï¸ æ˜¾å­˜è¦æ±‚

| ç»„ä»¶ | æ˜¾å­˜éœ€æ±‚ | GPUåˆ†é… |
|------|---------|---------|
| AffectGPT (7B) | ~15GB | cuda:0 |
| AU Agent (7B + LoRA) | ~15GB | cuda:1 |
| **æ€»è®¡** | **~30GB** | 2x GPU |

**æ¨èé…ç½®**:
- å•å¡è®­ç»ƒ: 80GB A100
- åŒå¡è®­ç»ƒ: 2x 40GB A100 (AffectGPTåœ¨GPU 0, AU Agentåœ¨GPU 1)
- DDPè®­ç»ƒ: ä½¿ç”¨GPU 0,2,3è®­ç»ƒï¼ŒAU Agentåœ¨GPU 1

---

## AUç‰¹å¾æå–ä¸è®­ç»ƒ

### ğŸ”„ æ›´æ–°è¯´æ˜ (2024-11-23)

- **JSONå­—æ®µ**: `summary_description`
- **æè¿°å†…å®¹**: `summary_description` åªåŒ…å«çº¯å‡€çš„assistantéƒ¨åˆ†
- **ç”¨é€”**: ä¸“é—¨ç”¨äºCLIPç‰¹å¾æå–

### ğŸ“ ç›®å½•ç»“æ„

#### MER-Factoryè¾“å‡º
```
/home/project/MER-Factory/output/
â”œâ”€â”€ MERCaptionPlus/
â”‚   â”œâ”€â”€ samplenew3_00000120/
â”‚   â”‚   â””â”€â”€ samplenew3_00000120_au_analysis.json
â””â”€â”€ ... (å…¶ä»–9ä¸ªæ•°æ®é›†)
```

#### AffectGPTé¢„æå–ç‰¹å¾
```
/home/project/AffectGPT/AffectGPT/preextracted_features/
â””â”€â”€ mercaptionplus/
    â””â”€â”€ au_CLIP_VITB32_8frms/
        â”œâ”€â”€ samplenew3_00000120.npy  # [N, 512]
        â””â”€â”€ ...
```

### ğŸš€ ç‰¹å¾æå–æµç¨‹

#### æ­¥éª¤1: æµ‹è¯•å•æ ·æœ¬

```bash
cd /home/project/AffectGPT/AffectGPT

# æµ‹è¯•å•ä¸ªæ ·æœ¬çš„å®Œæ•´æµç¨‹
python test_single_sample.py --sample samplenew3_00000120
```

#### æ­¥éª¤2: æ‰¹é‡æå–CLIPç‰¹å¾

```bash
# æ–¹å¼1: ä½¿ç”¨æä¾›çš„è„šæœ¬ï¼ˆæ¨èï¼‰
bash extract_mercaptionplus_features.sh

# æ–¹å¼2: æ‰‹åŠ¨æŒ‡å®šå‚æ•°
python extract_multimodal_features_precompute.py \
    --dataset mercaptionplus \
    --modality au \
    --device cuda:0 \
    --mer-factory-output /home/project/MER-Factory/output/MERCaptionPlus \
    --csv_path /path/to/train_file.csv \
    --csv_column name \
    --save_root ./preextracted_features
```

**å‚æ•°è¯´æ˜**:

| å‚æ•° | è¯´æ˜ | ç¤ºä¾‹å€¼ |
|------|------|--------|
| `--dataset` | æ•°æ®é›†åç§° | `mercaptionplus` |
| `--modality` | æå–æ¨¡æ€ | `au` (ä»…AUç‰¹å¾) |
| `--device` | GPUè®¾å¤‡ | `cuda:0` |
| `--mer-factory-output` | MER-Factoryè¾“å‡ºç›®å½• | `/home/project/MER-Factory/output/MERCaptionPlus` |
| `--csv_path` | æ ·æœ¬åˆ—è¡¨CSV | åŒ…å«nameåˆ—çš„CSVæ–‡ä»¶ |
| `--save_root` | ä¿å­˜ç›®å½• | `./preextracted_features` |

#### æ­¥éª¤3: éªŒè¯æ•°æ®å®Œæ•´æ€§

```bash
# è¿è¡ŒéªŒè¯è„šæœ¬
python verify_au_pipeline.py
```

**éªŒè¯å†…å®¹**:
1. âœ… MER-Factory JSONæ–‡ä»¶å­˜åœ¨
2. âœ… åŒ…å« `summary_description` å­—æ®µ
3. âœ… CLIPç‰¹å¾æ–‡ä»¶å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®
4. âœ… æè¿°æ•°é‡ä¸ç‰¹å¾ç»´åº¦åŒ¹é…

#### æ­¥éª¤4: è®­ç»ƒä½¿ç”¨é¢„æå–ç‰¹å¾

**ä¿®æ”¹è®­ç»ƒé…ç½®**:
```yaml
datasets:
  mercaptionplus:
    # å¯ç”¨é¢„æå–ç‰¹å¾æ¨¡å¼
    use_preextracted_features: True
    preextracted_root: './preextracted_features/mercaptionplus'
    
    # ç¼–ç å™¨é…ç½®ï¼ˆç”¨äºæ„å»ºç‰¹å¾è·¯å¾„ï¼‰
    visual_encoder: 'CLIP_VIT_LARGE'
    acoustic_encoder: 'HUBERT_LARGE'
    clips_per_video: 8
```

**è¿è¡Œè®­ç»ƒ**:
```bash
python train.py --cfg-path train_configs/your_config.yaml
```

### ğŸ“Š æ•°æ®åŠ è½½é€»è¾‘

`base_dataset.py` åŠ è½½ä¼˜å…ˆçº§:

1. **é¢„æå–æ¨¡å¼** (`use_preextracted_features=True`):
   ```python
   # ä» .npy æ–‡ä»¶åŠ è½½CLIPç‰¹å¾
   au_feat_path = preextracted_root/au_CLIP_VITB32_8frms/sample_name.npy
   au = torch.from_numpy(np.load(au_feat_path))  # [N, 512]
   ```

2. **AU Agentæ¨¡å¼** (`use_preextracted_features=False`):
   ```python
   # ä» MER-Factory JSONå®æ—¶åŠ è½½ï¼Œä½¿ç”¨AU Agentæ¨ç†
   au = self._load_au_result_from_mer_factory(video_name)
   ```

### ğŸ¯ æ€§èƒ½å¯¹æ¯”

| æ¨¡å¼ | è®­ç»ƒé€Ÿåº¦ | æ˜¾å­˜å ç”¨ | ç‰¹å¾ä¸€è‡´æ€§ |
|------|---------|---------|-----------|
| é¢„æå–ç‰¹å¾ | âš¡ å¿« | ğŸ’¾ ä½ | âœ… ä¸€è‡´ |
| å®æ—¶ç”Ÿæˆ | ğŸŒ æ…¢ | ğŸ”¥ é«˜ | âš ï¸ å¯èƒ½å˜åŒ– |

**å»ºè®®**:
- âœ… è®­ç»ƒä½¿ç”¨é¢„æå–ç‰¹å¾ï¼ˆé€Ÿåº¦å¿«ã€æ˜¾å­˜ä½ï¼‰
- â¸ï¸ æ¨ç†å¯å®æ—¶ç”Ÿæˆï¼ˆæ›´çµæ´»ï¼‰

---

## Frameé‡‡æ ·ç­–ç•¥

### ğŸ“‹ é‡‡æ ·ç­–ç•¥å¯¹æ¯”

#### 1. Uniformé‡‡æ ·ï¼ˆå‡åŒ€é‡‡æ ·ï¼‰
```yaml
frame_n_frms: 8
frame_sampling: 'uniform'
```

**ç‰¹ç‚¹**:
- ä»è§†é¢‘ä¸­å‡åŒ€é‡‡æ ·8å¸§
- è¦†ç›–æ•´ä¸ªè§†é¢‘æ—¶é•¿
- é€‚åˆè¡¨æƒ…å˜åŒ–å¹³ç¼“çš„è§†é¢‘

#### 2. Emotion Peaké‡‡æ ·ï¼ˆæƒ…æ„Ÿå³°å€¼é‡‡æ ·ï¼‰
```yaml
frame_n_frms: 8
frame_sampling: 'emotion_peak'
```

**ç‰¹ç‚¹**:
- åŸºäºAUå³°å€¼å¸§æ™ºèƒ½é€‰æ‹©
- é€‰æ‹©è¡¨æƒ…å˜åŒ–æœ€æ˜æ˜¾çš„åŒºåŸŸ
- éœ€è¦MER-Factoryçš„ `au_info` æ•°æ®

**é‡‡æ ·é€»è¾‘**:
```python
def calculate_smart_frame_indices(au_info, n_frms=8):
    """æ™ºèƒ½é€‰æ‹©å…³é”®å¸§"""
    peak_frames = au_info.get('peak_frames', [])
    
    if not peak_frames:
        # æ— å³°å€¼ï¼Œå›é€€åˆ°å‡åŒ€é‡‡æ ·
        return uniform_sample(total_frames, n_frms)
    
    # 1. å³°å€¼å¸§å¿…é€‰
    selected_indices.add(peak_index)
    
    # 2. å³°å€¼å‰åå¸§ï¼ˆå¦‚ Â±3å¸§ï¼‰
    for offset in range(-3, 4):
        if 0 <= peak_index + offset < total_frames:
            selected_indices.add(peak_index + offset)
    
    # 3. è¡¥å……å‡åŒ€é‡‡æ ·çš„å¸§
    while len(selected_indices) < n_frms:
        # ä»æœªé€‰æ‹©çš„å¸§ä¸­å‡åŒ€é‡‡æ ·
        ...
    
    return sorted(selected_indices)[:n_frms]
```

### ğŸ”§ é…ç½®ç¤ºä¾‹

#### è®­ç»ƒé…ç½®
```yaml
datasets:
  mercaptionplus:
    # Frameé‡‡æ ·
    frame_n_frms: 8
    frame_sampling: 'uniform'  # è®­ç»ƒå»ºè®®uniform
    
    # MER-Factoryè·¯å¾„ï¼ˆemotion_peakéœ€è¦ï¼‰
    mer_factory_output: '/home/project/MER-Factory/output'
```

#### æ¨ç†é…ç½®
```yaml
datasets:
  mer2023:
    # Frameé‡‡æ ·
    frame_n_frms: 8
    frame_sampling: 'emotion_peak'  # æ¨ç†å¯ç”¨emotion_peak
    
    # MER-Factoryè·¯å¾„
    mer_factory_output: '/home/project/MER-Factory/output'
```

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **emotion_peakéœ€è¦au_info**
   - ç¡®ä¿MER-Factoryå·²ç”Ÿæˆ `au_info` å­—æ®µ
   - å¦‚æœç¼ºå¤±ï¼Œè‡ªåŠ¨å›é€€åˆ°uniformé‡‡æ ·

2. **é‡‡æ ·æ•°é‡**
   - æ¨è `frame_n_frms: 8` (é»˜è®¤)
   - å³°å€¼æ¨¡å¼å¯èƒ½é€‰æ‹©å°‘äº8å¸§ï¼ˆå¦‚æœå³°å€¼åŒºåŸŸè¾ƒå°ï¼‰

3. **å…¼å®¹æ€§**
   - æ‰€æœ‰é‡‡æ ·ç­–ç•¥éƒ½å…¼å®¹é¢„æå–ç‰¹å¾æ¨¡å¼
   - AUæ¨¡æ€ç‹¬ç«‹äºFrameé‡‡æ ·ç­–ç•¥

---

## ç¼–ç å™¨è·³è¿‡ç­–ç•¥

### ğŸ“Š `skip_encoders` å‚æ•°è¯¦è§£

`skip_encoders` æ§åˆ¶**æ¨¡å‹åˆå§‹åŒ–æ—¶**æ˜¯å¦åŠ è½½ç¼–ç å™¨ï¼š
- CLIP ViT-Largeï¼ˆFrame/Faceç‰¹å¾ï¼‰
- HuBERT-Largeï¼ˆAudioç‰¹å¾ï¼‰

**æ³¨æ„**: ä¸å½±å“AUç‰¹å¾ç¼–ç ï¼ˆAUç”¨CLIP ViT-B/32æˆ–é¢„æå–.npyï¼‰

### ğŸ”„ é…ç½®ç­–ç•¥å¯¹æ¯”

| ç­–ç•¥ | è®­ç»ƒskip_encoders | æ¨ç†skip_encoders | é€‚ç”¨åœºæ™¯ |
|------|------------------|------------------|---------|
| **ç­–ç•¥1: åŸå§‹** | False | False | å¼€å‘æµ‹è¯• |
| **ç­–ç•¥2: æ¨è** â­ | True | False | ç”Ÿäº§ç¯å¢ƒ |
| **ç­–ç•¥3: æè‡´** | True | True | å›ºå®šæ•°æ®é›† |

### æ¨èç­–ç•¥ â­

**è®­ç»ƒé…ç½®**:
```yaml
model:
  skip_encoders: True  # âœ… è·³è¿‡ï¼ˆé…åˆé¢„æå–ç‰¹å¾ï¼‰
  
datasets:
  mercaptionplus:
    use_preextracted_features: True
```

**æ¨ç†é…ç½®**:
```yaml
model:
  skip_encoders: False  # âŒ ä¸è·³è¿‡ï¼ˆä¿æŒçµæ´»æ€§ï¼‰
  
inference:
  use_au_clip_realtime: True  # AUå®æ—¶CLIPç¼–ç 
```

### å½±å“åˆ†æ

#### è®­ç»ƒæ—¶ `skip_encoders: True`
- âœ… èŠ‚çœ3GBæ˜¾å­˜
- âœ… å¿…é¡»ä½¿ç”¨é¢„æå–ç‰¹å¾
- âœ… è®­ç»ƒé€Ÿåº¦æœ€å¿«

#### æ¨ç†æ—¶ `skip_encoders: False`
- âœ… å¯å®æ—¶ç¼–ç æ–°æ•°æ®
- âœ… çµæ´»æ€§é«˜
- âš ï¸ æ˜¾å­˜ç¨å¤§ï¼ˆ17-18GBï¼‰

---

## é…ç½®æ–‡ä»¶å¯¹æ¯”

### ğŸ“‹ AUé…ç½®é¡¹æ¸…å•

| é…ç½®é¡¹ | è¯´æ˜ | æ¨èå€¼ |
|--------|------|--------|
| `preextracted_au_dim` | AUç‰¹å¾ç»´åº¦ | 512 |
| `frozen_au_Qformer` | AU Q-Formerå†»ç»“ | False |
| `frozen_au_proj` | AUæŠ•å½±å±‚å†»ç»“ | False |
| `au_fusion_type` | AUèåˆæ–¹å¼ | attention |
| `num_au_query_token` | AU query tokenæ•° | 1 |

### é…ç½®æ–‡ä»¶å¯¹æ¯”

#### åŸå§‹é…ç½® (`emercoarse_highlevelfilter4_outputhybird_bestsetup_bestfusion_lz_face_frame_au.yaml`)

```yaml
model:
  skip_encoders: False
  use_au_agent: True  # âœ… ä½¿ç”¨AU Agent
  
datasets:
  mercaptionplus:
    use_preextracted_features: False
```

#### æ–°è®­ç»ƒé…ç½® (`recommended_train_with_preextracted_au.yaml`) â­

```yaml
model:
  skip_encoders: True  # âœ… è®­ç»ƒæ—¶è·³è¿‡
  use_au_agent: False  # âŒ ä¸ä½¿ç”¨AU Agent
  
datasets:
  mercaptionplus:
    use_preextracted_features: True  # âœ… é¢„æå–
```

#### æ–°æ¨ç†é…ç½® (`recommended_inference_with_clip_realtime.yaml`) â­

```yaml
model:
  skip_encoders: False  # âŒ æ¨ç†ä¸è·³è¿‡
  use_au_agent: False  # âŒ ä¸ä½¿ç”¨AU Agent
  
inference:
  use_au_clip_realtime: True  # âœ… å®æ—¶CLIPç¼–ç 
```

### æ€§èƒ½å¯¹æ¯”

| é…ç½® | æ˜¾å­˜ | è®­ç»ƒé€Ÿåº¦ | æ¨ç†é€Ÿåº¦ | çµæ´»æ€§ |
|------|------|---------|---------|--------|
| åŸå§‹é…ç½® | 30GB | åŸºå‡† | åŸºå‡† | é«˜ |
| æ–°è®­ç»ƒé…ç½® | 15GB | å¿«3å€ | - | ä½ |
| æ–°æ¨ç†é…ç½® | 17GB | - | å¿«3å€ | é«˜ |

---

## é¢„æå–ç‰¹å¾ä¼˜åŒ–

### ğŸ¯ ä¼˜åŒ–ç›®æ ‡

- âš¡ **åŠ é€Ÿè®­ç»ƒ**: é¿å…æ¯ä¸ªepoché‡å¤ç¼–ç 
- ğŸ’¾ **èŠ‚çœæ˜¾å­˜**: ä¸éœ€è¦åŠ è½½CLIP/HuBERTç¼–ç å™¨
- âœ… **ä¸€è‡´æ€§**: æ‰€æœ‰epochä½¿ç”¨ç›¸åŒç‰¹å¾

### ğŸ“Š æ”¯æŒçš„æ¨¡æ€

| æ¨¡æ€ | ç¼–ç å™¨ | è¾“å‡ºç»´åº¦ | ç‰¹å¾ç›®å½• |
|------|--------|---------|----------|
| Frame | CLIP ViT-L | [8, 768] | `frame_CLIP_VIT_LARGE_8frms` |
| Face | CLIP ViT-L | [8, 768] | `face_CLIP_VIT_LARGE_8frms` |
| Audio | HuBERT-Large | [8, 1024] | `audio_HUBERT_LARGE_8clips` |
| AU | CLIP ViT-B/32 | [N, 512] | `au_CLIP_VITB32_8frms` |

### ğŸš€ æå–æ‰€æœ‰æ¨¡æ€ç‰¹å¾

```bash
cd /home/project/AffectGPT/AffectGPT

# æå–æ‰€æœ‰æ¨¡æ€ï¼ˆFrame, Face, Audio, AUï¼‰
python extract_multimodal_features_precompute.py \
    --dataset mercaptionplus \
    --modality all \
    --device cuda:0 \
    --video_root /path/to/videos \
    --face_root /path/to/faces \
    --audio_root /path/to/audios \
    --mer-factory-output /home/project/MER-Factory/output/MERCaptionPlus \
    --csv_path /path/to/train.csv \
    --save_root ./preextracted_features \
    --visual_encoder CLIP_VIT_LARGE \
    --acoustic_encoder HUBERT_LARGE

# ä»…æå–ç‰¹å®šæ¨¡æ€
python extract_multimodal_features_precompute.py \
    --dataset mercaptionplus \
    --modality frame \  # æˆ– face, audio, au
    ...
```

### ğŸ“ é¢„æå–ç‰¹å¾ç›®å½•ç»“æ„

```
preextracted_features/
â””â”€â”€ mercaptionplus/
    â”œâ”€â”€ frame_CLIP_VIT_LARGE_8frms/
    â”‚   â”œâ”€â”€ sample_00000120.npy  # [8, 768]
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ face_CLIP_VIT_LARGE_8frms/
    â”‚   â”œâ”€â”€ sample_00000120.npy  # [8, 768]
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ audio_HUBERT_LARGE_8clips/
    â”‚   â”œâ”€â”€ sample_00000120.npy  # [8, 1024]
    â”‚   â””â”€â”€ ...
    â””â”€â”€ au_CLIP_VITB32_8frms/
        â”œâ”€â”€ sample_00000120.npy  # [N, 512]
        â””â”€â”€ ...
```

### ğŸ”§ è®­ç»ƒé…ç½®

**å¯ç”¨é¢„æå–ç‰¹å¾**:
```yaml
model:
  # å®Œå…¨è·³è¿‡ç¼–ç å™¨åŠ è½½ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
  skip_encoders: True
  preextracted_visual_dim: 768
  preextracted_acoustic_dim: 1024

datasets:
  mercaptionplus:
    # å¯ç”¨é¢„æå–ç‰¹å¾
    use_preextracted_features: True
    preextracted_root: './preextracted_features/mercaptionplus'
    
    # ç¼–ç å™¨é…ç½®ï¼ˆç”¨äºæ„å»ºè·¯å¾„ï¼‰
    visual_encoder: 'CLIP_VIT_LARGE'
    acoustic_encoder: 'HUBERT_LARGE'
    clips_per_video: 8
```

### ğŸ“Š æ€§èƒ½æå‡

| æŒ‡æ ‡ | å®æ—¶ç¼–ç  | é¢„æå–ç‰¹å¾ | æå‡ |
|------|---------|-----------|------|
| è®­ç»ƒé€Ÿåº¦ | 100% | 150%+ | âš¡ +50% |
| æ˜¾å­˜å ç”¨ | 20GB | 15GB | ğŸ’¾ -25% |
| ç‰¹å¾ä¸€è‡´æ€§ | å¯èƒ½å˜åŒ– | å®Œå…¨ä¸€è‡´ | âœ… 100% |

---

## è®­ç»ƒå¯è§†åŒ–ï¼ˆè‡ªåŠ¨ä¿å­˜æ›²çº¿å›¾ï¼‰

### ğŸ“Š åŠŸèƒ½è¯´æ˜

è®­ç»ƒè¿‡ç¨‹ä¸­**è‡ªåŠ¨ç”Ÿæˆ**å­¦ä¹ ç‡å’ŒLossæ›²çº¿å›¾ï¼Œæ— éœ€é¢å¤–æ“ä½œï¼

### âœ¨ ç‰¹ç‚¹

- âœ… **è‡ªåŠ¨ä¿å­˜**: æ¯ä¸ªepochç»“æŸè‡ªåŠ¨ä¿å­˜å›¾ç‰‡
- âœ… **æ— éœ€é¢å¤–è„šæœ¬**: é›†æˆåœ¨è®­ç»ƒä»£ç ä¸­
- âœ… **é«˜è´¨é‡å›¾è¡¨**: æ ‡å‡†ç‰ˆ(150 DPI) + é«˜æ¸…ç‰ˆ(300 DPI)
- âœ… **å¤šç»´åº¦å±•ç¤º**: å­¦ä¹ ç‡ã€Lossã€Epochç»Ÿè®¡
- âœ… **ä¸€é”®å¼€å…³**: é…ç½®æ–‡ä»¶æ§åˆ¶å¯ç”¨/ç¦ç”¨

### ğŸ¨ ç”Ÿæˆçš„å›¾è¡¨

æ¯ä¸ªepochç»“æŸæ—¶è‡ªåŠ¨ç”ŸæˆåŒ…å«4ä¸ªå­å›¾çš„æ›²çº¿å›¾ï¼š

1. **å­¦ä¹ ç‡ vs æ­¥æ•°ï¼ˆçº¿æ€§ï¼‰**: æŸ¥çœ‹warmupå’Œè¡°å‡è¿‡ç¨‹
2. **å­¦ä¹ ç‡ vs æ­¥æ•°ï¼ˆå¯¹æ•°ï¼‰**: æ›´æ¸…æ™°åœ°çœ‹åˆ°å­¦ä¹ ç‡å˜åŒ–
3. **Loss vs æ­¥æ•°**: åŸå§‹loss + å¹³æ»‘æ›²çº¿ï¼ˆ100æ­¥çª—å£ï¼‰
4. **Loss vs Epoch**: æ¯ä¸ªepochçš„å¹³å‡loss + æ ‡å‡†å·®

### ğŸ“ è¾“å‡ºä½ç½®

```
output/your_experiment/training_curves/
â”œâ”€â”€ training_curves_epoch1.png       # Epoch 1ç»“æŸæ—¶çš„æ›²çº¿
â”œâ”€â”€ training_curves_epoch2.png       # Epoch 2ç»“æŸæ—¶çš„æ›²çº¿
â”œâ”€â”€ ...
â”œâ”€â”€ training_curves_epoch10.png      # Epoch 10ç»“æŸæ—¶çš„æ›²çº¿
â”œâ”€â”€ training_curves_hd_epoch5.png    # é«˜æ¸…ç‰ˆï¼ˆæ¯5ä¸ªepochï¼‰
â”œâ”€â”€ training_curves_hd_epoch10.png   # é«˜æ¸…ç‰ˆï¼ˆæ¯5ä¸ªepochï¼‰
â””â”€â”€ training_data_epoch10.npz        # åŸå§‹æ•°æ®ï¼ˆå¯é€‰ï¼‰
```

### âš™ï¸ é…ç½®æ–¹æ³•

#### é»˜è®¤å¯ç”¨ï¼ˆæ¨èï¼‰

è®­ç»ƒå¯è§†åŒ–**é»˜è®¤å¯ç”¨**ï¼Œæ— éœ€ä»»ä½•é…ç½®ï¼š

```bash
# ç›´æ¥è®­ç»ƒå³å¯
python train.py --cfg-path train_configs/your_config.yaml
```

#### æ‰‹åŠ¨æ§åˆ¶

åœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ ï¼ˆå¯é€‰ï¼‰ï¼š

```yaml
run_cfg:
  # ... å…¶ä»–é…ç½® ...
  
  visualize_training: True    # å¯ç”¨è®­ç»ƒå¯è§†åŒ–ï¼ˆé»˜è®¤Trueï¼‰
  # visualize_training: False # ç¦ç”¨è®­ç»ƒå¯è§†åŒ–
```

### ğŸ“Š å®æ—¶æŸ¥çœ‹ç»Ÿè®¡

æ¯ä¸ªepochç»“æŸæ—¶ï¼Œä¼šåœ¨è®­ç»ƒæ—¥å¿—ä¸­è‡ªåŠ¨æ‰“å°ç»Ÿè®¡ä¿¡æ¯ï¼š

```
======================================================================
ğŸ“Š Training Statistics
======================================================================
  Total Steps:          31,327
  Current Epoch:        1
  Current Learning Rate: 9.82e-05
  Latest Loss:          0.654321
  Recent 100 Avg Loss:  0.723456
  Best Loss:            0.345678 (Step 28934)
  Max Learning Rate:    1.00e-04
  Min Learning Rate:    1.23e-06
======================================================================
```

### ğŸ–¼ï¸ æŸ¥çœ‹å›¾ç‰‡

#### æ–¹å¼1: æœ¬åœ°è®­ç»ƒ

```bash
# æŸ¥çœ‹ç”Ÿæˆçš„å›¾ç‰‡
ls -lh output/your_experiment/training_curves/

# ç›´æ¥æ‰“å¼€æŸ¥çœ‹ï¼ˆLinuxæ¡Œé¢ç¯å¢ƒï¼‰
xdg-open output/your_experiment/training_curves/training_curves_epoch10.png
```

#### æ–¹å¼2: è¿œç¨‹æœåŠ¡å™¨è®­ç»ƒ

```bash
# æ–¹å¼A: ä½¿ç”¨scpä¸‹è½½åˆ°æœ¬åœ°
scp user@server:/path/to/output/your_experiment/training_curves/*.png ./local_folder/

# æ–¹å¼B: ä½¿ç”¨rsyncåŒæ­¥
rsync -avz user@server:/path/to/output/your_experiment/training_curves/ ./local_folder/

# æ–¹å¼C: ä½¿ç”¨VS Code Remote
# ç›´æ¥åœ¨VS Codeä¸­æµè§ˆå’ŒæŸ¥çœ‹å›¾ç‰‡
```

### ğŸ’¡ ä½¿ç”¨æŠ€å·§

#### 1. å®æ—¶ç›‘æ§è¿›åº¦

```bash
# æ¯éš”ä¸€æ®µæ—¶é—´ä¸‹è½½æœ€æ–°å›¾ç‰‡
while true; do
    scp user@server:/path/to/training_curves/training_curves_epoch*.png ./
    sleep 300  # æ¯5åˆ†é’ŸåŒæ­¥ä¸€æ¬¡
done
```

#### 2. è®­ç»ƒä¸­æœŸæ£€æŸ¥

```python
# å¦‚æœéœ€è¦ä¸­é€”æŸ¥çœ‹æ›²çº¿ï¼Œå¯ä»¥æ‰‹åŠ¨è°ƒç”¨
# åœ¨è®­ç»ƒä»£ç ä¸­æ·»åŠ ï¼ˆå¯é€‰ï¼‰
if epoch % 5 == 0:  # æ¯5ä¸ªepoch
    visualizer.plot_and_save(suffix=f'_epoch{epoch}_checkpoint')
```

#### 3. å¯¹æ¯”ä¸åŒå®éªŒ

```bash
# å°†ä¸åŒå®éªŒçš„æ›²çº¿æ”¾åœ¨ä¸€èµ·å¯¹æ¯”
experiment1/training_curves/training_curves_epoch10.png
experiment2/training_curves/training_curves_epoch10.png
```

### ğŸ¯ å®é™…ç¤ºä¾‹

#### é¢„æœŸçš„å­¦ä¹ ç‡æ›²çº¿

```
Learning Rate (log scale)
    â”‚
1e-4â”‚     â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
    â”‚    â•±            â•²___
    â”‚   â•±                 â•²___
1e-6â”‚  â•±                      â•²___
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Steps
    0  6265        156635       313270
    â†‘ Warmup      â†‘ Peak         â†‘ End
```

#### é¢„æœŸçš„Lossæ›²çº¿

```
Loss
  4â”‚â•²
   â”‚ â•²___
  2â”‚     â•²___
   â”‚         â•²___
  0â”‚             â”€â”€â”€â”€â”€â”€â”€â”€
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Steps
   å¿«é€Ÿä¸‹é™    å¹³ç¨³     æ”¶æ•›
```

### ğŸ”§ é«˜çº§åŠŸèƒ½

#### ä¿å­˜åŸå§‹æ•°æ®

å¯è§†åŒ–å™¨ä¼šè‡ªåŠ¨ä¿å­˜åŸå§‹æ•°æ®ä¸º`.npz`æ–‡ä»¶ï¼š

```python
# åŠ è½½æ•°æ®è¿›è¡Œè‡ªå®šä¹‰åˆ†æ
import numpy as np

data = np.load('output/your_experiment/training_curves/training_data_epoch10.npz')
steps = data['steps']
lrs = data['lrs']
losses = data['losses']
epochs = data['epochs']

# è‡ªå®šä¹‰ç»˜å›¾
import matplotlib.pyplot as plt
plt.plot(steps, losses)
plt.savefig('custom_plot.png')
```

#### ç¦ç”¨é«˜æ¸…ç‰ˆä¿å­˜

å¦‚æœä¸éœ€è¦é«˜æ¸…ç‰ˆï¼ˆèŠ‚çœç©ºé—´ï¼‰ï¼Œå¯ä»¥ä¿®æ”¹ä»£ç ï¼š

```python
# åœ¨ training_visualizer.py ä¸­
# æ³¨é‡Šæ‰é«˜æ¸…ç‰ˆä¿å­˜çš„ä»£ç ï¼ˆç¬¬121-180è¡Œï¼‰
```

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **å­˜å‚¨ç©ºé—´**: æ¯ä¸ªepochçº¦ç”Ÿæˆ1-2MBå›¾ç‰‡ï¼Œ10ä¸ªepochçº¦10-20MB
2. **ä»…ä¸»è¿›ç¨‹**: å¤šGPUè®­ç»ƒæ—¶ï¼Œåªæœ‰ä¸»è¿›ç¨‹ï¼ˆrank 0ï¼‰ç”Ÿæˆå›¾ç‰‡
3. **matplotlibåç«¯**: ä½¿ç”¨Aggåç«¯ï¼Œæ— éœ€GUIç¯å¢ƒ
4. **è‡ªåŠ¨è¦†ç›–**: åŒåæ–‡ä»¶ä¼šè¢«è¦†ç›–ï¼ˆå»ºè®®å®šæœŸå¤‡ä»½é‡è¦æ›²çº¿ï¼‰

### â“ å¸¸è§é—®é¢˜

#### Q1: æ²¡æœ‰ç”Ÿæˆå›¾ç‰‡ï¼Ÿ

**A**: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
```bash
# 1. ç¡®è®¤è¾“å‡ºç›®å½•
ls output/your_experiment/

# 2. æ£€æŸ¥è®­ç»ƒæ—¥å¿—
grep "Training curves saved" train.log

# 3. ç¡®è®¤å¯è§†åŒ–å·²å¯ç”¨
grep "visualize_training" train_configs/your_config.yaml
```

#### Q2: å›¾ç‰‡ä¸æ¸…æ™°ï¼Ÿ

**A**: ä½¿ç”¨é«˜æ¸…ç‰ˆï¼š
```bash
# æŸ¥æ‰¾é«˜æ¸…ç‰ˆå›¾ç‰‡ï¼ˆ300 DPIï¼‰
ls output/your_experiment/training_curves/*_hd.png
```

#### Q3: èƒ½å¦ä¿®æ”¹å›¾è¡¨æ ·å¼ï¼Ÿ

**A**: å¯ä»¥ç¼–è¾‘ `my_affectgpt/common/training_visualizer.py`ï¼š
```python
# ä¿®æ”¹é¢œè‰²ã€çº¿å®½ã€å›¾è¡¨å¤§å°ç­‰
plt.style.use('seaborn-v0_8-darkgrid')  # ç¬¬72è¡Œ
fig, axes = plt.subplots(2, 2, figsize=(16, 12))  # ç¬¬75è¡Œ
```

#### Q4: èƒ½å¦åœ¨è®­ç»ƒç»“æŸåé‡æ–°ç”Ÿæˆï¼Ÿ

**A**: å¯ä»¥ï¼Œæ•°æ®å·²ä¿å­˜ï¼š
```python
# åŠ è½½æ•°æ®é‡æ–°ç»˜åˆ¶
from my_affectgpt.common.training_visualizer import TrainingVisualizer

vis = TrainingVisualizer('output/new_plots', enabled=True)
vis.load_data('output/your_experiment/training_curves/training_data_epoch10.npz')
vis.plot_and_save(suffix='_final')
```

### ğŸ“ å®Œæ•´å·¥ä½œæµç¨‹

```bash
# 1. å¯åŠ¨è®­ç»ƒï¼ˆå¯è§†åŒ–è‡ªåŠ¨å¯ç”¨ï¼‰
python train.py --cfg-path train_configs/your_config.yaml > train.log 2>&1 &

# 2. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f train.log

# 3. æ¯ä¸ªepochç»“æŸæ—¶ï¼Œæ—¥å¿—ä¼šæ˜¾ç¤ºï¼š
#    âœ… Training curves saved: output/.../training_curves_epoch1.png
#    ğŸ“Š Training Statistics
#    ... (ç»Ÿè®¡ä¿¡æ¯) ...

# 4. ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°æŸ¥çœ‹ï¼ˆè¿œç¨‹è®­ç»ƒï¼‰
scp user@server:/path/to/output/*/training_curves/*.png ./

# 5. æŸ¥çœ‹æ›²çº¿ï¼Œç›‘æ§è®­ç»ƒè¿›åº¦
# - å­¦ä¹ ç‡æ˜¯å¦æ­£å¸¸è¡°å‡ï¼Ÿ
# - Lossæ˜¯å¦å¹³ç¨³ä¸‹é™ï¼Ÿ
# - æ˜¯å¦è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆï¼Ÿ

# 6. è®­ç»ƒç»“æŸåï¼Œæ‰€æœ‰epochçš„æ›²çº¿éƒ½å·²ä¿å­˜
ls -lh output/your_experiment/training_curves/
```

### ğŸ“ æ€»ç»“

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
- âœ… é›¶é…ç½®ï¼šé»˜è®¤å¯ç”¨ï¼Œæ— éœ€é¢å¤–æ“ä½œ
- âœ… è‡ªåŠ¨åŒ–ï¼šæ¯ä¸ªepochè‡ªåŠ¨ä¿å­˜ï¼Œæ— éœ€æ‰‹åŠ¨è§¦å‘
- âœ… é«˜è´¨é‡ï¼šä¸“ä¸šçš„å›¾è¡¨æ ·å¼å’Œç»Ÿè®¡ä¿¡æ¯
- âœ… è½»é‡çº§ï¼šé›†æˆåœ¨è®­ç»ƒæµç¨‹ï¼Œæ— æ€§èƒ½å½±å“

**ä½¿ç”¨å»ºè®®**ï¼š
- ğŸ“Š å®šæœŸæŸ¥çœ‹æ›²çº¿ï¼ŒåŠæ—¶å‘ç°è®­ç»ƒé—®é¢˜
- ğŸ’¾ é‡è¦å®éªŒå»ºè®®å¤‡ä»½æ›²çº¿å›¾
- ğŸ” å¯¹æ¯”ä¸åŒå®éªŒçš„æ›²çº¿ï¼Œé€‰æ‹©æœ€ä½³é…ç½®

---

## Pre-Fusionå†…éƒ¨æœºåˆ¶è¯¦è§£

### ğŸ¯ æ¦‚è¿°

Pre-Fusionæ˜¯AffectGPTä¸­**Audioå’ŒFace/Videoæ¨¡æ€èåˆ**çš„æ ¸å¿ƒæœºåˆ¶ï¼Œä½¿ç”¨**Cross-Attention (Q-Former)**å®ç°è·¨æ¨¡æ€ä¿¡æ¯æ•´åˆã€‚

### ğŸ“Š æ ¸å¿ƒæ¶æ„

```
è¾“å…¥: Audioç‰¹å¾ + Faceç‰¹å¾
  â†“
ç‰¹å¾å¯¹é½ï¼ˆLinearæŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦ï¼‰
  â†“
ç‰¹å¾æ‹¼æ¥ï¼ˆConcatï¼‰+ ä½ç½®ç¼–ç 
  â†“
Cross-Attention (Q-Former)
  â”œâ”€ Q (Query): 16ä¸ªå¯å­¦ä¹ çš„æŸ¥è¯¢å‘é‡
  â”œâ”€ K (Key): æ¥è‡ªAudio+Faceçš„concatç‰¹å¾
  â””â”€ V (Value): æ¥è‡ªAudio+Faceçš„concatç‰¹å¾
  â†“
è¾“å‡º: 16ä¸ªèåˆtoken
  â†“
æŠ•å½±åˆ°LLMç©ºé—´
```

### ğŸ” Q, K, V è¯¦è§£

#### Query (Q) - "æˆ‘æƒ³è¦ä»€ä¹ˆä¿¡æ¯"

- **æ¥æº**: `self.multi_query_tokens` - å¯å­¦ä¹ å‚æ•°
- **ç»´åº¦**: `[batch, 16, 768]`
- **ç‰¹æ€§**: 
  - âœ… å›ºå®šæ•°é‡ï¼ˆ16ä¸ªï¼‰
  - âœ… è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ æœ€ä¼˜queryç­–ç•¥
  - âœ… æ¯ä¸ªqueryå…³æ³¨ä¸åŒçš„è·¨æ¨¡æ€ä¿¡æ¯æ–¹é¢

**ç¤ºä¾‹**:
```python
Q1: å…³æ³¨æ•´ä½“æƒ…æ„Ÿå¼ºåº¦
Q2: å…³æ³¨é¢éƒ¨è¡¨æƒ…ç»†èŠ‚
Q3: å…³æ³¨éŸ³è§†é¢‘ä¸€è‡´æ€§
...
Q16: å…³æ³¨å…¨å±€ä¸Šä¸‹æ–‡
```

#### Key & Value (K, V) - "è¿™é‡Œæœ‰ä»€ä¹ˆä¿¡æ¯"

- **æ¥æº**: Audioç‰¹å¾ + Faceç‰¹å¾çš„concat
- **ç»´åº¦**: `[batch, 40, 1024]` (å‡è®¾8ä¸ªaudioå¸§ + 32ä¸ªfaceå¸§)
- **å¤„ç†æµç¨‹**:
  ```python
  Za = Linear(Audio)        # [batch, 8, 1024]
  Zf = Linear(Face)         # [batch, 32, 1024]
  Z_concat = Concat(Za, Zf) # [batch, 40, 1024]
  Z_kv = Z_concat + PosEmb  # æ·»åŠ ä½ç½®ç¼–ç 
  ```

#### Cross-Attentionè®¡ç®—

```python
Attention(Q, K, V) = softmax(QÂ·K^T / âˆšd_k) Â· V

Q: [batch, 16, 768]   # 16ä¸ªqueries
K: [batch, 40, 768]   # 40ä¸ªæ—¶é—´æ­¥çš„keys
V: [batch, 40, 768]   # 40ä¸ªæ—¶é—´æ­¥çš„values

Attention_Scores: [batch, 16, 40]  # æ¯ä¸ªqueryå¯¹40ä¸ªæ—¶é—´æ­¥çš„å…³æ³¨åº¦
Output: [batch, 16, 768]           # 16ä¸ªèåˆåçš„token
```

### ğŸ“ ç»´åº¦å˜åŒ–ç¤ºä¾‹

```
è¾“å…¥:
  Audio: [3, 8, 1024]
  Face:  [3, 32, 1024]

å¯¹é½:
  Audio': [3, 8, 1024]
  Face':  [3, 32, 1024]

æ‹¼æ¥:
  Concat: [3, 40, 1024]  # 8+32=40

ä½ç½®ç¼–ç :
  Z_kv: [3, 40, 1024]

Queryå‡†å¤‡:
  Zq: [3, 16, 768]

Cross-Attention:
  Output: [3, 16, 768]

æŠ•å½±åˆ°LLM:
  Ef': [3, 16, 4096]  # æŠ•å½±åˆ°LLaMAç©ºé—´
```

### ğŸ’¡ è®¾è®¡ä¼˜åŠ¿

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **ç»´åº¦å‹ç¼©** | 40ä¸ªæ—¶é—´æ­¥ â†’ 16ä¸ªtoken |
| **è·¨æ¨¡æ€èåˆ** | Audio + Faceä¿¡æ¯æœ‰æ•ˆæ•´åˆ |
| **è‡ªé€‚åº”å­¦ä¹ ** | Queryå­¦ä¹ æœ€ä¼˜çš„ä¿¡æ¯æå–ç­–ç•¥ |
| **å›ºå®šè¾“å‡º** | è¾“å‡ºç»´åº¦å›ºå®šï¼Œä¾¿äºåç»­LLMå¤„ç† |

### ğŸ”— è¯¦ç»†æ–‡æ¡£

- **æŠ€æœ¯è¯¦è§£**: å‚è§ `PRE_FUSION_MECHANISM.md`
  - å®Œæ•´ä»£ç åˆ†æ
  - è¯¦ç»†è®¡ç®—æµç¨‹
  - å‚æ•°é…ç½®è¯´æ˜
  
- **å¯è§†åŒ–å›¾è¡¨**: å‚è§ `PRE_FUSION_VISUAL.md`
  - æ•°æ®æµå›¾
  - Q-K-Väº¤äº’ç¤ºæ„
  - ç»´åº¦å˜åŒ–å…¨æµç¨‹
  - å›¾ä¹¦é¦†æŸ¥è¯¢ç±»æ¯”

### ğŸ“Š Attentionæƒé‡ç¤ºä¾‹

```
Query 1 (å…³æ³¨æƒ…æ„Ÿå¼ºåº¦):
  Audioå¸§:  [0.05, 0.08, 0.15, 0.20, 0.10, 0.05, 0.03, 0.02]
  Faceå¸§:   [0.01, 0.01, ..., 0.05, 0.08, 0.10, 0.04, ...]
  â†’ ä¸»è¦å…³æ³¨éŸ³é¢‘çš„ä¸­é—´æ®µå’Œé¢éƒ¨çš„åæ®µ

Query 2 (å…³æ³¨è¡¨æƒ…ç»†èŠ‚):
  Audioå¸§:  [0.01, 0.01, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01]
  Faceå¸§:   [0.12, 0.10, 0.09, ..., 0.05, 0.04, 0.03, ...]
  â†’ ä¸»è¦å…³æ³¨é¢éƒ¨çš„å‰å‡ å¸§
```

### ğŸ“ å…³é”®ä»£ç 

**æ–‡ä»¶**: `my_affectgpt/models/affectgpt.py`

**å‡½æ•°**: `encode_multi_qformer` (Line 843-876)

```python
def encode_multi_qformer(self, video_hidden_state, audio_hidden_state):
    # ç‰¹å¾å¯¹é½
    video_hidden_state = self.multi_video_embs(video_hidden_state)
    audio_hidden_state = self.multi_audio_embs(audio_hidden_state)
    
    # æ‹¼æ¥
    multi_hidden_state = torch.concat([video_hidden_state, audio_hidden_state], axis=1)
    
    # ä½ç½®ç¼–ç 
    multi_hidden_state = multi_hidden_state + multi_position_embeddings
    
    # Cross-Attention (Q-Former)
    multi_query_tokens = self.multi_query_tokens.expand(batch, -1, -1)
    multi_query_output = self.multi_Qformer.bert(
        query_embeds=multi_query_tokens,      # Q
        encoder_hidden_states=multi_hidden_state,  # K, V
        encoder_attention_mask=frame_atts,
        return_dict=True,
    )
    
    # æŠ•å½±åˆ°LLM
    inputs_llama = self.multi_llama_proj(multi_query_output.last_hidden_state)
    return multi_hidden, inputs_llama
```

---

## è®­ç»ƒé‡‡æ ·æœºåˆ¶è¯¦è§£

### ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

è®­ç»ƒä¸æ˜¯ç®€å•åœ° "æ¯ä¸ªepochéšæœºæŠ½1000ä¸ªæ ·æœ¬"ï¼Œè€Œæ˜¯ä½¿ç”¨**è¿­ä»£å™¨ï¼ˆIteratorï¼‰æ¨¡å¼**ï¼š

#### é…ç½®å‚æ•°
```yaml
run_cfg:
  max_epoch: 30           # æ€»epochæ•°
  iters_per_epoch: 1000   # æ¯ä¸ªepochçš„è¿­ä»£æ¬¡æ•°
  warmup_steps: 1000      # warmupæ­¥æ•°
```

#### è®­ç»ƒæ—¥å¿—è§£æ
```
training sample number: 5000         # æ•°æ®é›†æ€»æ ·æœ¬æ•°
Loaded 5000 records for train split  # åŠ è½½çš„è®­ç»ƒæ ·æœ¬æ•°
Start training epoch 1, 1000 iters per inner epoch  # æ¯ä¸ªepochè·‘1000æ¬¡è¿­ä»£
```

### ğŸ“Š é‡‡æ ·åŸç†

#### 1. DataLoader é…ç½®
```python
# runner_base.py
sampler = DistributedSampler(
    dataset,
    shuffle=True,  # âœ… è®­ç»ƒæ—¶éšæœºæ‰“ä¹±
    num_replicas=world_size,
    rank=rank
)

loader = DataLoader(
    dataset,
    batch_size=batch_size,
    sampler=sampler,
    shuffle=False,  # samplerå·²å¤„ç†æ‰“ä¹±ï¼Œè¿™é‡Œä¸éœ€è¦
    drop_last=True  # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„batch
)
```

#### 2. è¿­ä»£å™¨æ¨¡å¼
```python
# base_task.py - train_epoch()
data_loader = iter(data_loader)  # è½¬æ¢ä¸ºè¿­ä»£å™¨

for i in range(iters_per_epoch):  # å¾ªç¯1000æ¬¡
    samples = next(data_loader)   # æ¯æ¬¡å–1ä¸ªbatch
    # ... å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ã€æ›´æ–°å‚æ•°
```

### ğŸ”„ å®é™…è®­ç»ƒæµç¨‹

å‡è®¾é…ç½®å¦‚ä¸‹ï¼š
- æ€»æ ·æœ¬æ•°: 5000
- batch_size: 8
- iters_per_epoch: 1000
- max_epoch: 30

#### Epoch 1
1. **æ‰“ä¹±æ•°æ®**: DistributedSampler éšæœºæ‰“ä¹±5000ä¸ªæ ·æœ¬çš„é¡ºåº
2. **è¿­ä»£è®­ç»ƒ**: 
   - iter 0: å–batch [æ ·æœ¬0-7]
   - iter 1: å–batch [æ ·æœ¬8-15]
   - ...
   - iter 624: å–batch [æ ·æœ¬4992-4999] â†’ 5000ä¸ªæ ·æœ¬ç”¨å®Œ
   - iter 625: **å¾ªç¯å›åˆ°å¼€å¤´**ï¼Œå–batch [æ ·æœ¬0-7]
   - ...
   - iter 999: è®­ç»ƒå®Œæˆï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªepoch

**å…³é”®**: è¿­ä»£å™¨ä¼š**å¾ªç¯ä½¿ç”¨**æ•°æ®é›†ï¼

#### Epoch 2
1. **é‡æ–°æ‰“ä¹±**: DistributedSampler é‡æ–°éšæœºæ‰“ä¹±ï¼ˆé¡ºåºä¸epoch 1ä¸åŒï¼‰
2. **ç»§ç»­è¿­ä»£**: åŒæ ·è·‘1000æ¬¡è¿­ä»£

### ğŸ“ˆ è®¡ç®—å…³ç³»

```python
# æ¯ä¸ªepochå®é™…è®¿é—®çš„æ ·æœ¬æ•°é‡
actual_samples_per_epoch = iters_per_epoch * batch_size
                         = 1000 * 8 = 8000 ä¸ªæ ·æœ¬

# æ•°æ®é›†å¾ªç¯æ¬¡æ•°
dataset_cycles = actual_samples_per_epoch / total_samples
               = 8000 / 5000 = 1.6 è½®

# æ€»è®­ç»ƒæ­¥æ•°
total_steps = max_epoch * iters_per_epoch
            = 30 * 1000 = 30000 æ­¥

# æ¯ä¸ªæ ·æœ¬è¢«è®¿é—®æ¬¡æ•°ï¼ˆå¹³å‡ï¼‰
samples_seen_per_data = (total_steps * batch_size) / total_samples
                      = (30000 * 8) / 5000 = 48 æ¬¡
```

### âš ï¸ é‡è¦ç‰¹æ€§

#### 1. å¾ªç¯é‡‡æ ·
- **ä¸æ˜¯**: æ¯ä¸ªepochåªç”¨5000ä¸ªæ ·æœ¬ä¸­çš„1000ä¸ª
- **è€Œæ˜¯**: æ¯ä¸ªepochè·‘1000æ¬¡è¿­ä»£ï¼Œä¼š**å¾ªç¯ä½¿ç”¨**æ•°æ®é›†
- 5000ä¸ªæ ·æœ¬ç”¨å®Œåï¼Œ**è‡ªåŠ¨å›åˆ°å¼€å¤´ç»§ç»­**ï¼ˆæ¯ä¸ªepoché‡æ–°æ‰“ä¹±ï¼‰

#### 2. éšæœºæ€§ä¿è¯
```python
# DistributedSampler æ¯ä¸ªepochéƒ½ä¼šé‡æ–°æ‰“ä¹±
def set_epoch(self, epoch):
    self.epoch = epoch
    # ä½¿ç”¨epochä½œä¸ºéšæœºç§å­ï¼Œä¿è¯æ¯ä¸ªepoché¡ºåºä¸åŒ
```

#### 3. ä¸ºä»€ä¹ˆç”¨è¿­ä»£å™¨æ¨¡å¼ï¼Ÿ

| å¯¹æ¯” | è¿­ä»£å™¨æ¨¡å¼ | ä¼ ç»Ÿepochæ¨¡å¼ |
|------|-----------|--------------|
| **çµæ´»æ€§** | é«˜ï¼ˆç²¾ç¡®æ§åˆ¶æ­¥æ•°ï¼‰ | ä½ï¼ˆå¿…é¡»è·‘å®Œæ•´ä¸ªæ•°æ®é›†ï¼‰ |
| **å°æ•°æ®é›†** | å¯å¾ªç¯åˆ©ç”¨ | æ¯ä¸ªepochå¾ˆçŸ­ |
| **å­¦ä¹ ç‡è°ƒåº¦** | åŸºäºæ­¥æ•°ï¼ˆç²¾ç¡®ï¼‰ | åŸºäºepochï¼ˆç²—ç³™ï¼‰ |
| **åˆ†å¸ƒå¼è®­ç»ƒ** | å„GPUæ­¥æ•°ä¸€è‡´ | å¯èƒ½ä¸ä¸€è‡´ |

### ğŸ’¡ å®é™…ç¤ºä¾‹

#### æ—¥å¿—ç¤ºä¾‹
```
Train: data epoch: [1]  [  0/1000]  eta: 0:38:28  lr: 0.00001000  loss: 8.37608719
Train: data epoch: [1]  [ 50/1000]  eta: 0:04:58  lr: 0.00005000  loss: 2.65583491
Train: data epoch: [1]  [100/1000]  eta: 0:04:58  lr: 0.00010000  loss: 1.85704851
...
Train: data epoch: [1]  [150/1000]  eta: 0:04:24  lr: 0.00001000  loss: 1.63498379
```

- `[100/1000]`: å½“å‰epochçš„ç¬¬100æ¬¡è¿­ä»£ï¼ˆå…±1000æ¬¡ï¼‰
- æ¯æ¬¡è¿­ä»£å¤„ç† batch_size ä¸ªæ ·æœ¬
- 1000æ¬¡è¿­ä»£åè¿›å…¥ä¸‹ä¸€ä¸ªepoch

### ğŸ“ æ€»ç»“

**æ ¸å¿ƒè¦ç‚¹**:
1. âœ… æ¯ä¸ªepochå›ºå®šè·‘ **1000æ¬¡è¿­ä»£**ï¼ˆä¸æ˜¯1000ä¸ªæ ·æœ¬ï¼‰
2. âœ… æ¯æ¬¡è¿­ä»£å– **batch_sizeä¸ªæ ·æœ¬**ï¼ˆå¦‚8ä¸ªï¼‰
3. âœ… æ•°æ®é›†ä¼š**å¾ªç¯ä½¿ç”¨**ï¼ˆ5000æ ·æœ¬è·‘1.6è½®ï¼‰
4. âœ… æ¯ä¸ªepoch **é‡æ–°éšæœºæ‰“ä¹±**é¡ºåº
5. âœ… å­¦ä¹ ç‡åŸºäº**æ€»æ­¥æ•°**è°ƒåº¦ï¼ˆ30000æ­¥ï¼‰

**ä¼˜åŠ¿**:
- ç²¾ç¡®æ§åˆ¶è®­ç»ƒæ­¥æ•°
- é€‚åˆå°æ•°æ®é›†ï¼ˆå……åˆ†åˆ©ç”¨ï¼‰
- å­¦ä¹ ç‡è°ƒåº¦æ›´å¹³æ»‘
- åˆ†å¸ƒå¼è®­ç»ƒæ›´ç¨³å®š

### ğŸ”¥ Warmup æœºåˆ¶è¯¦è§£

#### ä»€ä¹ˆæ˜¯ Warmupï¼Ÿ

**Warmup** æ˜¯å­¦ä¹ ç‡é¢„çƒ­æœºåˆ¶ï¼Œåœ¨è®­ç»ƒåˆæœŸ**é€æ­¥å¢åŠ **å­¦ä¹ ç‡ï¼Œé¿å…åˆå§‹æ¢¯åº¦è¿‡å¤§å¯¼è‡´æ¨¡å‹ä¸ç¨³å®šã€‚

#### é…ç½®å‚æ•°
```yaml
run_cfg:
  warmup_steps: 1000      # warmupæ­¥æ•°
  warmup_lr: 1e-6         # èµ·å§‹å­¦ä¹ ç‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤ç”¨init_lrï¼‰
  init_lr: 1e-4           # ç›®æ ‡å­¦ä¹ ç‡ï¼ˆwarmupç»“æŸæ—¶ï¼‰
  min_lr: 0               # æœ€å°å­¦ä¹ ç‡ï¼ˆcosineè¡°å‡ç»“æŸæ—¶ï¼‰
```

#### å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥

**LinearWarmupCosineLR**: Warmupçº¿æ€§å¢é•¿ + Cosineè¡°å‡

```python
total_cur_step = cur_epoch * iters_per_epoch + cur_step

if total_cur_step < warmup_steps:
    # é˜¶æ®µ1: Warmup (0 â†’ 1000æ­¥)
    lr = warmup_start_lr + (init_lr - warmup_start_lr) * (total_cur_step / warmup_steps)
    # çº¿æ€§å¢é•¿: 1e-6 â†’ 1e-4
else:
    # é˜¶æ®µ2: Cosineè¡°å‡ (1000 â†’ 30000æ­¥)
    progress = (total_cur_step - warmup_steps) / (total_steps - warmup_steps)
    lr = min_lr + (init_lr - min_lr) * 0.5 * (1 + cos(Ï€ * progress))
    # ä½™å¼¦è¡°å‡: 1e-4 â†’ 0
```

#### å®é™…ç¤ºä¾‹

å‡è®¾é…ç½®ï¼š
- `warmup_steps: 1000`
- `warmup_lr: 1e-6` (èµ·å§‹)
- `init_lr: 1e-4` (warmupç»“æŸ)
- `min_lr: 0` (è®­ç»ƒç»“æŸ)
- `max_epoch: 30`
- `iters_per_epoch: 1000`
- æ€»æ­¥æ•°: 30,000

**å­¦ä¹ ç‡å˜åŒ–æ›²çº¿**:
```
Step 0:       lr = 1e-6     (warmupå¼€å§‹)
Step 500:     lr = 5e-5     (warmupä¸­æœŸï¼Œçº¿æ€§å¢é•¿)
Step 1000:    lr = 1e-4     (warmupç»“æŸï¼Œè¾¾åˆ°å³°å€¼) â† warmup_steps
Step 10000:   lr â‰ˆ 8e-5     (cosineè¡°å‡)
Step 20000:   lr â‰ˆ 5e-5     (ç»§ç»­è¡°å‡)
Step 30000:   lr â†’ 0        (è¡°å‡åˆ°æœ€å°å€¼)
```

#### å¯è§†åŒ–

```
Learning Rate
    â”‚
1e-4â”‚     â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
    â”‚    â•±            â•²
    â”‚   â•±              â•²
    â”‚  â•±                â•²___
1e-6â”‚ â•±                      â•²___
    â”‚â•±                            â•²___
  0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Steps
    0   1000              15000          30000
       â†‘warmup            â†‘ä¸­æœŸ           â†‘ç»“æŸ
```

#### è®­ç»ƒæ—¥å¿—å¯¹åº”

æ‚¨çš„æ—¥å¿—ä¸­å¯ä»¥çœ‹åˆ°å­¦ä¹ ç‡å˜åŒ–ï¼š
```
Train: [  0/1000]  lr: 0.00001000  # Step 0, warmupå¼€å§‹
Train: [ 50/1000]  lr: 0.00005000  # Step 50, warmupä¸­
Train: [100/1000]  lr: 0.00010000  # Step 100, warmupä¸­
Train: [150/1000]  lr: 0.00001000  # Step 150, å·²è¿›å…¥cosineè¡°å‡æœŸ
```

**æ³¨æ„**: æ‚¨çš„æ—¥å¿—æ˜¾ç¤ºç¬¬150æ­¥lr=0.00001000ï¼Œè¯´æ˜**warmupå·²å®Œæˆ**ï¼Œæ­£åœ¨cosineè¡°å‡ã€‚

#### ä¸ºä»€ä¹ˆéœ€è¦ Warmupï¼Ÿ

| é—®é¢˜ | ä¸ç”¨Warmup | ç”¨Warmup |
|------|-----------|---------|
| **åˆå§‹æ¢¯åº¦** | å¯èƒ½å¾ˆå¤§ | é€æ­¥é€‚åº” |
| **å‚æ•°æ›´æ–°** | å‰§çƒˆéœ‡è¡ | å¹³ç¨³è¿‡æ¸¡ |
| **è®­ç»ƒç¨³å®šæ€§** | å®¹æ˜“å´©æºƒ | æ›´åŠ ç¨³å®š |
| **æ”¶æ•›é€Ÿåº¦** | å¯èƒ½å˜æ…¢ | æ›´å¿«æ”¶æ•› |
| **æœ€ç»ˆæ•ˆæœ** | å¯èƒ½è¾ƒå·® | æ€§èƒ½æ›´å¥½ |

#### æœ€ä½³å®è·µ

```yaml
# æ¨èé…ç½®
warmup_steps: 1000              # çº¦ä¸ºæ€»æ­¥æ•°çš„3-5%
warmup_lr: 1e-6                 # çº¦ä¸ºinit_lrçš„1/100
init_lr: 1e-4                   # æ ¹æ®batch_sizeè°ƒæ•´
min_lr: 0                       # æˆ–è®¾ä¸ºinit_lrçš„1/100

# è®¡ç®—å…¬å¼
warmup_steps = total_steps * 0.03  # æ€»æ­¥æ•°çš„3%
warmup_lr = init_lr / 100          # åˆå§‹å­¦ä¹ ç‡çš„1%
```

#### è°ƒè¯•æŠ€å·§

**æŸ¥çœ‹å­¦ä¹ ç‡æ›²çº¿**:
```python
# è®­ç»ƒæ—¥å¿—ä¸­æå–lr
import re
with open('train.log') as f:
    lrs = re.findall(r'lr: (\d+\.\d+)', f.read())
    lrs = [float(lr) for lr in lrs]
    
# ç»˜åˆ¶æ›²çº¿
import matplotlib.pyplot as plt
plt.plot(lrs)
plt.xlabel('Step')
plt.ylabel('Learning Rate')
plt.title('LR Schedule')
plt.show()
```

### ğŸ’¡ Warmup å¸¸è§é—®é¢˜

#### Q1: warmup_steps è®¾å¤šå°‘åˆé€‚ï¼Ÿ
**A**: ä¸€èˆ¬ä¸ºæ€»æ­¥æ•°çš„ **3-5%**
```python
total_steps = max_epoch * iters_per_epoch = 30 * 1000 = 30,000
warmup_steps = total_steps * 0.03 â‰ˆ 1000  âœ…
```

#### Q2: warmupå¤ªçŸ­æˆ–å¤ªé•¿ä¼šæ€æ ·ï¼Ÿ
- **å¤ªçŸ­** (å¦‚100æ­¥): å­¦ä¹ ç‡å¢é•¿å¤ªå¿«ï¼Œå¯èƒ½ä¸ç¨³å®š
- **å¤ªé•¿** (å¦‚5000æ­¥): æµªè´¹è®­ç»ƒæ—¶é—´ï¼Œæ”¶æ•›å˜æ…¢
- **åˆé€‚** (1000æ­¥): å¹³è¡¡ç¨³å®šæ€§å’Œæ•ˆç‡

#### Q3: ä¸ºä»€ä¹ˆæ—¥å¿—æ˜¾ç¤ºlråœ¨150æ­¥å°±å¾ˆå°ï¼Ÿ
**A**: å¯èƒ½åŸå› ï¼š
1. `warmup_steps < 150`ï¼ˆwarmupå·²ç»“æŸï¼‰
2. Cosineè¡°å‡å·²ç»å¼€å§‹
3. æ£€æŸ¥é…ç½®æ–‡ä»¶ç¡®è®¤ `warmup_steps` å€¼

#### Q4: èƒ½å¦ä¸ç”¨warmupï¼Ÿ
**A**: å¯ä»¥ï¼Œä½†**ä¸æ¨è**ï¼š
- å¤§æ¨¡å‹å¾®è°ƒï¼š**å¿…é¡»**ç”¨warmup
- å°æ•°æ®é›†ï¼šå»ºè®®ç”¨warmup
- ä»å¤´è®­ç»ƒï¼šå¼ºçƒˆå»ºè®®ç”¨warmup

---

## å¿«é€Ÿå¼€å§‹

### ğŸ¬ å®Œæ•´å·¥ä½œæµç¨‹

#### 1. ç¯å¢ƒå‡†å¤‡

```bash
# æ¿€æ´»condaç¯å¢ƒ
conda activate vllm2

# æ£€æŸ¥GPU
nvidia-smi
```

#### 2. ç”ŸæˆAUåˆ†æï¼ˆMER-Factoryï¼‰

```bash
cd /home/project/MER-Factory

# æ‰¹é‡å¤„ç†æ‰€æœ‰æ•°æ®é›†
python batch_extract_au_multi_datasets.py

# æˆ–åå°è¿è¡Œ
nohup python batch_extract_au_multi_datasets.py > batch_run.log 2>&1 &
```

#### 3. æå–CLIPç‰¹å¾ï¼ˆAffectGPTï¼‰

```bash
cd /home/project/AffectGPT/AffectGPT

# æµ‹è¯•å•æ ·æœ¬
python test_single_sample.py

# æ‰¹é‡æå–
bash extract_mercaptionplus_features.sh

# éªŒè¯
python verify_au_pipeline.py
```

#### 4. è®­ç»ƒæ¨¡å‹

```bash
# ä¿®æ”¹é…ç½®æ–‡ä»¶
# vim train_configs/your_config.yaml

# è¿è¡Œè®­ç»ƒ
python train.py --cfg-path train_configs/your_config.yaml
```

#### 5. æ¨ç†æµ‹è¯•

```bash
# ä¿®æ”¹æ¨ç†é…ç½®
# vim eval_configs/your_config.yaml

# è¿è¡Œæ¨ç†
python inference.py --cfg-path eval_configs/your_config.yaml
```

### ğŸ“ é…ç½®æ–‡ä»¶æ¨¡æ¿

**è®­ç»ƒé…ç½®æ¨¡æ¿**:
```yaml
model:
  arch: affectgpt
  
  # é¢„æå–ç‰¹å¾ä¼˜åŒ–
  skip_encoders: True
  use_preextracted_features: True
  
  # AU Agenté…ç½®
  use_au_agent: True
  au_agent_base_model: "/path/to/Qwen2.5-7B-Instruct"
  au_agent_lora_weights: "/path/to/au_agent_lora"
  au_agent_device: "cuda:1"

datasets:
  mercaptionplus:
    data_type: video
    face_or_frame: 'multiface_audio_face_frame_text'
    
    # Frameé‡‡æ ·
    frame_n_frms: 8
    frame_sampling: 'uniform'
    
    # é¢„æå–ç‰¹å¾
    use_preextracted_features: True
    preextracted_root: './preextracted_features/mercaptionplus'
    
    # MER-Factoryè¾“å‡º
    mer_factory_output: '/home/project/MER-Factory/output'
```

---

## å¸¸è§é—®é¢˜

### â“ AU Agentç›¸å…³

#### Q1: AU Agentç”Ÿæˆå¤±è´¥
```
âš ï¸ AU Agentç”Ÿæˆå¤±è´¥: xxx
```

**åŸå› **:
- AU Agentæ¨¡å‹æœªæ­£ç¡®åŠ è½½
- æ˜¾å­˜ä¸è¶³
- é…ç½®è·¯å¾„é”™è¯¯

**è§£å†³**:
1. æ£€æŸ¥ `use_au_agent: True`
2. æ£€æŸ¥AU Agentæ¨¡å‹è·¯å¾„
3. æ£€æŸ¥GPUæ˜¾å­˜ï¼ˆéœ€è¦~15GBï¼‰
4. ç¡®è®¤AU Agentåœ¨å•ç‹¬çš„GPUä¸Š

#### Q2: AU resultåŠ è½½å¤±è´¥
```
âš ï¸ AU resultåŠ è½½å¤±è´¥: sample_xxx
```

**åŸå› **:
- MER-Factory JSONæ–‡ä»¶ä¸å­˜åœ¨
- JSONæ–‡ä»¶è·¯å¾„é…ç½®é”™è¯¯

**è§£å†³**:
1. æ£€æŸ¥ `mer_factory_output` è·¯å¾„
2. ç¡®è®¤JSONæ–‡ä»¶å­˜åœ¨: `{mer_factory_output}/{sample_name}/{sample_name}_au_analysis.json`
3. é‡æ–°è¿è¡ŒMER-Factoryæ‰¹å¤„ç†

### â“ ç‰¹å¾æå–ç›¸å…³

#### Q3: summary_descriptionä¸ºç©º
```
âš ï¸ summary_descriptionä¸å­˜åœ¨æˆ–ä¸ºç©º
```

**åŸå› **:
- MER-Factoryæœªä½¿ç”¨AU Agentç”Ÿæˆæè¿°
- JSONæ–‡ä»¶æ˜¯æ—§æ ¼å¼

**è§£å†³**:
```bash
# é‡æ–°è¿è¡Œï¼Œç¡®ä¿ä½¿ç”¨AU Agentæ¨¡å¼
cd /home/project/MER-Factory
python batch_extract_au_multi_datasets.py --mode 2 --gen-method 1
```

#### Q4: CLIPç‰¹å¾ç»´åº¦ä¸åŒ¹é…
```
âŒ Expected shape (N, 512), got (N, 768)
```

**åŸå› **:
- ä½¿ç”¨äº†é”™è¯¯çš„CLIPæ¨¡å‹
- ç‰¹å¾æ–‡ä»¶ç‰ˆæœ¬ä¸å…¼å®¹

**è§£å†³**:
1. AUç‰¹å¾åº”ä½¿ç”¨CLIP ViT-B/32ï¼ˆè¾“å‡º512ç»´ï¼‰
2. åˆ é™¤æ—§çš„ç‰¹å¾æ–‡ä»¶é‡æ–°æå–
3. ç¡®è®¤ `extract_multimodal_features_precompute.py` ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹

### â“ è®­ç»ƒç›¸å…³

#### Q5: æ˜¾å­˜ä¸è¶³
```
CUDA out of memory
```

**è§£å†³**:
1. **å¯ç”¨é¢„æå–ç‰¹å¾**: `use_preextracted_features: True`
2. **è·³è¿‡ç¼–ç å™¨åŠ è½½**: `skip_encoders: True`
3. **å‡å°batch size**: `batch_size: 1`
4. **ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯**: `gradient_accumulation_steps: 4`
5. **AU Agentå•ç‹¬GPU**: `au_agent_device: "cuda:1"`

#### Q6: æ•°æ®åŠ è½½æ…¢
```
DataLoader is too slow
```

**è§£å†³**:
1. ä½¿ç”¨é¢„æå–ç‰¹å¾ï¼ˆæœ€æœ‰æ•ˆï¼‰
2. å¢åŠ  `num_workers`
3. ä½¿ç”¨SSDå­˜å‚¨ç‰¹å¾æ–‡ä»¶
4. å¯ç”¨æ•°æ®é¢„åŠ è½½

### â“ é‡‡æ ·ç­–ç•¥ç›¸å…³

#### Q7: emotion_peaké‡‡æ ·å¤±è´¥
```
âš ï¸ No peak frames found, falling back to uniform sampling
```

**åŸå› **:
- MER-Factory JSONç¼ºå°‘ `au_info` å­—æ®µ
- è§†é¢‘AUå¼ºåº¦è¿‡ä½

**è§£å†³**:
1. è‡ªåŠ¨å›é€€åˆ°uniformé‡‡æ ·ï¼ˆæ­£å¸¸è¡Œä¸ºï¼‰
2. å¦‚éœ€å³°å€¼é‡‡æ ·ï¼Œé‡æ–°è¿è¡ŒMER-Factoryç¡®ä¿ç”Ÿæˆ `au_info`

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### æ–‡ä»¶æ¸…å•

**è„šæœ¬æ–‡ä»¶**:
- `batch_extract_au_multi_datasets.py` - MER-Factoryæ‰¹å¤„ç†
- `extract_multimodal_features_precompute.py` - ç‰¹å¾æå–
- `extract_mercaptionplus_features.sh` - å¿«æ·æå–è„šæœ¬
- `test_single_sample.py` - å•æ ·æœ¬æµ‹è¯•
- `verify_au_pipeline.py` - æµç¨‹éªŒè¯

**é…ç½®æ–‡ä»¶**:
- `train_configs/*.yaml` - è®­ç»ƒé…ç½®
- `eval_configs/*.yaml` - æ¨ç†é…ç½®

**æ–‡æ¡£æ–‡ä»¶**:
- `MY_README.md` - æœ¬æ–‡æ¡£ï¼ˆä¸»æ–‡æ¡£ï¼‰

### ç›¸å…³èµ„æº

- **MER-Factory**: `/home/project/MER-Factory/`
- **AffectGPT**: `/home/project/AffectGPT/AffectGPT/`
- **AU Agentæ¨¡å‹**: `/home/project/AffectGPT/AffectGPT/output/au_agent_qwen2.5_7b_lora/`
- **é¢„æå–ç‰¹å¾**: `./preextracted_features/`

---

## ğŸ“ˆ ç‰ˆæœ¬å†å²

### v2.5.2 (2024-11-24 19:25)
- âœ… **æ¨ç†Reasoningè¾“å‡º**: æ¨ç†é»˜è®¤è¾“å‡ºæ¨ç†è¿‡ç¨‹ï¼Œä¸åªæ˜¯åˆ†ç±»ç»“æœ
- âœ… **å¯æ§è¾“å‡ºæ¨¡å¼**: æ·»åŠ `--no_reasoning`å‚æ•°æ§åˆ¶æ˜¯å¦è¾“å‡ºreasoning
- âœ… **Promptä¼˜åŒ–**: ä½¿ç”¨"Please infer the person's emotional state and provide your reasoning process."

### v2.5.1 (2024-11-24 16:20)
- âœ… **dtypeä¸åŒ¹é…ä¿®å¤**: ä¿®å¤CLIPç¼–ç Float32ä¸æ¨¡å‹Halfä¸åŒ¹é…çš„é—®é¢˜
- âœ… **æ—¥å¿—è¾“å‡ºä¼˜åŒ–**: CLIPæ¨¡å‹åŠ è½½å…¨å±€åªè¾“å‡ºä¸€æ¬¡ï¼Œé¿å…åˆ·å±
- âœ… **è­¦å‘Šä¿¡æ¯ç²¾ç®€**: æ–‡ä»¶ç¼ºå¤±/åŠ è½½å¤±è´¥åªæç¤ºå‰å‡ æ¬¡
- âœ… **æ€§èƒ½ä¼˜åŒ–**: CLIPç‰¹å¾ç›´æ¥åœ¨GPUä¸Šè½¬æ¢ä¸ºhalfï¼Œå‡å°‘CPU-GPUä¼ è¾“

### v2.5 (2024-11-24 16:10)
- âœ… **AUæ¨ç†æ¨¡å¼ä¿®å¤**: ä¿®å¤æ¨ç†æ—¶AUæ•°æ®ç±»å‹é”™è¯¯çš„é—®é¢˜
- âœ… **CLIPå®æ—¶ç¼–ç **: æ¨ç†æ—¶è‡ªåŠ¨å¯ç”¨`use_au_clip_realtime`æ¨¡å¼
- âœ… **è¯¦ç»†è°ƒè¯•ä¿¡æ¯**: æ·»åŠ AUåŠ è½½å’ŒCLIPç¼–ç çš„è¯¦ç»†æ—¥å¿—è¾“å‡º
- âœ… **å®‰å…¨æ£€æŸ¥å¢å¼º**: åœ¨conversationå±‚æ·»åŠ AUæ•°æ®ç±»å‹æ£€æŸ¥ï¼Œé¿å…å´©æºƒ
- âœ… **é”™è¯¯æç¤ºä¼˜åŒ–**: æä¾›æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯å’Œè§£å†³å»ºè®®

### v2.4 (2024-11-23 21:30)
- âœ… **è®­ç»ƒå¯è§†åŒ–é›†æˆ**: è‡ªåŠ¨ç”Ÿæˆå­¦ä¹ ç‡å’ŒLossæ›²çº¿å›¾ï¼Œæ— éœ€é¢å¤–è„šæœ¬
- âœ… **è‡ªåŠ¨ä¿å­˜å›¾ç‰‡**: æ¯ä¸ªepochç»“æŸè‡ªåŠ¨ä¿å­˜æ ‡å‡†ç‰ˆå’Œé«˜æ¸…ç‰ˆæ›²çº¿å›¾
- âœ… **ç»Ÿè®¡ä¿¡æ¯è¾“å‡º**: å®æ—¶æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡ï¼ˆæ­¥æ•°ã€å­¦ä¹ ç‡ã€æœ€ä½³lossç­‰ï¼‰
- âœ… **é›¶é…ç½®ä½¿ç”¨**: é»˜è®¤å¯ç”¨ï¼Œå¯é€šè¿‡é…ç½®æ–‡ä»¶ä¸€é”®å¼€å…³
- âœ… **æ•°æ®æŒä¹…åŒ–**: è‡ªåŠ¨ä¿å­˜åŸå§‹æ•°æ®ä¸º.npzæ–‡ä»¶ï¼Œæ”¯æŒåç»­åˆ†æ

### v2.3 (2024-11-23 20:40)
- âœ… **è®­ç»ƒé‡‡æ ·æœºåˆ¶è¯¦è§£**: æ·»åŠ å®Œæ•´çš„è¿­ä»£å™¨æ¨¡å¼è®­ç»ƒæœºåˆ¶è¯´æ˜
- âœ… **å¾ªç¯é‡‡æ ·åŸç†**: è¯¦ç»†è§£é‡Šæ•°æ®é›†å¦‚ä½•å¾ªç¯ä½¿ç”¨å’Œéšæœºæ‰“ä¹±
- âœ… **Warmupæœºåˆ¶è¯¦è§£**: å®Œæ•´çš„å­¦ä¹ ç‡é¢„çƒ­æœºåˆ¶è¯´æ˜ï¼ŒåŒ…æ‹¬å…¬å¼ã€å¯è§†åŒ–å’Œæœ€ä½³å®è·µ
- âœ… **å­¦ä¹ ç‡è°ƒåº¦**: LinearWarmupCosineLRç­–ç•¥è¯¦è§£ï¼ˆçº¿æ€§å¢é•¿+ä½™å¼¦è¡°å‡ï¼‰
- âœ… **è®¡ç®—å…³ç³»è¯´æ˜**: æ ·æœ¬è®¿é—®æ¬¡æ•°ã€æ€»æ­¥æ•°ã€warmupæ¯”ä¾‹ç­‰è¯¦ç»†è®¡ç®—
- âœ… **å®é™…ç¤ºä¾‹**: åŸºäº5000æ ·æœ¬ã€1000 iters_per_epochçš„å®Œæ•´ç¤ºä¾‹
- âœ… **è°ƒè¯•æŠ€å·§**: å­¦ä¹ ç‡æ›²çº¿æå–å’Œå¯è§†åŒ–æ–¹æ³•

### v2.2 (2024-11-23 16:45)
- âœ… **ä¿®å¤è·¯å¾„bug**: ä¿®å¤æ¨ç†æ—¶AU JSONè·¯å¾„ç¼ºå°‘æ•°æ®é›†åç§°å±‚çº§çš„é—®é¢˜
- âœ… **è‡ªåŠ¨è·¯å¾„æ„å»º**: ä»£ç ç°åœ¨è‡ªåŠ¨ä» `self.dataset` è·å–æ•°æ®é›†åç§°å¹¶æ„å»ºå®Œæ•´è·¯å¾„
- âœ… **å¤šæ•°æ®é›†æ”¯æŒ**: æ¨ç†æ—¶å¯ä»¥ç”¨åŒä¸€ä¸ª `mer_factory_output` æ ¹è·¯å¾„å¤„ç†å¤šä¸ªæ•°æ®é›†
- âœ… **è·¯å¾„è¯´æ˜æ–‡æ¡£**: åœ¨MY_README.mdä¸­æ·»åŠ è¯¦ç»†çš„è·¯å¾„é…ç½®è¯´æ˜

### v2.1 (2024-11-23 16:30)
- âœ… **æ•´åˆæ‰€æœ‰MDæ–‡æ¡£**: åˆ é™¤é›¶æ•£MDæ–‡ä»¶ï¼Œæ‰€æœ‰å†…å®¹ç»Ÿä¸€åœ¨ `MY_README.md`
- âœ… **AUä¸‰ç§æ¨¡å¼**: æ·»åŠ æ¨¡å¼å¯¹æ¯”ï¼ˆé¢„æå–/å®æ—¶CLIP/AU Agentï¼‰
- âœ… **å¿«é€Ÿå¼€å§‹æŒ‡å—**: ä¸ä½¿ç”¨AU Agentçš„å®Œæ•´æµç¨‹
- âœ… **ç¼–ç å™¨è·³è¿‡ç­–ç•¥**: è¯¦ç»†è¯´æ˜ `skip_encoders` é…ç½®
- âœ… **é…ç½®æ–‡ä»¶å¯¹æ¯”**: åŸå§‹é…ç½® vs æ–°é…ç½®å®Œæ•´å¯¹æ¯”
- âœ… **æ–°å¢é…ç½®æ–‡ä»¶**: `recommended_train_with_preextracted_au.yaml` å’Œ `recommended_inference_with_clip_realtime.yaml`
- âœ… **ä»£ç ä¿®æ”¹**: `base_dataset.py` æ–°å¢ `_load_au_clip_features_from_json()` æ–¹æ³•

### v2.0 (2024-11-23)
- âœ… æ•´åˆæ‰€æœ‰MDæ–‡æ¡£åˆ° `MY_README.md`
- âœ… æ›´æ–°AUç‰¹å¾æå–æµç¨‹ï¼ˆä½¿ç”¨ `summary_description`ï¼‰
- âœ… æ·»åŠ å®Œæ•´çš„éªŒè¯å’Œæµ‹è¯•å·¥å…·
- âœ… ä¼˜åŒ–æ–‡æ¡£ç»“æ„å’Œä½¿ç”¨è¯´æ˜

### v2024-11-22
- âœ… é›†æˆAU Agentåˆ°è®­ç»ƒå’Œæ¨ç†æµç¨‹
- âœ… æ”¯æŒFrameé‡‡æ ·ç­–ç•¥ï¼ˆuniform/emotion_peakï¼‰
- âœ… æ·»åŠ é¢„æå–ç‰¹å¾ä¼˜åŒ–

### v2024-11-21
- âœ… å®ç°AU Agent LoRAå¾®è°ƒ
- âœ… é›†æˆMER-Factoryæ‰¹å¤„ç†
- âœ… åˆå§‹é¡¹ç›®æ¶æ„

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªåŸAffectGPTé¡¹ç›®è®¸å¯è¯ã€‚

---

**æœ€åæ›´æ–°**: 2024-11-23  
**ç»´æŠ¤è€…**: Project Team  
**è”ç³»æ–¹å¼**: è¯·é€šè¿‡GitHub Issuesåé¦ˆé—®é¢˜
