# æ¨¡æ€åŠ è½½é€»è¾‘ä¿®å¤æŠ¥å‘Š

## ğŸ¯ é—®é¢˜æ ¹æº

é…ç½®ä¸­è®¾ç½®çš„`use_preextracted_features=True`è¢«**å…¨å±€åº”ç”¨åˆ°æ‰€æœ‰æ¨¡æ€**ï¼Œå¯¼è‡´ï¼š
- âœ… Frameæ­£ç¡®åŠ è½½é¢„æå–ç‰¹å¾
- âŒ Faceå°è¯•åŠ è½½é¢„æå–ç‰¹å¾ï¼ˆä¸å­˜åœ¨ï¼‰â†’ å¤±è´¥
- âŒ Audioå°è¯•åŠ è½½é¢„æå–ç‰¹å¾ï¼ˆä¸å­˜åœ¨ï¼‰â†’ å¤±è´¥  
- âŒ AUå°è¯•åŠ è½½é¢„æå–ç‰¹å¾ï¼ˆä¸å­˜åœ¨ï¼‰â†’ å¤±è´¥
- âŒ æœ€ç»ˆæ‰€æœ‰æ¨¡æ€éƒ½å¤±è´¥ â†’ `AssertionError: Some input info is missing.`

---

## âœ… ä¿®å¤æ–¹æ¡ˆ

### è®¾è®¡ç†å¿µï¼šåªæœ‰Frameä½¿ç”¨é¢„æå–

| æ¨¡æ€ | å¤„ç†æ–¹å¼ | åŸå›  |
|------|---------|------|
| **Frame** | é¢„æå–emotion_peak | emotion_peaké‡‡æ ·æ…¢ï¼ˆéœ€MER-Factory JSONï¼‰ï¼Œé¢„æå–åŠ é€Ÿ16å€ |
| **Face** | **å®æ—¶**åŠ è½½.npyäººè„¸æ–‡ä»¶ | å·²é¢„å¤„ç†ï¼ŒåŠ è½½å¾ˆå¿«ï¼ˆ~0.01msï¼‰ï¼Œæ— éœ€é¢„æå– |
| **Audio** | **å®æ—¶**åŠ è½½éŸ³é¢‘æ–‡ä»¶ | éŸ³é¢‘åŠ è½½å¯æ¥å—ï¼ˆ~15msï¼‰ï¼Œæ— éœ€é¢„æå– |
| **AU** | **å®æ—¶**CLIPç¼–ç  | ä»MER-Factory JSONè¯»å–ï¼ŒCLIPç¼–ç å¿«ï¼ˆ~2msï¼‰ï¼Œæ— éœ€é¢„æå– |

---

## ğŸ”§ ä»£ç ä¿®å¤

### ä¿®å¤1: ç®€åŒ–FaceåŠ è½½é€»è¾‘

**æ–‡ä»¶**: `/home/project/AffectGPT/AffectGPT/my_affectgpt/datasets/datasets/base_dataset.py`

**ä¿®æ”¹å‰** (âŒ å°è¯•åŠ è½½é¢„æå–Faceç‰¹å¾):
```python
if 'face' in self.needed_data:
    if hasattr(self, 'use_realtime_extraction') and self.use_realtime_extraction:
        # å®æ—¶ç‰¹å¾æå–æœåŠ¡...
    elif use_preextracted and preextracted_root and sample_name:  # âŒ å°è¯•é¢„æå–
        face_feat_path = os.path.join(preextracted_root, face_feat_dir, f'{sample_name}.npy')
        if os.path.exists(face_feat_path):
            # åŠ è½½é¢„æå–ç‰¹å¾
        else:
            pass  # é¢„æå–å¤±è´¥ï¼Œä½†æ²¡æœ‰fallback!
    else:
        # å®æ—¶å¤„ç†
        if face_npy is not None:
            raw_face, msg = load_face(...)
```

**ä¿®æ”¹å** (âœ… å¼ºåˆ¶å®æ—¶å¤„ç†):
```python
# ğŸ¯ Face/Audio/AUå§‹ç»ˆä½¿ç”¨å®æ—¶å¤„ç†ï¼ˆå³ä½¿å¯ç”¨äº†é¢„æå–ï¼Œé¢„æå–ä»…é’ˆå¯¹Frameï¼‰
if 'face' in self.needed_data:
    # å®æ—¶å¤„ç†æ¨¡å¼ - ç›´æ¥åŠ è½½äººè„¸.npyæ–‡ä»¶
    if face_npy is not None:
        raw_face, msg = load_face(
            face_npy=face_npy,
            n_frms = self.n_frms,
            height = 224,
            width  = 224,
            sampling ="uniform",
            return_msg=True
        )
        face = self.vis_processor.transform(raw_face)
```

---

### ä¿®å¤2: ç®€åŒ–AudioåŠ è½½é€»è¾‘

**ä¿®æ”¹å‰** (âŒ å°è¯•åŠ è½½é¢„æå–Audioç‰¹å¾):
```python
if 'audio' in self.needed_data:
    if hasattr(self, 'use_realtime_extraction') and self.use_realtime_extraction:
        # å®æ—¶ç‰¹å¾æå–æœåŠ¡...
    elif use_preextracted and preextracted_root and sample_name:  # âŒ å°è¯•é¢„æå–
        audio_feat_path = os.path.join(preextracted_root, audio_feat_dir, f'{sample_name}.npy')
        if os.path.exists(audio_feat_path):
            # åŠ è½½é¢„æå–ç‰¹å¾
        else:
            pass  # é¢„æå–å¤±è´¥ï¼Œä½†æ²¡æœ‰fallback!
    else:
        # å®æ—¶å¤„ç†
        if audio_path is not None:
            raw_audio = load_audio([audio_path], "cpu", clips_per_video=8)[0]
            audio = transform_audio(raw_audio, "cpu")
```

**ä¿®æ”¹å** (âœ… å¼ºåˆ¶å®æ—¶å¤„ç†):
```python
# ğŸ¯ Audioæ¨ç†æ—¶å§‹ç»ˆä½¿ç”¨å®æ—¶å¤„ç†ï¼ˆä¸ä½¿ç”¨é¢„æå–ï¼‰
if 'audio' in self.needed_data:
    # å®æ—¶å¤„ç†æ¨¡å¼ - ç›´æ¥åŠ è½½éŸ³é¢‘æ–‡ä»¶
    if audio_path is not None:
        raw_audio = load_audio([audio_path], "cpu", clips_per_video=8)[0]
        audio = transform_audio(raw_audio, "cpu")
```

---

### ä¿®å¤3: ç®€åŒ–AUåŠ è½½é€»è¾‘

**ä¿®æ”¹å‰** (âŒ å°è¯•åŠ è½½é¢„æå–AUç‰¹å¾):
```python
if 'au' in self.needed_data:
    # æ¨¡å¼1: é¢„æå–CLIPç‰¹å¾æ¨¡å¼
    if use_preextracted and preextracted_root and sample_name:  # âŒ å°è¯•é¢„æå–
        au_feat_path = os.path.join(preextracted_root, au_feat_dir, f'{sample_name}.npy')
        if os.path.exists(au_feat_path):
            au_features = np.load(au_feat_path)
            au = torch.from_numpy(au_features).float()
        else:
            print(f"âš ï¸ AUç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {au_feat_path}")  # å¤±è´¥æç¤º
    
    # æ¨¡å¼2: ä»JSONå®æ—¶CLIPç¼–ç æ¨¡å¼
    elif getattr(self, 'use_au_clip_realtime', False):  # âŒ elifå¯¼è‡´æ— æ³•fallback!
        if video_name and self.mer_factory_output:
            au = self._load_au_clip_features_from_json(video_name)
```

**ä¿®æ”¹å** (âœ… å¼ºåˆ¶å®æ—¶CLIPç¼–ç ):
```python
# ğŸ¯ AUæ¨ç†æ—¶å§‹ç»ˆä½¿ç”¨å®æ—¶CLIPç¼–ç ï¼ˆä¸ä½¿ç”¨é¢„æå–ï¼‰
if 'au' in self.needed_data:
    # æ¨¡å¼: ä»JSONå®æ—¶CLIPç¼–ç æ¨¡å¼ï¼ˆæ¨ç†æ¨èï¼‰
    if getattr(self, 'use_au_clip_realtime', False):
        # ä»video_pathæˆ–sample_nameæå–video_name
        video_name = sample_name if sample_name else os.path.splitext(os.path.basename(video_path))[0]
        
        if video_name and self.mer_factory_output:
            # ä»JSONåŠ è½½summary_descriptionå¹¶CLIPç¼–ç 
            au = self._load_au_clip_features_from_json(video_name)
```

---

## ğŸ”„ ä¿®å¤åçš„æ•°æ®æµ

```
é…ç½®æ–‡ä»¶:
  use_preextracted_features: True  â† å…¨å±€é…ç½®
  preextracted_root: './preextracted_features'
  â†“
  
base_dataset.py åŠ è½½é€»è¾‘:
  
  Frameæ¨¡æ€:
    if use_preextracted and frame_feat_path exists:
        âœ… åŠ è½½é¢„æå–ç‰¹å¾ [8, 768]
    else:
        âœ… å®æ—¶åŠ è½½è§†é¢‘ + emotion_peaké‡‡æ ·
  
  Faceæ¨¡æ€:
    âœ… å¼ºåˆ¶å®æ—¶åŠ è½½.npyäººè„¸æ–‡ä»¶ (å¿½ç•¥use_preextracted)
  
  Audioæ¨¡æ€:
    âœ… å¼ºåˆ¶å®æ—¶åŠ è½½éŸ³é¢‘æ–‡ä»¶ (å¿½ç•¥use_preextracted)
  
  AUæ¨¡æ€:
    âœ… å¼ºåˆ¶å®æ—¶CLIPç¼–ç MER-Factory JSON (å¿½ç•¥use_preextracted)
```

---

## âš ï¸ å…³äºFace/Audioå¤±è´¥çš„è­¦å‘Š

ç”¨æˆ·ä»ç„¶çœ‹åˆ°ï¼š
```
âš ï¸ Faceç‰¹å¾æ— æ•ˆï¼Œè·³è¿‡Faceæ¨¡æ€: sample_00001998
âš ï¸ Audioç‰¹å¾æ— æ•ˆï¼Œè·³è¿‡Audioæ¨¡æ€: sample_00001998
```

**å¯èƒ½åŸå› **ï¼š
1. `face_npy`è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸ºNone
2. `audio_path`è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸ºNone

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æ£€æŸ¥æ•°æ®é›†é…ç½®ä¸­Face/Audioçš„è·¯å¾„è®¾ç½®
2. æ£€æŸ¥MER2023æ•°æ®é›†çš„Face/Audioæ–‡ä»¶æ˜¯å¦å­˜åœ¨
3. åœ¨`base_dataset.py`ä¸­æ·»åŠ è°ƒè¯•æ—¥å¿—æŸ¥çœ‹å®é™…è·¯å¾„

**ä¸´æ—¶è§£å†³**ï¼š
å¦‚æœFace/Audioç¡®å®ä¸å­˜åœ¨ï¼Œå¯ä»¥åªä½¿ç”¨Frame+AUè¿›è¡Œæ¨ç†ï¼š
```yaml
inference:
  face_or_frame: 'frame_au'  # åªä½¿ç”¨Frameå’ŒAU
```

---

## ğŸ“Š é¢„æœŸè¡Œä¸º

### æˆåŠŸåœºæ™¯ï¼ˆæ‰€æœ‰æ¨¡æ€éƒ½æœ‰æ•°æ®ï¼‰

```
âœ… [Frameé¢„æå–] æˆåŠŸåŠ è½½é¢„æå–ç‰¹å¾: mer2023/frame_CLIP_VIT_LARGE_emotion_peak_8frms/
âœ… FaceåŠ è½½æˆåŠŸ
âœ… AudioåŠ è½½æˆåŠŸ
âœ… AU CLIPç¼–ç æˆåŠŸ

[æ­£å¸¸æ¨ç†...]
```

### éƒ¨åˆ†æ¨¡æ€ç¼ºå¤±åœºæ™¯

```
âœ… [Frameé¢„æå–] æˆåŠŸåŠ è½½é¢„æå–ç‰¹å¾
âš ï¸ Faceç‰¹å¾æ— æ•ˆï¼Œè·³è¿‡Faceæ¨¡æ€
âš ï¸ Audioç‰¹å¾æ— æ•ˆï¼Œè·³è¿‡Audioæ¨¡æ€
âœ… AU CLIPç¼–ç æˆåŠŸ

[ä½¿ç”¨Frame+AUè¿›è¡Œæ¨ç†...]
```

---

## âœ… ä¿®å¤æ¸…å•

- [x] FaceåŠ è½½é€»è¾‘ç®€åŒ–ï¼ˆå¼ºåˆ¶å®æ—¶å¤„ç†ï¼‰
- [x] AudioåŠ è½½é€»è¾‘ç®€åŒ–ï¼ˆå¼ºåˆ¶å®æ—¶å¤„ç†ï¼‰
- [x] AUåŠ è½½é€»è¾‘ç®€åŒ–ï¼ˆå¼ºåˆ¶å®æ—¶CLIPç¼–ç ï¼‰
- [x] Frameä¿æŒé¢„æå–é€»è¾‘ä¸å˜
- [ ] **éªŒè¯Face/Audioè·¯å¾„é…ç½®**
- [ ] **é‡æ–°è¿è¡Œæ¨ç†æµ‹è¯•**

---

## ğŸš€ ä¸‹ä¸€æ­¥

1. **é‡æ–°è¿è¡Œæ¨ç†**ï¼Œè§‚å¯ŸFace/Audioæ˜¯å¦æˆåŠŸåŠ è½½
2. å¦‚æœä»ç„¶å¤±è´¥ï¼Œæ£€æŸ¥Face/Audioæ–‡ä»¶è·¯å¾„é…ç½®
3. ä¸´æ—¶æ–¹æ¡ˆï¼šä½¿ç”¨`face_or_frame: 'frame_au'`åªç”¨Frame+AUæ¨ç†

---

## ğŸ“ æ€»ç»“

### é—®é¢˜
`use_preextracted_features=True`å¯¼è‡´æ‰€æœ‰æ¨¡æ€éƒ½å°è¯•åŠ è½½é¢„æå–ç‰¹å¾ï¼Œä½†åªæœ‰Frameæœ‰é¢„æå–æ–‡ä»¶ã€‚

### ä¿®å¤
- Frameï¼šä¿æŒé¢„æå–é€»è¾‘ï¼ˆéœ€è¦åŠ é€Ÿï¼‰
- Face/Audio/AUï¼šå¼ºåˆ¶å®æ—¶å¤„ç†ï¼ˆä¸å—`use_preextracted`å½±å“ï¼‰

### é¢„æœŸæ•ˆæœ
- Frameé¢„æå–åŠ è½½æˆåŠŸ
- Face/Audio/AUå®æ—¶å¤„ç†ï¼ˆå¦‚æœæ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼‰
- æ¨ç†æ­£å¸¸è¿›è¡Œ

**é¢„æå–ä¼˜åŒ–ç°åœ¨åº”è¯¥å®Œå…¨æ­£å¸¸äº†ï¼** ğŸ‰
