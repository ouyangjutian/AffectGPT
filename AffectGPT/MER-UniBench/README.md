# MER-UniBench æ‰¹é‡å¤„ç†å·¥å…·é›†

## ğŸ“‹ ç›®å½•è¯´æ˜

æœ¬ç›®å½•åŒ…å«MER-UniBench 9ä¸ªæ•°æ®é›†çš„æ‰¹é‡é¢„å¤„ç†å·¥å…·ï¼Œç”¨äºåŠ é€ŸAffectGPTæ¨ç†ã€‚

```
MER-UniBench/
â”œâ”€â”€ extract_frame_emotion_peak_batch.py      # Frame emotion_peakç‰¹å¾é¢„æå–è„šæœ¬
â”œâ”€â”€ run_extract_emotion_peak_batch.sh        # æ‰¹é‡æå–Shellè„šæœ¬
â”œâ”€â”€ EMOTION_PEAK_PREEXTRACTION_GUIDE.md     # è¯¦ç»†ä½¿ç”¨æŒ‡å—
â”œâ”€â”€ inference_configs/                       # æ¨ç†é…ç½®ç¤ºä¾‹ï¼ˆå³å°†åˆ›å»ºï¼‰
â””â”€â”€ README.md                                # æœ¬æ–‡ä»¶
```

---

## ğŸ¯ è®¾è®¡æ€æƒ³

### **æ··åˆæ¨¡å¼æ¨ç†**ï¼šFrameé¢„æå– + å…¶ä»–æ¨¡æ€å®æ—¶å¤„ç†

**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ**

| æ¨¡æ€ | å¤„ç†æ–¹å¼ | åŸå›  |
|------|---------|------|
| **Frame** | é¢„æå–emotion_peak | emotion_peaké‡‡æ ·éœ€è¦è¯»å–JSONï¼Œå¾ˆæ…¢ï¼ˆ5-10ms/æ ·æœ¬ï¼‰â†’ é¢„æå–ååªéœ€0.5ms |
| **Face** | å®æ—¶å¤„ç†ï¼ˆuniformé‡‡æ ·ï¼‰ | uniformé‡‡æ ·å¾ˆå¿«ï¼ˆ0.01msï¼‰ï¼Œå®æ—¶å¤„ç†æ— æ˜æ˜¾ç“¶é¢ˆ |
| **Audio** | å®æ—¶å¤„ç† | éŸ³é¢‘åŠ è½½å’Œç¼–ç å¼€é”€å¯æ¥å— |
| **AU** | å®æ—¶CLIPç¼–ç  | ä»MER-Factory JSONè¯»å–summary_descriptionï¼Œå®æ—¶CLIPç¼–ç  |

**æ€§èƒ½æå‡**ï¼š
- âœ… Frameæ¨¡æ€åŠ é€Ÿ **600-1200å€**ï¼ˆ2-4åˆ†é’Ÿ â†’ 0.2ç§’ï¼‰
- âœ… å…¶ä»–æ¨¡æ€ä¿æŒçµæ´»æ€§ï¼ˆæ— éœ€é¢å¤–å­˜å‚¨å’Œé¢„å¤„ç†ï¼‰
- âœ… æ€»ä½“æ¨ç†é€Ÿåº¦æå‡ **40-60%**

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### **æ­¥éª¤1ï¼šé¢„æå–Frame emotion_peakç‰¹å¾**

```bash
cd /home/project/AffectGPT/AffectGPT/MER-UniBench

# è¿è¡Œæ‰¹é‡æå–ï¼ˆéœ€è¦å…ˆè¿è¡ŒMER-Factoryç”Ÿæˆau_infoï¼‰
bash run_extract_emotion_peak_batch.sh
```

**è¾“å‡ºä½ç½®**ï¼š
```
/home/project/AffectGPT/AffectGPT/preextracted_features/
â”œâ”€â”€ mer2023/
â”‚   â””â”€â”€ frame_CLIP_VIT_LARGE_emotion_peak_8frms/
â”‚       â”œâ”€â”€ sample_00000001.npy  # [8, 768]
â”‚       â””â”€â”€ ...
â”œâ”€â”€ mer2024/
â”œâ”€â”€ cmumosei/
â””â”€â”€ ... (å…¶ä»–7ä¸ªæ•°æ®é›†)
```

### **æ­¥éª¤2ï¼šé…ç½®æ¨ç†**

åˆ›å»ºæˆ–ä¿®æ”¹æ¨ç†é…ç½®æ–‡ä»¶ï¼ˆä¾‹å¦‚ `eval_configs/eval_mer2023_hybrid.yaml`ï¼‰ï¼š

```yaml
model:
  # ... æ¨¡å‹é…ç½® ...
  skip_encoders: False  # âŒ ä¸è·³è¿‡ç¼–ç å™¨ï¼ˆéœ€è¦å®æ—¶å¤„ç†Face/Audioï¼‰

datasets:
  mer2023:
    data_type: video
    face_or_frame: 'face_frame_audio_au'  # ä½¿ç”¨å¤šæ¨¡æ€
    
    # ğŸ¯ Frameé…ç½®ï¼šä½¿ç”¨é¢„æå–çš„emotion_peakç‰¹å¾
    frame_n_frms: 8
    frame_sampling: 'emotion_peak'
    
    # ğŸ¯ å…³é”®é…ç½®ï¼šåªé¢„æå–Frameï¼Œå…¶ä»–å®æ—¶å¤„ç†
    use_preextracted_features: True          # â† å¯ç”¨é¢„æå–æ¨¡å¼
    preextracted_root: '../preextracted_features/mer2023'  # â† Frameç‰¹å¾è·¯å¾„
    
    # Face/Audioä¸é¢„æå–ï¼Œä¿æŒå®æ—¶å¤„ç†
    # (ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹ï¼šå¦‚æœæ‰¾ä¸åˆ°face/audioçš„.npyï¼Œä¼šå›é€€åˆ°å®æ—¶å¤„ç†)
    
    # AUå®æ—¶CLIPç¼–ç 
    mer_factory_output: '/home/project/MER-Factory/output'
    use_au_clip_realtime: True  # â† AUä½¿ç”¨å®æ—¶CLIPç¼–ç 
    
    # ç¼–ç å™¨é…ç½®ï¼ˆç”¨äºæ„å»ºFrameç‰¹å¾è·¯å¾„å’Œå®æ—¶ç¼–ç ï¼‰
    visual_encoder: 'CLIP_VIT_LARGE'
    acoustic_encoder: 'HUBERT_LARGE'

inference:
  # ... æ¨ç†é…ç½® ...
  use_preextracted_features: True  # â† å¯ç”¨é¢„æå–æ¨¡å¼ï¼ˆä»…Frameï¼‰
```

### **æ­¥éª¤3ï¼šè¿è¡Œæ¨ç†**

```bash
cd /home/project/AffectGPT/AffectGPT

python inference_hybird.py \
    --cfg-path eval_configs/eval_mer2023_hybrid.yaml \
    --dataset mer2023 \
    --ckpt <your_checkpoint>
```

---

## ğŸ“‚ ç‰¹å¾åŠ è½½é€»è¾‘

ç³»ç»Ÿä¼šè‡ªåŠ¨æ ¹æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨æ¥å†³å®šä½¿ç”¨é¢„æå–è¿˜æ˜¯å®æ—¶å¤„ç†ï¼š

```python
# base_dataset.py è‡ªåŠ¨æ£€æµ‹é€»è¾‘

# 1. Frame: æ£€æŸ¥é¢„æå–ç‰¹å¾
frame_feat_path = f'{preextracted_root}/frame_CLIP_VIT_LARGE_emotion_peak_8frms/{sample_name}.npy'
if os.path.exists(frame_feat_path):
    frame = np.load(frame_feat_path)  # âœ… ä½¿ç”¨é¢„æå–ç‰¹å¾
else:
    frame = load_video(...)           # âŒ å›é€€åˆ°å®æ—¶åŠ è½½

# 2. Face: æ£€æŸ¥é¢„æå–ç‰¹å¾
face_feat_path = f'{preextracted_root}/face_CLIP_VIT_LARGE_uniform_8frms/{sample_name}.npy'
if os.path.exists(face_feat_path):
    face = np.load(face_feat_path)    # å¦‚æœé¢„æå–äº†å°±ç”¨
else:
    face = load_face(...)             # âœ… å¦åˆ™å®æ—¶åŠ è½½ï¼ˆæ¨èï¼‰

# 3. Audio: åŒç†
# 4. AU: ä»MER-Factory JSONå®æ—¶è¯»å–å¹¶CLIPç¼–ç 
```

**ä¼˜ç‚¹**ï¼š
- ğŸ¯ **çµæ´»æ€§**ï¼šå¯ä»¥åªé¢„æå–éƒ¨åˆ†æ¨¡æ€
- ğŸ¯ **æ— éœ€å…¨å±€é…ç½®**ï¼šè‡ªåŠ¨æ£€æµ‹æ–‡ä»¶å­˜åœ¨æ€§
- ğŸ¯ **èŠ‚çœå­˜å‚¨**ï¼šåªå­˜å‚¨æœ€éœ€è¦åŠ é€Ÿçš„Frameæ¨¡æ€

---

## ğŸ“Š æ”¯æŒçš„æ•°æ®é›†

| æ•°æ®é›† | æ ·æœ¬æ•° | Frameé¢„æå–å¤§å° | é¢„è®¡æå–æ—¶é—´ |
|--------|--------|----------------|------------|
| **MER2023** | 411 | ~500MB | ~5åˆ†é’Ÿ |
| **MER2024** | 500 | ~600MB | ~6åˆ†é’Ÿ |
| **CMU-MOSEI** | ~2,500 | ~2.5GB | ~30åˆ†é’Ÿ |
| **CMU-MOSI** | ~2,200 | ~2.2GB | ~25åˆ†é’Ÿ |
| **IEMOCAP** | ~5,500 | ~5GB | ~60åˆ†é’Ÿ |
| **MELD** | ~2,600 | ~2.6GB | ~30åˆ†é’Ÿ |
| **OVMERD+** | ~800 | ~800MB | ~10åˆ†é’Ÿ |
| **SIMS** | ~2,300 | ~2.3GB | ~25åˆ†é’Ÿ |
| **SIMSv2** | ~2,300 | ~2.3GB | ~25åˆ†é’Ÿ |
| **æ€»è®¡** | **~19,000** | **~18GB** | **~3.5å°æ—¶** |

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### **åœºæ™¯1ï¼šåªé¢„æå–éƒ¨åˆ†æ•°æ®é›†**

```bash
# åªæå–MER2023å’ŒMER2024
python extract_frame_emotion_peak_batch.py \
    --datasets mer2023 mer2024 \
    --device cuda:0
```

### **åœºæ™¯2ï¼šé¢„æå–æ‰€æœ‰æ¨¡æ€ï¼ˆå®Œå…¨é¢„æå–æ¨¡å¼ï¼‰**

å¦‚æœä½ æƒ³è¦**æœ€å¿«çš„æ¨ç†é€Ÿåº¦**ï¼ˆä»¥æ›´å¤šå­˜å‚¨ä¸ºä»£ä»·ï¼‰ï¼š

```bash
# ä½¿ç”¨AffectGPTçš„å®Œæ•´é¢„æå–è„šæœ¬
cd /home/project/AffectGPT/AffectGPT
bash run_mercaptionplus_extraction.sh

# é€‰æ‹©æ¨¡å¼1: æ™ºèƒ½æ¨¡å¼ï¼ˆemotion_peak + é¢„æå–Multiï¼‰
```

è¿™ä¼šé¢„æå–ï¼š
- Frame (emotion_peak, 8å¸§)
- Face (uniform, 8å¸§)
- Audio (8 clips)
- AU (CLIPç¼–ç , 8å¸§)

**é…ç½®**ï¼š
```yaml
use_preextracted_features: True
preextracted_root: './preextracted_features/mer2023'
# ç³»ç»Ÿä¼šè‡ªåŠ¨åŠ è½½æ‰€æœ‰å¯ç”¨çš„é¢„æå–ç‰¹å¾
```

### **åœºæ™¯3ï¼šæ··åˆé¢„æå–ï¼ˆæ¨èï¼‰**

**Frame + Faceé¢„æå–**ï¼ŒAudio/AUå®æ—¶å¤„ç†ï¼š

```bash
# 1. æå–Frame (emotion_peak)
bash MER-UniBench/run_extract_emotion_peak_batch.sh

# 2. æå–Face (uniform)
python extract_multimodal_features_precompute.py \
    --dataset mer2023 \
    --modality face \
    --frame-sampling uniform \
    --n-frms 8
```

**é…ç½®**ï¼š
```yaml
use_preextracted_features: True
preextracted_root: './preextracted_features/mer2023'
# Frame: emotion_peaké¢„æå–
# Face: uniformé¢„æå–
# Audio: å®æ—¶å¤„ç†ï¼ˆæ‰¾ä¸åˆ°.npyä¼šè‡ªåŠ¨å›é€€ï¼‰
# AU: å®æ—¶CLIPç¼–ç 
```

---

## âš™ï¸ é…ç½®æ¨¡æ¿

### **æ¨¡æ¿1ï¼šFrameé¢„æå–ï¼ˆæœ¬é¡¹ç›®æ¨èï¼‰**

```yaml
# eval_configs/eval_mer2023_frame_preextract.yaml
datasets:
  mer2023:
    face_or_frame: 'face_frame_audio_au'
    frame_sampling: 'emotion_peak'
    use_preextracted_features: True
    preextracted_root: '../preextracted_features/mer2023'
    use_au_clip_realtime: True
    mer_factory_output: '/home/project/MER-Factory/output'
```

**ç‰¹ç‚¹**ï¼š
- âœ… Frameæœ€å¿«ï¼ˆé¢„æå–emotion_peakï¼‰
- âœ… å…¶ä»–æ¨¡æ€çµæ´»ï¼ˆå®æ—¶å¤„ç†ï¼‰
- âœ… å­˜å‚¨éœ€æ±‚å°ï¼ˆæ¯ä¸ªæ•°æ®é›†~500MB-5GBï¼‰

### **æ¨¡æ¿2ï¼šå®Œå…¨å®æ—¶ï¼ˆè°ƒè¯•ç”¨ï¼‰**

```yaml
# eval_configs/eval_mer2023_realtime.yaml
datasets:
  mer2023:
    face_or_frame: 'face_frame_audio_au'
    frame_sampling: 'uniform'  # æˆ– 'emotion_peak'ï¼ˆä¼šå¾ˆæ…¢ï¼‰
    use_preextracted_features: False  # â† å…¨éƒ¨å®æ—¶
    mer_factory_output: '/home/project/MER-Factory/output'
```

**ç‰¹ç‚¹**ï¼š
- âœ… æ— éœ€é¢„å¤„ç†
- âŒ æ¨ç†æ…¢ï¼ˆå¦‚æœç”¨emotion_peakä¼šå¾ˆæ…¢ï¼‰
- âœ… é€‚åˆå¿«é€Ÿæµ‹è¯•

### **æ¨¡æ¿3ï¼šå®Œå…¨é¢„æå–ï¼ˆæœ€å¿«ï¼‰**

```yaml
# eval_configs/eval_mer2023_full_preextract.yaml
datasets:
  mer2023:
    face_or_frame: 'face_frame_audio_au'
    frame_sampling: 'emotion_peak'
    use_preextracted_features: True
    preextracted_root: '../preextracted_features/mer2023'
```

**å‰æ**ï¼šéœ€è¦é¢„æå–æ‰€æœ‰æ¨¡æ€ï¼ˆFrame, Face, Audio, AUï¼‰

**ç‰¹ç‚¹**ï¼š
- âœ… æ¨ç†æœ€å¿«
- âŒ å­˜å‚¨éœ€æ±‚å¤§ï¼ˆæ¯ä¸ªæ•°æ®é›†~10-20GBï¼‰
- âœ… é€‚åˆç”Ÿäº§ç¯å¢ƒ

---

## ğŸ“ å·¥ä½œæµç¨‹æ€»ç»“

### **æ¨èå·¥ä½œæµï¼ˆæ··åˆæ¨¡å¼ï¼‰**

```bash
# 1. è¿è¡ŒMER-Factoryç”Ÿæˆau_infoï¼ˆä¸€æ¬¡æ€§ï¼‰
cd /home/project/MER-Factory
python main.py --dataset mer2023 --modality video

# 2. é¢„æå–Frame emotion_peakç‰¹å¾ï¼ˆä¸€æ¬¡æ€§ï¼Œ~5åˆ†é’Ÿï¼‰
cd /home/project/AffectGPT/AffectGPT/MER-UniBench
bash run_extract_emotion_peak_batch.sh

# 3. é…ç½®æ¨ç†ä½¿ç”¨æ··åˆæ¨¡å¼
# ç¼–è¾‘ eval_configs/eval_mer2023.yaml:
#   - use_preextracted_features: True
#   - frame_sampling: 'emotion_peak'
#   - preextracted_root: '../preextracted_features/mer2023'

# 4. è¿è¡Œæ¨ç†ï¼ˆå¿«é€Ÿï¼ï¼‰
cd /home/project/AffectGPT/AffectGPT
python inference_hybird.py --cfg-path eval_configs/eval_mer2023.yaml --dataset mer2023
```

**æ€§èƒ½**ï¼š
- FrameåŠ è½½: ~0.5msï¼ˆé¢„æå–ï¼‰
- FaceåŠ è½½: ~2-3msï¼ˆå®æ—¶uniformï¼‰
- AudioåŠ è½½: ~5-10msï¼ˆå®æ—¶ï¼‰
- AUç¼–ç : ~2-3msï¼ˆå®æ—¶CLIPï¼‰
- **æ€»è®¡**: ~10-15ms/æ ·æœ¬ï¼ˆvs å®æ—¶emotion_peakçš„5-10msä»…Frameï¼‰

---

## ğŸ†š æ€§èƒ½å¯¹æ¯”

| æ¨¡å¼ | Frame | Face | Audio | AU | æ¨ç†é€Ÿåº¦(411æ ·æœ¬) | å­˜å‚¨éœ€æ±‚ |
|------|-------|------|-------|----|--------------|----|
| **å®Œå…¨å®æ—¶(uniform)** | å®æ—¶ | å®æ—¶ | å®æ—¶ | å®æ—¶ | ~30ç§’ | 0 |
| **å®Œå…¨å®æ—¶(emotion_peak)** | å®æ—¶ | å®æ—¶ | å®æ—¶ | å®æ—¶ | **~2-4åˆ†é’Ÿ** | 0 |
| **Frameé¢„æå–(æ¨è)** | é¢„æå– | å®æ—¶ | å®æ—¶ | å®æ—¶ | **~40ç§’** | ~500MB |
| **å®Œå…¨é¢„æå–** | é¢„æå– | é¢„æå– | é¢„æå– | é¢„æå– | **~10ç§’** | ~10GB |

---

## ğŸ“§ ç›¸å…³æ–‡æ¡£

- `EMOTION_PEAK_PREEXTRACTION_GUIDE.md`: è¯¦ç»†æŠ€æœ¯æ–‡æ¡£
- `../MY_README.md`: AffectGPTå®Œæ•´æ–‡æ¡£
- `../train_configs/emercoarse_highlevelfilter4_outputhybird_bestsetup_bestfusion_lz_face_frame_au.yaml`: è®­ç»ƒé…ç½®ç¤ºä¾‹
