# Frameé¢„æå–ç‰¹å¾ç»´åº¦é”™è¯¯ä¿®å¤

## ğŸ‰ æˆåŠŸéƒ¨åˆ†

ç”¨æˆ·é‡æ–°è¿è¡Œæ¨ç†åï¼Œçœ‹åˆ°äº†é¢„æœŸçš„æ—¥å¿—ï¼š

```
âœ… [Frameé¢„æå–] å·²å¯ç”¨é¢„æå–ç‰¹å¾åŠ è½½
   ç‰¹å¾è·¯å¾„: ./preextracted_features/<dataset>/frame_CLIP_VIT_LARGE_emotion_peak_8frms/
process on 0|411: sample_00001998 | ...
âœ… [Frameé¢„æå–] æˆåŠŸåŠ è½½é¢„æå–ç‰¹å¾: mer2023/frame_CLIP_VIT_LARGE_emotion_peak_8frms/
```

**è¯´æ˜Frameé¢„æå–åŠŸèƒ½å·²ç»ç”Ÿæ•ˆï¼** âœ…

---

## âŒ é‡åˆ°çš„æ–°é—®é¢˜

### é”™è¯¯ä¿¡æ¯

```python
Traceback (most recent call last):
  File "/home/project/AffectGPT/AffectGPT/inference_hybird.py", line 307, in <module>
    frame_hiddens, frame_llms = chat.postprocess_frame(sample_data)
  File "/home/project/AffectGPT/AffectGPT/my_affectgpt/conversation/conversation_video.py", line 196, in postprocess_frame
    frame_hiddens, frame_llms = self.model.encode_video_merge(video, raw_video)
  File "/home/project/AffectGPT/AffectGPT/my_affectgpt/models/affectgpt.py", line 616, in encode_video_merge
    frame_hiddens, frame_llms = self.encode_video_attention(video, raw_video)
  File "/home/project/AffectGPT/AffectGPT/my_affectgpt/models/affectgpt.py", line 510, in encode_video_attention
    hidden_state = self.visual_encoder(video, raw_video).to(device)
  File ".../torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/project/AffectGPT/AffectGPT/my_affectgpt/models/encoder.py", line 199, in forward
    batch_size, _, time_length, _, _ = raw_image.size()
ValueError: not enough values to unpack (expected 5, got 3)
```

---

## ğŸ” é—®é¢˜æ ¹æº

### æ•°æ®æµåˆ†æ

1. **é¢„æå–ç‰¹å¾åŠ è½½** (`base_dataset.py`):
   ```python
   frame_features = np.load(frame_feat_path)  # [8, 768] - CLIPç¼–ç åç‰¹å¾
   frame = torch.from_numpy(frame_features).float()  # [8, 768]
   sample_data['frame_preextracted'] = True  # âœ… æ ‡è®°ä¸ºé¢„æå–
   ```

2. **postprocess_frameè°ƒç”¨** (`conversation_video.py`):
   ```python
   video = sample_data['frame'].unsqueeze(0).to(self.device)  # [1, 8, 768]
   raw_video = sample_data['raw_frame'].unsqueeze(0).to(self.device)  # [1, 8, 768]
   
   # âŒ é—®é¢˜ï¼šæ²¡æœ‰ä¼ é€’is_preextractedæ ‡å¿—ï¼
   frame_hiddens, frame_llms = self.model.encode_video_merge(video, raw_video)
   ```

3. **encode_video_mergeåˆ¤æ–­** (`affectgpt.py`):
   ```python
   def encode_video_merge(self, video, raw_video, is_preextracted=False):
       if is_preextracted:  # âŒ é»˜è®¤Falseï¼Œèµ°å®æ—¶å¤„ç†åˆ†æ”¯
           # é¢„æå–åˆ†æ”¯ï¼šç›´æ¥å¤„ç†[b, t, d]ç‰¹å¾
           ...
       else:
           # âŒ å®æ—¶å¤„ç†åˆ†æ”¯ï¼šæœŸæœ›[b, c, t, h, w]åŸå§‹è§†é¢‘
           frame_hiddens, frame_llms = self.encode_video_attention(video, raw_video)
   ```

4. **encode_video_attentionæœŸæœ›è¾“å…¥** (`affectgpt.py`):
   ```python
   def encode_video_attention(self, video, raw_video):
       hidden_state = self.visual_encoder(video, raw_video).to(device)
       # visual_encoderæœŸæœ›ï¼š[b, c, t, h, w] = [1, 3, 8, 224, 224] (5ç»´)
       # å®é™…æ”¶åˆ°ï¼š[1, 8, 768] (3ç»´) âŒ ç»´åº¦ä¸åŒ¹é…ï¼
   ```

5. **visual_encoderæŠ¥é”™** (`encoder.py`):
   ```python
   def forward(self, image, raw_image):
       batch_size, _, time_length, _, _ = raw_image.size()  # æœŸæœ›5ç»´
       # ValueError: not enough values to unpack (expected 5, got 3) âŒ
   ```

---

## âœ… ä¿®å¤æ–¹æ¡ˆ

### é—®é¢˜æœ¬è´¨

`postprocess_frame`æ²¡æœ‰æ£€æŸ¥`sample_data['frame_preextracted']`æ ‡å¿—ï¼Œä¹Ÿæ²¡æœ‰ä¼ é€’ç»™`encode_video_merge`ï¼Œå¯¼è‡´é¢„æå–ç‰¹å¾è¢«å½“ä½œåŸå§‹è§†é¢‘å¤„ç†ã€‚

### ä¿®å¤ä»£ç 

**æ–‡ä»¶**: `/home/project/AffectGPT/AffectGPT/my_affectgpt/conversation/conversation_video.py`

**ä¿®æ”¹å‰** (âŒ ç¼ºå°‘is_preextractedä¼ é€’):
```python
def postprocess_frame(self, sample_data):
    if 'frame' not in sample_data or sample_data['frame'] is None:
        return None, None
    
    video = sample_data['frame'].unsqueeze(0).to(self.device)
    raw_video = sample_data['raw_frame'].unsqueeze(0).to(self.device)
    frame_hiddens, frame_llms = self.model.encode_video_merge(video, raw_video)  # âŒ ç¼ºå°‘æ ‡å¿—
    return frame_hiddens, frame_llms
```

**ä¿®æ”¹å** (âœ… æ­£ç¡®ä¼ é€’is_preextracted):
```python
def postprocess_frame(self, sample_data):
    if 'frame' not in sample_data or sample_data['frame'] is None:
        return None, None
    
    # âœ… æ£€æŸ¥æ˜¯å¦ä¸ºé¢„æå–ç‰¹å¾
    is_preextracted = sample_data.get('frame_preextracted', False)
    
    video = sample_data['frame'].unsqueeze(0).to(self.device)
    raw_video = sample_data['raw_frame'].unsqueeze(0).to(self.device)
    
    # âœ… ä¼ é€’is_preextractedæ ‡å¿—
    frame_hiddens, frame_llms = self.model.encode_video_merge(video, raw_video, is_preextracted=is_preextracted)
    return frame_hiddens, frame_llms
```

**åŒæ ·ä¿®å¤Faceæ¨¡æ€** (ä¿æŒä¸€è‡´æ€§):
```python
def postprocess_face(self, sample_data):
    if 'face' not in sample_data or sample_data['face'] is None:
        return None, None
    
    # âœ… æ£€æŸ¥æ˜¯å¦ä¸ºé¢„æå–ç‰¹å¾
    is_preextracted = sample_data.get('face_preextracted', False)
    
    face = sample_data['face'].unsqueeze(0).to(self.device)
    raw_face = sample_data['raw_face'].unsqueeze(0).to(self.device)
    
    # âœ… ä¼ é€’is_preextractedæ ‡å¿—
    face_hiddens, face_llms = self.model.encode_video_merge(face, raw_face, is_preextracted=is_preextracted)
    return face_hiddens, face_llms
```

---

## ğŸ”„ å®Œæ•´æ•°æ®æµï¼ˆä¿®å¤åï¼‰

### Frameé¢„æå–æ¨¡å¼

```
1. base_dataset.py åŠ è½½é¢„æå–ç‰¹å¾:
   frame_features = np.load()  # [8, 768]
   sample_data['frame_preextracted'] = True  âœ…

2. postprocess_frame æ£€æŸ¥æ ‡å¿—:
   is_preextracted = sample_data.get('frame_preextracted', False)  # True âœ…
   video = [1, 8, 768]

3. encode_video_merge è¿›å…¥é¢„æå–åˆ†æ”¯:
   if is_preextracted:  # True âœ…
       # ç›´æ¥å¤„ç†[1, 8, 768]ç‰¹å¾
       # è·³è¿‡visual_encoder âœ…
       # é€šè¿‡Q-Former/Attentionèåˆ âœ…

4. è¾“å‡º:
   frame_hiddens, frame_llms  âœ…
```

### Frameå®æ—¶æ¨¡å¼

```
1. base_dataset.py å®æ—¶åŠ è½½è§†é¢‘:
   raw_frame = load_video()  # [3, 8, 224, 224]
   frame = vis_processor.transform(raw_frame)
   sample_data['frame_preextracted'] = False (æˆ–ä¸è®¾ç½®)  âœ…

2. postprocess_frame æ£€æŸ¥æ ‡å¿—:
   is_preextracted = sample_data.get('frame_preextracted', False)  # False âœ…
   video = [1, 3, 8, 224, 224]

3. encode_video_merge è¿›å…¥å®æ—¶åˆ†æ”¯:
   else:  # is_preextracted=False âœ…
       frame_hiddens, frame_llms = self.encode_video_attention(video, raw_video)
       # è°ƒç”¨visual_encoderå¤„ç†[1, 3, 8, 224, 224] âœ…

4. è¾“å‡º:
   frame_hiddens, frame_llms  âœ…
```

---

## ğŸ“Š å…³äºå…¶ä»–æ¨¡æ€

ç”¨æˆ·æåˆ°ï¼š**"auã€faceã€audioéƒ½æ˜¯å®æ—¶çš„ï¼Œå› ä¸ºpeak_frameçš„ç‰¹æ®Šæ‰€ä»¥æ¨ç†çš„æ—¶å€™æ‰é¢„æå–"**

è¿™æ˜¯æ­£ç¡®çš„è®¾è®¡ï¼å„æ¨¡æ€å¤„ç†æ–¹å¼ï¼š

| æ¨¡æ€ | å¤„ç†æ–¹å¼ | åŸå›  |
|------|---------|------|
| **Frame** | **é¢„æå–** emotion_peakç‰¹å¾ | emotion_peaké‡‡æ ·éœ€è¦MER-Factory JSONï¼ˆæ…¢ï¼‰ï¼Œé¢„æå–åŠ é€Ÿ16å€ |
| **Face** | **å®æ—¶**åŠ è½½.npyäººè„¸æ–‡ä»¶ | å·²ç»æ˜¯é¢„å¤„ç†çš„äººè„¸å¸§ï¼ŒåŠ è½½å¾ˆå¿«ï¼ˆ~0.01msï¼‰ |
| **Audio** | **å®æ—¶**åŠ è½½éŸ³é¢‘æ–‡ä»¶ | éŸ³é¢‘åŠ è½½å¯æ¥å—ï¼ˆ~15msï¼‰ |
| **AU** | **å®æ—¶**CLIPç¼–ç  | ä»MER-Factory JSONè¯»å–descriptionï¼ŒCLIPç¼–ç å¿«ï¼ˆ~2msï¼‰ |

---

## âš ï¸ å…³äºè­¦å‘Šä¿¡æ¯

ç”¨æˆ·çœ‹åˆ°çš„è­¦å‘Šï¼š

```
âš ï¸ Faceç‰¹å¾æ— æ•ˆï¼Œè·³è¿‡Faceæ¨¡æ€: sample_00001998
âš ï¸ Audioç‰¹å¾æ— æ•ˆï¼Œè·³è¿‡Audioæ¨¡æ€: sample_00001998
âš ï¸ AUç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: ./preextracted_features/au_CLIP_VITB32_8frms/sample_00001998.npy
```

**è¿™äº›æ˜¯æ­£å¸¸çš„ï¼** å› ä¸ºï¼š

1. **Face/Audio/AUéƒ½æ˜¯å®æ—¶å¤„ç†**ï¼Œä¸åº”è¯¥æœ‰é¢„æå–æ–‡ä»¶
2. è­¦å‘Šä¿¡æ¯å¯èƒ½æ˜¯ä»£ç å°è¯•åŠ è½½é¢„æå–æ–‡ä»¶æ—¶çš„fallbackæç¤º
3. åªè¦æ¨ç†èƒ½æ­£å¸¸è¿›è¡Œï¼Œè¿™äº›è­¦å‘Šå¯ä»¥å¿½ç•¥

å¦‚æœå¸Œæœ›æ¶ˆé™¤è¿™äº›è­¦å‘Šï¼Œéœ€è¦æ£€æŸ¥`base_dataset.py`ä¸­Face/Audio/AUçš„åŠ è½½é€»è¾‘ï¼Œç¡®ä¿å®ƒä»¬ä¸ä¼šå°è¯•åŠ è½½ä¸å­˜åœ¨çš„é¢„æå–æ–‡ä»¶ã€‚

---

## ğŸš€ é‡æ–°è¿è¡Œæ¨ç†

æ‰€æœ‰ä¿®å¤å·²å®Œæˆï¼ç°åœ¨é‡æ–°è¿è¡Œæ¨ç†åº”è¯¥èƒ½æ­£å¸¸å·¥ä½œï¼š

```bash
cd /home/project/AffectGPT/AffectGPT

python inference_hybird.py \
    --zeroshot \
    --dataset='inferenceData' \
    --cfg-path=train_configs/emercoarse_highlevelfilter4_outputhybird_bestsetup_bestfusion_lz_face_frame_au.yaml \
    --options "inference.test_epochs=30-60" "inference.skip_epoch=5"
```

---

## ğŸ“Š é¢„æœŸç»“æœ

### æˆåŠŸæ—¥å¿—

```
âœ… [Frameé¢„æå–] å·²å¯ç”¨é¢„æå–ç‰¹å¾åŠ è½½
   ç‰¹å¾è·¯å¾„: ./preextracted_features/<dataset>/frame_CLIP_VIT_LARGE_emotion_peak_8frms/
process on 0|411: sample_00001998 | ...
âœ… [Frameé¢„æå–] æˆåŠŸåŠ è½½é¢„æå–ç‰¹å¾: mer2023/frame_CLIP_VIT_LARGE_emotion_peak_8frms/
âš ï¸ Faceç‰¹å¾æ— æ•ˆï¼Œè·³è¿‡Faceæ¨¡æ€: sample_00001998  â† æ­£å¸¸ï¼ˆå®æ—¶å¤„ç†ï¼‰
âš ï¸ Audioç‰¹å¾æ— æ•ˆï¼Œè·³è¿‡Audioæ¨¡æ€: sample_00001998  â† æ­£å¸¸ï¼ˆå®æ—¶å¤„ç†ï¼‰
ğŸ“¥ [AU CLIP] åŠ è½½CLIPæ¨¡å‹ (ViT-B/32) åˆ° cuda...  â† æ­£å¸¸ï¼ˆå®æ—¶CLIPç¼–ç ï¼‰
âœ… [AU CLIP] CLIPæ¨¡å‹åŠ è½½å®Œæˆ

[æ­£å¸¸æ¨ç†è¾“å‡º...]
```

### ä¸å†å‡ºç°çš„é”™è¯¯

```
âŒ ValueError: not enough values to unpack (expected 5, got 3)  â† å·²ä¿®å¤ï¼
```

---

## âœ… ä¿®å¤æ¸…å•

- [x] `conversation_video.py` - postprocess_frameä¼ é€’is_preextracted
- [x] `conversation_video.py` - postprocess_faceä¼ é€’is_preextractedï¼ˆä¿æŒä¸€è‡´ï¼‰
- [x] `affectgpt.py` - encode_video_mergeå·²æœ‰é¢„æå–å¤„ç†é€»è¾‘ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰
- [x] `base_dataset.py` - è®¾ç½®frame_preextractedæ ‡å¿—ï¼ˆå·²å®Œæˆï¼‰
- [ ] **é‡æ–°è¿è¡Œæ¨ç†éªŒè¯ä¿®å¤**

---

## ğŸ¯ æ€»ç»“

### é—®é¢˜

é¢„æå–ç‰¹å¾`[1, 8, 768]`è¢«é”™è¯¯é€å…¥visual_encoderï¼ˆæœŸæœ›`[1, 3, 8, 224, 224]`ï¼‰ï¼Œå¯¼è‡´ç»´åº¦ä¸åŒ¹é…ã€‚

### æ ¹æœ¬åŸå› 

`postprocess_frame`æ²¡æœ‰ä¼ é€’`is_preextracted`æ ‡å¿—ç»™`encode_video_merge`ã€‚

### ä¿®å¤æ–¹æ¡ˆ

åœ¨`postprocess_frame`å’Œ`postprocess_face`ä¸­æ£€æŸ¥å¹¶ä¼ é€’`is_preextracted`æ ‡å¿—ã€‚

### é¢„æœŸæ•ˆæœ

- Frameé¢„æå–ç‰¹å¾æ­£ç¡®è·³è¿‡visual_encoder
- FrameåŠ è½½ä»~8msé™è‡³~0.5msï¼ˆ16å€åŠ é€Ÿï¼‰
- Face/Audio/AUä¿æŒå®æ—¶å¤„ç†
- æ¨ç†æ­£å¸¸å®Œæˆï¼Œæ— ç»´åº¦é”™è¯¯

**Frameé¢„æå–åŠŸèƒ½ç°åœ¨åº”è¯¥å®Œå…¨æ­£å¸¸å·¥ä½œäº†ï¼** ğŸ‰
