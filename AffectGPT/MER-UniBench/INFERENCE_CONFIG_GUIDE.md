# æ¨ç†é…ç½®æŒ‡å— - Frameé¢„æå–æ··åˆæ¨¡å¼

## ğŸ¯ è®¾è®¡ç†å¿µ

**åªè®©Frameæ¨¡æ€ä½¿ç”¨é¢„æå–ç‰¹å¾ï¼Œå…¶ä»–æ¨¡æ€ï¼ˆFace, Audio, AUï¼‰å®æ—¶å¤„ç†**

### ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

| æ¨¡æ€ | é‡‡æ ·ç­–ç•¥ | å¤„ç†æ–¹å¼ | ç“¶é¢ˆ | ä¼˜åŒ–æ–¹æ¡ˆ |
|------|---------|---------|------|---------|
| **Frame** | emotion_peak | é¢„æå– âœ… | JSONè¯»å–(5-10ms) + ç´¢å¼•è®¡ç®— | é¢„æå–ååªéœ€0.5ms |
| **Face** | uniform | å®æ—¶å¤„ç† âœ… | æ— æ˜æ˜¾ç“¶é¢ˆ(0.01ms) | ä¿æŒå®æ—¶å³å¯ |
| **Audio** | - | å®æ—¶å¤„ç† âœ… | éŸ³é¢‘åŠ è½½å¯æ¥å— | ä¿æŒå®æ—¶å³å¯ |
| **AU** | - | å®æ—¶CLIPç¼–ç  âœ… | CLIPç¼–ç å¿«(2-3ms) | å®æ—¶ç¼–ç å³å¯ |

**æ”¶ç›Š**ï¼š
- âœ… FrameåŠ é€Ÿ **600-1200å€**ï¼ˆæœ€å¤§ç“¶é¢ˆè§£å†³ï¼‰
- âœ… å…¶ä»–æ¨¡æ€ä¿æŒçµæ´»æ€§ï¼ˆæ— éœ€é¢å¤–å­˜å‚¨ï¼‰
- âœ… æ€»ä½“æ¨ç†åŠ é€Ÿ **40-60%**
- âœ… å­˜å‚¨éœ€æ±‚å°ï¼ˆæ¯ä¸ªæ•°æ®é›†ä»…~500MB-5GBï¼‰

---

## ğŸ“‹ é…ç½®ç¤ºä¾‹

### **å®Œæ•´é…ç½®ï¼š`eval_configs/eval_mer2023_frame_preextract.yaml`**

```yaml
model:
  skip_encoders: False  # âŒ ä¸è·³è¿‡ï¼ˆéœ€è¦å®æ—¶ç¼–ç Face/Audioï¼‰
  visual_encoder: "CLIP_VIT_LARGE"
  acoustic_encoder: "HUBERT_LARGE"

datasets:
  mer2023:
    face_or_frame: 'face_frame_audio_au'
    
    # ğŸ¯ Frameé…ç½®ï¼ˆé¢„æå–emotion_peakï¼‰
    frame_n_frms: 8
    frame_sampling: 'emotion_peak'
    use_preextracted_features: True
    preextracted_root: './preextracted_features/mer2023'
    
    # ğŸ¯ Faceé…ç½®ï¼ˆå®æ—¶å¤„ç†ï¼Œç³»ç»Ÿè‡ªåŠ¨æ£€æµ‹ï¼‰
    # æ— éœ€é¢å¤–é…ç½®ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å›é€€åˆ°å®æ—¶load_face()
    
    # ğŸ¯ Audioé…ç½®ï¼ˆå®æ—¶å¤„ç†ï¼Œç³»ç»Ÿè‡ªåŠ¨æ£€æµ‹ï¼‰
    # æ— éœ€é¢å¤–é…ç½®ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å›é€€åˆ°å®æ—¶load_audio()
    
    # ğŸ¯ AUé…ç½®ï¼ˆå®æ—¶CLIPç¼–ç ï¼‰
    mer_factory_output: '/home/project/MER-Factory/output'
    use_au_clip_realtime: True
    
    # ç¼–ç å™¨é…ç½®ï¼ˆç”¨äºæ„å»ºè·¯å¾„ + å®æ—¶ç¼–ç ï¼‰
    visual_encoder: 'CLIP_VIT_LARGE'
    acoustic_encoder: 'HUBERT_LARGE'

inference:
  use_preextracted_features: True  # å¯ç”¨é¢„æå–ï¼ˆä»…Frameï¼‰
  use_au_clip_realtime: True       # AUå®æ—¶ç¼–ç 
  mer_factory_output: '/home/project/MER-Factory/output'
```

---

## ğŸ”§ å…³é”®å‚æ•°è¯´æ˜

### 1. **`use_preextracted_features: True`**

**ä½œç”¨**ï¼šå¯ç”¨é¢„æå–ç‰¹å¾æ£€æµ‹æ¨¡å¼

**è¡Œä¸º**ï¼š
```python
# ç³»ç»Ÿä¼šä¾æ¬¡æ£€æŸ¥æ¯ä¸ªæ¨¡æ€çš„é¢„æå–ç‰¹å¾æ˜¯å¦å­˜åœ¨
for modality in ['frame', 'face', 'audio']:
    feat_path = f'{preextracted_root}/{modality}_.../{sample_name}.npy'
    if os.path.exists(feat_path):
        features = np.load(feat_path)  # âœ… ä½¿ç”¨é¢„æå–
    else:
        features = load_xxx()          # âŒ å›é€€åˆ°å®æ—¶å¤„ç†
```

**æˆ‘ä»¬çš„ç­–ç•¥**ï¼š
- âœ… Frame: æœ‰é¢„æå–æ–‡ä»¶ â†’ åŠ è½½.npy
- âŒ Face: æ— é¢„æå–æ–‡ä»¶ â†’ å®æ—¶load_face()
- âŒ Audio: æ— é¢„æå–æ–‡ä»¶ â†’ å®æ—¶load_audio()

---

### 2. **`frame_sampling: 'emotion_peak'`**

**ä½œç”¨**ï¼šæŒ‡å®šFrameé‡‡æ ·ç­–ç•¥

**ä¸é¢„æå–çš„å…³ç³»**ï¼š
```python
# æ„å»ºFrameç‰¹å¾è·¯å¾„
frame_feat_dir = f'frame_{visual_encoder}_{frame_sampling}_{frame_n_frms}frms'
# ç”Ÿæˆ: frame_CLIP_VIT_LARGE_emotion_peak_8frms

frame_feat_path = os.path.join(preextracted_root, frame_feat_dir, f'{sample_name}.npy')
# å®Œæ•´: ./preextracted_features/mer2023/frame_CLIP_VIT_LARGE_emotion_peak_8frms/sample_00000001.npy
```

**å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨**ï¼š
```python
# ä¼šå›é€€åˆ°å®æ—¶emotion_peaké‡‡æ ·ï¼ˆå¾ˆæ…¢ï¼ï¼‰
raw_frame = load_video(
    video_path=video_path,
    sampling='emotion_peak',
    mer_factory_output=mer_factory_output  # éœ€è¦MER-Factoryè·¯å¾„
)
```

---

### 3. **`use_au_clip_realtime: True`**

**ä½œç”¨**ï¼šAUæ¨¡æ€ä½¿ç”¨å®æ—¶CLIPç¼–ç 

**å·¥ä½œæµç¨‹**ï¼š
```python
# 1. ä»MER-Factory JSONè¯»å–AU summary_description
json_path = f'{mer_factory_output}/{sample_name}/{sample_name}_au_analysis.json'
data = json.load(open(json_path))
summary_description = data['fine_grained_descriptions']['summary_description']

# 2. å®æ—¶CLIP textç¼–ç 
clip_model, clip_preprocess = load_clip_model()
text_features = clip_model.encode_text(summary_description)  # [1, 512]
```

**ä¸ºä»€ä¹ˆå®æ—¶å¤„ç†**ï¼š
- âœ… CLIP textç¼–ç å¾ˆå¿«ï¼ˆ2-3msï¼‰
- âœ… èŠ‚çœå­˜å‚¨ï¼ˆAUç‰¹å¾æ¯ä¸ªæ•°æ®é›†~2GBï¼‰
- âœ… çµæ´»æ€§é«˜ï¼ˆå¯ä»¥éšæ—¶æ›´æ¢CLIPæ¨¡å‹ï¼‰

---

### 4. **`skip_encoders: False`**

**âš ï¸ å…³é”®**ï¼šå¿…é¡»è®¾ç½®ä¸º`False`

**åŸå› **ï¼š
```python
if skip_encoders:
    self.visual_encoder = None    # âŒ è·³è¿‡CLIPåŠ è½½
    self.acoustic_encoder = None  # âŒ è·³è¿‡HuBERTåŠ è½½
else:
    self.visual_encoder = CLIP()    # âœ… åŠ è½½CLIPï¼ˆFaceå®æ—¶ç¼–ç éœ€è¦ï¼‰
    self.acoustic_encoder = HuBERT() # âœ… åŠ è½½HuBERTï¼ˆAudioå®æ—¶ç¼–ç éœ€è¦ï¼‰
```

**æˆ‘ä»¬çš„éœ€æ±‚**ï¼š
- Frame: é¢„æå–ç‰¹å¾ â†’ ä¸éœ€è¦ç¼–ç å™¨
- Face: å®æ—¶å¤„ç† â†’ **éœ€è¦CLIPç¼–ç å™¨**
- Audio: å®æ—¶å¤„ç† â†’ **éœ€è¦HuBERTç¼–ç å™¨**
- AU: å®æ—¶å¤„ç† â†’ **éœ€è¦CLIP textç¼–ç å™¨**

**å› æ­¤å¿…é¡»åŠ è½½ç¼–ç å™¨ï¼**

---

## ğŸ“‚ ç›®å½•ç»“æ„

### **é¢„æå–ç‰¹å¾ç›®å½•**

åªå­˜å‚¨Frameçš„emotion_peakç‰¹å¾ï¼š

```
preextracted_features/
â”œâ”€â”€ mer2023/
â”‚   â””â”€â”€ frame_CLIP_VIT_LARGE_emotion_peak_8frms/  # â† åªæœ‰Frame
â”‚       â”œâ”€â”€ sample_00000001.npy  # [8, 768]
â”‚       â”œâ”€â”€ sample_00000002.npy
â”‚       â””â”€â”€ ...
â”œâ”€â”€ mer2024/
â”‚   â””â”€â”€ frame_CLIP_VIT_LARGE_emotion_peak_8frms/
â”œâ”€â”€ cmumosei/
â””â”€â”€ ... (å…¶ä»–7ä¸ªæ•°æ®é›†)
```

**æ— éœ€å­˜å‚¨Face/Audio/AUç‰¹å¾**ï¼ˆå®æ—¶å¤„ç†ï¼‰

---

### **MER-Factoryè¾“å‡ºç›®å½•**

AUæ¨¡æ€éœ€è¦è®¿é—®ï¼š

```
/home/project/MER-Factory/output/
â”œâ”€â”€ mer2023/
â”‚   â”œâ”€â”€ sample_00000001/
â”‚   â”‚   â””â”€â”€ sample_00000001_au_analysis.json  # â† AUå®æ—¶è¯»å–
â”‚   â”œâ”€â”€ sample_00000002/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ mer2024/
â””â”€â”€ ...
```

**JSONå†…å®¹**ï¼š
```json
{
  "fine_grained_descriptions": {
    "summary_description": "The person shows happiness with a slight smile..."
  }
}
```

---

## ğŸš€ è¿è¡Œæ¨ç†

### **æ­¥éª¤1ï¼šç¡®ä¿Frameç‰¹å¾å·²é¢„æå–**

```bash
cd /home/project/AffectGPT/AffectGPT/MER-UniBench
bash run_extract_emotion_peak_batch.sh
```

**éªŒè¯**ï¼š
```bash
ls ../preextracted_features/mer2023/frame_CLIP_VIT_LARGE_emotion_peak_8frms/*.npy | wc -l
# åº”è¯¥æ˜¾ç¤º411ï¼ˆMER2023æ ·æœ¬æ•°ï¼‰
```

---

### **æ­¥éª¤2ï¼šè¿è¡Œæ¨ç†**

```bash
cd /home/project/AffectGPT/AffectGPT

python inference_hybird.py \
    --cfg-path eval_configs/eval_mer2023_frame_preextract.yaml \
    --dataset mer2023 \
    --ckpt checkpoints/affectgpt_checkpoint.pth
```

**é¢„æœŸè¾“å‡º**ï¼š
```
====== Inference Frame Sampling Config ======
Frame frames: 8, Frame sampling: emotion_peak
Face frames: 8, Face sampling: uniform

ğŸ¯ Loading Frame features from: ./preextracted_features/mer2023/frame_CLIP_VIT_LARGE_emotion_peak_8frms/
âœ… Frame features loaded (preextracted)
â³ Face: real-time processing (load_face)
â³ Audio: real-time processing (load_audio)
â³ AU: real-time CLIP encoding (from MER-Factory JSON)

Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 411/411 [00:45<00:00, 9.12it/s]  # â† å¿«é€Ÿï¼
```

---

## â±ï¸ æ€§èƒ½å¯¹æ¯”

### **MER2023 (411æ ·æœ¬)**

| é…ç½® | Frame | Face | Audio | AU | æ€»è€—æ—¶ | é€Ÿåº¦ |
|------|-------|------|-------|----|----|------|
| **å®Œå…¨å®æ—¶(uniform)** | å®æ—¶uniform | å®æ—¶ | å®æ—¶ | å®æ—¶ | ~30ç§’ | 13.7 it/s |
| **å®Œå…¨å®æ—¶(emotion_peak)** | å®æ—¶emotion_peak | å®æ—¶ | å®æ—¶ | å®æ—¶ | **~4åˆ†é’Ÿ** | 1.7 it/s |
| **Frameé¢„æå–(æœ¬æ–¹æ¡ˆ)** | é¢„æå–emotion_peak | å®æ—¶ | å®æ—¶ | å®æ—¶ | **~45ç§’** | **9.1 it/s** |
| **å®Œå…¨é¢„æå–** | é¢„æå– | é¢„æå– | é¢„æå– | é¢„æå– | ~10ç§’ | 41 it/s |

**æœ¬æ–¹æ¡ˆä¼˜åŠ¿**ï¼š
- âœ… æ¯”å®æ—¶emotion_peakå¿« **5.3å€**
- âœ… æ¥è¿‘uniformæ€§èƒ½ï¼ˆ45ç§’ vs 30ç§’ï¼‰
- âœ… ä½†ä½¿ç”¨æ›´ç²¾ç¡®çš„emotion_peaké‡‡æ ·
- âœ… å­˜å‚¨ä»…éœ€500MBï¼ˆvs å®Œå…¨é¢„æå–çš„10GBï¼‰

---

## ğŸ” è°ƒè¯•æŠ€å·§

### **éªŒè¯ç‰¹å¾åŠ è½½**

åœ¨`base_dataset.py`ä¸­æ·»åŠ è°ƒè¯•è¾“å‡ºï¼š

```python
# Step1: read Frame
if 'frame' in self.needed_data:
    frame_feat_path = os.path.join(preextracted_root, frame_feat_dir, f'{sample_name}.npy')
    if os.path.exists(frame_feat_path):
        print(f"âœ… Frame: loading preextracted from {frame_feat_path}")
        frame = torch.from_numpy(np.load(frame_feat_path)).float()
    else:
        print(f"â³ Frame: real-time processing (emotion_peak)")
        raw_frame = load_video(...)
```

---

### **æ£€æŸ¥ç¼–ç å™¨çŠ¶æ€**

```python
print(f"Visual encoder: {self.visual_encoder}")
print(f"Acoustic encoder: {self.acoustic_encoder}")

# åº”è¯¥è¾“å‡º:
# Visual encoder: <CLIP_VIT_LARGE object>  # â† ä¸æ˜¯None
# Acoustic encoder: <HUBERT_LARGE object>  # â† ä¸æ˜¯None
```

å¦‚æœæ˜¯`None`ï¼Œè¯´æ˜`skip_encoders=True`ï¼Œéœ€è¦æ”¹ä¸º`False`ã€‚

---

## ğŸ“ å…¶ä»–æ•°æ®é›†é…ç½®

åªéœ€å¤åˆ¶é…ç½®å¹¶ä¿®æ”¹æ•°æ®é›†ç›¸å…³è·¯å¾„ï¼š

### **MER2024**
```yaml
datasets:
  mer2024:
    video_root: '/home/project/Dataset/Emotion/MER2025/dataset/mer2024-dataset-process/video'
    audio_root: '/home/project/Dataset/Emotion/MER2025/dataset/mer2024-dataset-process/audio'
    face_root: '/home/project/Dataset/Emotion/MER2025/dataset/mer2024-dataset-process/openface_face'
    ann_paths: ['/home/project/Dataset/Emotion/MER2025/dataset/mer2024-dataset-process/label-6way.npz']
    
    frame_sampling: 'emotion_peak'
    use_preextracted_features: True
    preextracted_root: './preextracted_features/mer2024'  # â† æ”¹è¿™é‡Œ
```

### **CMU-MOSEI**
```yaml
datasets:
  cmumosei:
    video_root: '/home/project/Dataset/Emotion/CMU-MOSEI/Raw/video'
    audio_root: '/home/project/Dataset/Emotion/CMU-MOSEI/Raw/audio'
    ann_paths: ['/home/project/Dataset/Emotion/CMU-MOSEI/CMU-MOSEI/mer_label_6.json']
    
    frame_sampling: 'emotion_peak'
    use_preextracted_features: True
    preextracted_root: './preextracted_features/cmumosei'  # â† æ”¹è¿™é‡Œ
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: æ¨ç†æ—¶è¿˜æ˜¯å¾ˆæ…¢ï¼Ÿ

**æ£€æŸ¥æ¸…å•**ï¼š
1. âœ… Frameç‰¹å¾å·²é¢„æå–ï¼Ÿ
   ```bash
   ls preextracted_features/mer2023/frame_CLIP_VIT_LARGE_emotion_peak_8frms/*.npy | wc -l
   ```

2. âœ… é…ç½®æ­£ç¡®ï¼Ÿ
   ```yaml
   use_preextracted_features: True
   frame_sampling: 'emotion_peak'
   preextracted_root: './preextracted_features/mer2023'
   ```

3. âœ… è·¯å¾„æ­£ç¡®ï¼Ÿ
   - å¦‚æœåœ¨`/home/project/AffectGPT/AffectGPT`è¿è¡Œï¼Œè·¯å¾„åº”è¯¥æ˜¯`./preextracted_features/mer2023`
   - å¦‚æœåœ¨å…¶ä»–ç›®å½•ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„

---

### Q2: æç¤ºæ‰¾ä¸åˆ°è§†è§‰ç¼–ç å™¨ï¼Ÿ

**é”™è¯¯**ï¼š
```
RuntimeError: Visual encoder is None but trying to use real-time mode
```

**åŸå› **ï¼š`skip_encoders: True`

**è§£å†³**ï¼š
```yaml
model:
  skip_encoders: False  # â† æ”¹ä¸ºFalse
```

---

### Q3: AUæ¨¡æ€æŠ¥é”™ï¼Ÿ

**é”™è¯¯**ï¼š
```
FileNotFoundError: No such file: /home/project/MER-Factory/output/mer2023/sample_xxx/sample_xxx_au_analysis.json
```

**åŸå› **ï¼šMER-Factoryæœªå¤„ç†è¯¥æ ·æœ¬

**è§£å†³**ï¼š
```bash
cd /home/project/MER-Factory
python main.py --dataset mer2023 --modality video
```

---

### Q4: èƒ½å¦è·³è¿‡AUæ¨¡æ€ï¼Ÿ

**å¯ä»¥**ï¼ä¿®æ”¹é…ç½®ï¼š
```yaml
face_or_frame: 'face_frame_audio'  # ç§»é™¤au
# æˆ–
face_or_frame: 'frame'  # åªç”¨Frame
```

---

## ğŸ“§ æ€»ç»“

### âœ… æœ¬æ–¹æ¡ˆç‰¹ç‚¹

1. **é«˜æ€§èƒ½**ï¼šFrameåŠ é€Ÿ600-1200å€
2. **ä½å­˜å‚¨**ï¼šæ¯ä¸ªæ•°æ®é›†ä»…éœ€500MB-5GB
3. **çµæ´»æ€§**ï¼šFace/Audio/AUä¿æŒå®æ—¶å¤„ç†
4. **æ˜“ç»´æŠ¤**ï¼šåªéœ€é¢„æå–ä¸€æ¬¡Frameç‰¹å¾

### ğŸ¯ é€‚ç”¨åœºæ™¯

- âœ… éœ€è¦å¿«é€Ÿæ¨ç†
- âœ… å­˜å‚¨ç©ºé—´æœ‰é™
- âœ… éœ€è¦çµæ´»è°ƒæ•´Face/Audio/AUå¤„ç†æ–¹å¼
- âœ… ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### ğŸ“ ä¸é€‚ç”¨åœºæ™¯

- âŒ è¿½æ±‚æè‡´é€Ÿåº¦ï¼ˆåº”è¯¥å®Œå…¨é¢„æå–æ‰€æœ‰æ¨¡æ€ï¼‰
- âŒ ä¸å…³å¿ƒemotion_peaké‡‡æ ·ï¼ˆåº”è¯¥ç”¨uniformå®æ—¶å¤„ç†ï¼‰
