# Frameé¢„æå–åŠŸèƒ½ä¿®å¤å®ŒæˆæŠ¥å‘Š

## ğŸ¯ é—®é¢˜æ ¹æº

åœ¨`inference_hybird.py`ç¬¬225è¡Œï¼Œä»£ç **ç¡¬ç¼–ç **å¼ºåˆ¶è®¾ç½®ï¼š

```python
dataset_cls.use_preextracted_features = False  # æ¨ç†é»˜è®¤ä½¿ç”¨å®æ—¶å¤„ç†
```

è¿™å¯¼è‡´**æ— è®ºYAMLé…ç½®æ–‡ä»¶å¦‚ä½•è®¾ç½®ï¼Œæ¨ç†æ—¶éƒ½ä¼šå¼ºåˆ¶ä½¿ç”¨å®æ—¶è§†é¢‘å¤„ç†**ï¼Œå®Œå…¨å¿½ç•¥äº†é¢„æå–ç‰¹å¾ï¼

---

## âœ… å®Œæ•´ä¿®å¤æ¸…å•

### 1. **ä¿®å¤`inference_hybird.py`** âœ…

**æ–‡ä»¶**: `/home/project/AffectGPT/AffectGPT/inference_hybird.py`

**ä¿®æ”¹å‰** (ç¬¬225è¡Œï¼ŒâŒ é”™è¯¯):
```python
dataset_cls.use_preextracted_features = False  # æ¨ç†é»˜è®¤ä½¿ç”¨å®æ—¶å¤„ç†
```

**ä¿®æ”¹å** (âœ… æ­£ç¡®):
```python
# ğŸ¯ ä»é…ç½®æ–‡ä»¶è¯»å–é¢„æå–ç‰¹å¾é…ç½®ï¼ˆè€Œä¸æ˜¯ç¡¬ç¼–ç ä¸ºFalseï¼‰
dataset_cls.use_preextracted_features = getattr(inference_cfg, 'use_preextracted_features', False)
dataset_cls.preextracted_root = getattr(inference_cfg, 'preextracted_root', './preextracted_features')
dataset_cls.visual_encoder = getattr(inference_cfg, 'visual_encoder', 'CLIP_VIT_LARGE')
dataset_cls.acoustic_encoder = getattr(inference_cfg, 'acoustic_encoder', 'HUBERT_LARGE')
```

**æ·»åŠ æ—¥å¿—è¾“å‡º**:
```python
# æ˜¾ç¤ºFrameé¢„æå–é…ç½®çŠ¶æ€
if dataset_cls.use_preextracted_features:
    print(f'âœ… [Frameé¢„æå–] å·²å¯ç”¨é¢„æå–ç‰¹å¾åŠ è½½')
    print(f'   ç‰¹å¾è·¯å¾„: {dataset_cls.preextracted_root}/<dataset>/frame_{dataset_cls.visual_encoder}_{dataset_cls.frame_sampling}_{dataset_cls.frame_n_frms}frms/')
else:
    print(f'âš ï¸  [Frameå®æ—¶] ä½¿ç”¨å®æ—¶è§†é¢‘å¤„ç†ï¼ˆæœªå¯ç”¨é¢„æå–ï¼‰')
```

---

### 2. **ä¿®å¤`base_dataset.py`è·¯å¾„æ„å»º** âœ…

**æ–‡ä»¶**: `/home/project/AffectGPT/AffectGPT/my_affectgpt/datasets/datasets/base_dataset.py`

**é—®é¢˜**: è·¯å¾„ç¼ºå°‘æ•°æ®é›†åç§°å±‚çº§

**ä¿®æ”¹å‰** (âŒ é”™è¯¯):
```python
frame_feat_path = os.path.join(preextracted_root, frame_feat_dir, f'{sample_name}.npy')
# è·¯å¾„: ./preextracted_features/frame_CLIP_VIT_LARGE_emotion_peak_8frms/sample_xxx.npy âŒ
```

**ä¿®æ”¹å** (âœ… æ­£ç¡®):
```python
# ğŸ¯ æ„å»ºç‰¹å¾è·¯å¾„ï¼špreextracted_root/dataset_name/frame_xxx/*.npy
dataset_name = getattr(self, 'dataset', 'unknown')

# æ•°æ®é›†åç§°æ˜ å°„ï¼ˆå¤„ç†ç‰¹æ®Šæƒ…å†µï¼‰
dataset_name_mapping = {
    'IEMOCAPFour': 'iemocap',  # IEMOCAPFour -> iemocapï¼ˆä¸æå–è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
}
dataset_name_lower = dataset_name_mapping.get(dataset_name, dataset_name.lower())

frame_feat_dir = f'frame_{visual_encoder}_{frame_sampling}_{frame_n_frms}frms'
frame_feat_path = os.path.join(preextracted_root, dataset_name_lower, frame_feat_dir, f'{sample_name}.npy')
# è·¯å¾„: ./preextracted_features/mer2023/frame_CLIP_VIT_LARGE_emotion_peak_8frms/sample_xxx.npy âœ…
```

**æ·»åŠ æˆåŠŸåŠ è½½æ—¥å¿—**:
```python
if os.path.exists(frame_feat_path):
    frame_features = np.load(frame_feat_path)
    frame = torch.from_numpy(frame_features).float()
    raw_frame = frame
    sample_data['frame_preextracted'] = True
    
    # é¦–æ¬¡åŠ è½½æ—¶è¾“å‡ºæç¤º
    if not hasattr(BaseDataset, '_logged_preextract_success'):
        print(f"âœ… [Frameé¢„æå–] æˆåŠŸåŠ è½½é¢„æå–ç‰¹å¾: {dataset_name_lower}/frame_{visual_encoder}_{frame_sampling}_{frame_n_frms}frms/")
        BaseDataset._logged_preextract_success = True
```

**æ·»åŠ å›é€€æœºåˆ¶**:
```python
else:
    # é¢„æå–ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå›é€€åˆ°å®æ—¶å¤„ç†æ¨¡å¼
    if not hasattr(BaseDataset, '_warned_missing_preextract'):
        print(f"âš ï¸ Frameé¢„æå–ç‰¹å¾ä¸å­˜åœ¨: {frame_feat_path}")
        print(f"   å°†å›é€€åˆ°å®æ—¶å¤„ç†æ¨¡å¼")
        BaseDataset._warned_missing_preextract = True
    
    # å›é€€ï¼šå®æ—¶åŠ è½½è§†é¢‘
    if video_path is not None:
        raw_frame, msg = load_video(...)
        frame = self.vis_processor.transform(raw_frame)
```

---

### 3. **é…ç½®æ–‡ä»¶å·²ä¿®æ”¹** âœ…

**æ–‡ä»¶**: `/home/project/AffectGPT/AffectGPT/train_configs/emercoarse_highlevelfilter4_outputhybird_bestsetup_bestfusion_lz_face_frame_au.yaml`

```yaml
inference:
  # Frameé…ç½®
  frame_n_frms: 8
  frame_sampling: 'emotion_peak'
  
  # ğŸ¯ Frameé¢„æå–é…ç½®
  use_preextracted_features: True  âœ…
  preextracted_root: './preextracted_features'  âœ…
  visual_encoder: 'CLIP_VIT_LARGE'  âœ…
  
  # ğŸ¯ AUå®æ—¶CLIPç¼–ç é…ç½®
  mer_factory_output: '/home/project/MER-Factory/output'  âœ…
  use_au_clip_realtime: True  âœ…
```

---

## ğŸ“‚ é¢„æå–ç‰¹å¾æ–‡ä»¶çŠ¶æ€

æ‰€æœ‰9ä¸ªMER-UniBenchæ•°æ®é›†çš„ç‰¹å¾æ–‡ä»¶å·²ç”Ÿæˆå®Œæ¯•ï¼š

```
/home/project/AffectGPT/AffectGPT/preextracted_features/
â”œâ”€â”€ mer2023/frame_CLIP_VIT_LARGE_emotion_peak_8frms/     (411 files) âœ…
â”œâ”€â”€ mer2024/frame_CLIP_VIT_LARGE_emotion_peak_8frms/     (1169 files) âœ…
â”œâ”€â”€ meld/frame_CLIP_VIT_LARGE_emotion_peak_8frms/        (2610 files) âœ…
â”œâ”€â”€ iemocap/frame_CLIP_VIT_LARGE_emotion_peak_8frms/     (1241 files) âœ…
â”œâ”€â”€ cmumosi/frame_CLIP_VIT_LARGE_emotion_peak_8frms/     (686 files) âœ…
â”œâ”€â”€ cmumosei/frame_CLIP_VIT_LARGE_emotion_peak_8frms/    (4659 files) âœ…
â”œâ”€â”€ sims/frame_CLIP_VIT_LARGE_emotion_peak_8frms/        (457 files) âœ…
â”œâ”€â”€ simsv2/frame_CLIP_VIT_LARGE_emotion_peak_8frms/      (1034 files) âœ…
â””â”€â”€ ovmerdplus/frame_CLIP_VIT_LARGE_emotion_peak_8frms/  (532 files) âœ…

æ€»è®¡: 11,799 ä¸ªé¢„æå–ç‰¹å¾æ–‡ä»¶
```

---

## ğŸš€ ç°åœ¨è¿è¡Œæ¨ç†

æ‰€æœ‰ä¿®å¤å·²å®Œæˆï¼ç°åœ¨é‡æ–°è¿è¡Œæ¨ç†å°†ä¼šçœ‹åˆ°ï¼š

```bash
cd /home/project/AffectGPT/AffectGPT

setsid bash -c "CUDA_VISIBLE_DEVICES=3 python -u inference_hybird.py \
    --zeroshot \
    --dataset='inferenceData' \
    --cfg-path=train_configs/emercoarse_highlevelfilter4_outputhybird_bestsetup_bestfusion_lz_face_frame_au.yaml \
    --options 'inference.test_epochs=30-60' 'inference.skip_epoch=5' \
    " > output/log_information/.../result/reason_ov.log 2>&1
```

---

## ğŸ“Š é¢„æœŸæ—¥å¿—è¾“å‡º

### **æ¨ç†å¼€å§‹æ—¶**:
```
======== Step3: Inferece ========
process datasets:  ['MER2023', 'MER2024', ...]
current dataset: MER2023
[INFERENCE] AUæ¨¡å¼: CLIPå®æ—¶ç¼–ç æ¨¡å¼ï¼ˆä»MER-Factory JSONåŠ è½½summary_descriptionï¼‰
====== Inference Frame Sampling Config ======
Frame frames: 8, Frame sampling: emotion_peak
Face frames: 8, Face sampling: uniform
âœ… [Frameé¢„æå–] å·²å¯ç”¨é¢„æå–ç‰¹å¾åŠ è½½                    â† ğŸ†• æ–°å¢
   ç‰¹å¾è·¯å¾„: ./preextracted_features/<dataset>/frame_CLIP_VIT_LARGE_emotion_peak_8frms/  â† ğŸ†• æ–°å¢
```

### **é¦–æ¬¡åŠ è½½æ ·æœ¬æ—¶**:
```
process on 0|411: sample_00001998 | ...
âœ… [Frameé¢„æå–] æˆåŠŸåŠ è½½é¢„æå–ç‰¹å¾: mer2023/frame_CLIP_VIT_LARGE_emotion_peak_8frms/  â† ğŸ†• æ–°å¢
ğŸ“¥ [AU CLIP] åŠ è½½CLIPæ¨¡å‹ (ViT-B/32) åˆ° cuda...
âœ… [AU CLIP] CLIPæ¨¡å‹åŠ è½½å®Œæˆ
```

---

## ğŸ¯ æ€§èƒ½æå‡é¢„æœŸ

### FrameåŠ è½½é€Ÿåº¦å¯¹æ¯”

| æ¨¡å¼ | åŠ è½½æ–¹å¼ | æ—¶é—´/æ ·æœ¬ | åŠ é€Ÿæ¯” |
|------|---------|---------|-------|
| **ä¹‹å‰ï¼ˆå®æ—¶emotion_peakï¼‰** | è¯»å–è§†é¢‘ + MER-Factory JSON + è®¡ç®—ç´¢å¼• + CLIPç¼–ç  | ~5-10ms | 1x |
| **ç°åœ¨ï¼ˆé¢„æå–emotion_peakï¼‰** | `np.load()` .npyæ–‡ä»¶ | ~0.5ms | **10-20x** âš¡ |

### æ€»ä½“æ¨ç†é€Ÿåº¦æå‡

å‡è®¾å•æ ·æœ¬æ¨ç†æ—¶é—´åˆ†å¸ƒï¼š

**ä¹‹å‰**ï¼š
- FrameåŠ è½½: 8ms
- FaceåŠ è½½: 0.01ms
- AudioåŠ è½½: 15ms
- AUå¤„ç†: 2ms
- æ¨¡å‹æ¨ç†: 50ms
- **æ€»è®¡**: ~75ms

**ç°åœ¨**ï¼š
- FrameåŠ è½½: **0.5ms** âš¡
- FaceåŠ è½½: 0.01ms
- AudioåŠ è½½: 15ms
- AUå¤„ç†: 2ms
- æ¨¡å‹æ¨ç†: 50ms
- **æ€»è®¡**: ~67.5ms

**åŠ é€Ÿæ•ˆæœ**: 
- Frameæ¨¡å—åŠ é€Ÿ **16å€**
- æ€»ä½“æ¨ç†åŠ é€Ÿçº¦ **10%**

å¯¹äº411ä¸ªæ ·æœ¬ï¼ˆMER2023æµ‹è¯•é›†ï¼‰ï¼š
- ä¹‹å‰æ€»æ—¶é—´: 411 Ã— 75ms = **30.8ç§’**
- ç°åœ¨æ€»æ—¶é—´: 411 Ã— 67.5ms = **27.7ç§’**
- **èŠ‚çœæ—¶é—´**: 3.1ç§’

---

## âœ… éªŒè¯æ¸…å•

è¯·ç¡®è®¤ä»¥ä¸‹å†…å®¹ï¼š

- [x] `inference_hybird.py`å·²ä¿®æ”¹ï¼ˆä»é…ç½®è¯»å–é¢„æå–è®¾ç½®ï¼‰
- [x] `base_dataset.py`å·²ä¿®æ”¹ï¼ˆè·¯å¾„æ„å»º+IEMOCAPFouræ˜ å°„+æ—¥å¿—ï¼‰
- [x] é…ç½®æ–‡ä»¶å·²è®¾ç½®`use_preextracted_features: True`
- [x] æ‰€æœ‰9ä¸ªæ•°æ®é›†çš„`.npy`æ–‡ä»¶å·²ç”Ÿæˆ
- [ ] **é‡æ–°è¿è¡Œæ¨ç†ï¼Œè§‚å¯Ÿæ–°çš„æ—¥å¿—è¾“å‡º**
- [ ] ç¡®è®¤çœ‹åˆ°"âœ… [Frameé¢„æå–] å·²å¯ç”¨é¢„æå–ç‰¹å¾åŠ è½½"
- [ ] ç¡®è®¤çœ‹åˆ°"âœ… [Frameé¢„æå–] æˆåŠŸåŠ è½½é¢„æå–ç‰¹å¾"
- [ ] ç¡®è®¤æ²¡æœ‰"âš ï¸ Frameé¢„æå–ç‰¹å¾ä¸å­˜åœ¨"è­¦å‘Š
- [ ] éªŒè¯æ¨ç†é€Ÿåº¦æå‡

---

## ğŸ› æ•…éšœæ’æŸ¥

### å¦‚æœä»ç„¶çœ‹ä¸åˆ°é¢„æå–æ—¥å¿—ï¼š

1. **æ£€æŸ¥Pythonè¿›ç¨‹æ˜¯å¦ä½¿ç”¨æ–°ä»£ç **
   ```bash
   # ç¡®ä¿ä¹‹å‰çš„æ¨ç†è¿›ç¨‹å·²å®Œå…¨é€€å‡º
   ps aux | grep inference_hybird.py
   # å¦‚æœæœ‰æ®‹ç•™è¿›ç¨‹ï¼Œkillå®ƒä»¬
   ```

2. **æ‰‹åŠ¨éªŒè¯é…ç½®è¯»å–**
   ```bash
   python3 -c "
   import yaml
   cfg = yaml.safe_load(open('train_configs/emercoarse_highlevelfilter4_outputhybird_bestsetup_bestfusion_lz_face_frame_au.yaml'))
   print('use_preextracted_features:', cfg['inference']['use_preextracted_features'])
   print('preextracted_root:', cfg['inference']['preextracted_root'])
   "
   ```

3. **æ£€æŸ¥æ–‡ä»¶è·¯å¾„**
   ```bash
   # ä»æ¨ç†è„šæœ¬è¿è¡Œç›®å½•æ£€æŸ¥ç›¸å¯¹è·¯å¾„
   cd /home/project/AffectGPT/AffectGPT
   ls -la ./preextracted_features/mer2023/frame_CLIP_VIT_LARGE_emotion_peak_8frms/ | head -10
   ```

4. **æŸ¥çœ‹å®Œæ•´é”™è¯¯æ—¥å¿—**
   ```bash
   tail -100 output/log_information/.../result/reason_ov.log | grep -E "(Frame|é¢„æå–|preextract|âš ï¸|âŒ)"
   ```

---

## ğŸ“ æ€»ç»“

**æ‰€æœ‰å¿…è¦çš„ä»£ç ä¿®å¤å·²å®Œæˆï¼**

1. âœ… `inference_hybird.py` - ä¿®å¤ç¡¬ç¼–ç é—®é¢˜ï¼Œä»é…ç½®è¯»å–
2. âœ… `base_dataset.py` - ä¿®å¤è·¯å¾„æ„å»ºï¼Œæ·»åŠ IEMOCAPFouræ˜ å°„
3. âœ… é…ç½®æ–‡ä»¶ - å¯ç”¨Frameé¢„æå–
4. âœ… ç‰¹å¾æ–‡ä»¶ - æ‰€æœ‰æ•°æ®é›†å·²æå–å®Œæˆ
5. âœ… æ—¥å¿—è¾“å‡º - æ·»åŠ æ˜ç¡®çš„çŠ¶æ€æç¤º

**ä¸‹ä¸€æ­¥**ï¼šé‡æ–°è¿è¡Œæ¨ç†ï¼Œè§‚å¯Ÿæ—¥å¿—ç¡®è®¤Frameé¢„æå–åŠŸèƒ½ç”Ÿæ•ˆï¼

---

## ğŸ‰ é¢„æœŸç»“æœ

è¿è¡Œæ¨ç†åï¼Œä½ åº”è¯¥çœ‹åˆ°ï¼š

1. âœ… æ—¥å¿—å¼€å¤´æ˜¾ç¤º"âœ… [Frameé¢„æå–] å·²å¯ç”¨é¢„æå–ç‰¹å¾åŠ è½½"
2. âœ… é¦–æ¬¡åŠ è½½æ ·æœ¬æ—¶æ˜¾ç¤º"âœ… [Frameé¢„æå–] æˆåŠŸåŠ è½½é¢„æå–ç‰¹å¾"
3. âœ… æ²¡æœ‰ä»»ä½•"âš ï¸ Frameé¢„æå–ç‰¹å¾ä¸å­˜åœ¨"è­¦å‘Š
4. âœ… æ¨ç†é€Ÿåº¦æ˜æ˜¾æå‡ï¼ˆFrameåŠ è½½ä»8msé™è‡³0.5msï¼‰
5. âœ… Face/Audio/AUä»ç„¶å®æ—¶å¤„ç†ï¼Œä¿æŒçµæ´»æ€§

**Frameé¢„æå–ä¼˜åŒ–å®Œæˆï¼** ğŸš€
