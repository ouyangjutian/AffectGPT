# ç‰¹å¾æå–å‡½æ•°æ¶æ„è¯´æ˜

## ğŸ“Š å‡½æ•°è°ƒç”¨å…³ç³»å›¾

```
é¢„æå–æ¨¡å¼ä¸»å‡½æ•° (extract_dataset_features)
    â†“
    è°ƒç”¨: extract_frame_features(video_path, n_frms, sampling, video_name)
    â†“
    â”œâ”€ sampling='uniform'  â†’ æ ‡å‡†é‡‡æ ·ï¼ˆå‡åŒ€ï¼‰
    â”œâ”€ sampling='headtail' â†’ æ ‡å‡†é‡‡æ ·ï¼ˆå¤´å°¾ï¼‰
    â””â”€ sampling='emotion_peak' + video_name æä¾›
        â†“
        è‡ªåŠ¨è½¬å‘åˆ°: extract_frame_features_smart(video_path, video_name, n_frms=8)
            â†“
            1. åŠ è½½ au_info
            2. è®¡ç®—æ™ºèƒ½å¸§ç´¢å¼•ï¼ˆ4ç§ç­–ç•¥ï¼‰
            3. æ‰‹åŠ¨åŠ è½½æŒ‡å®šå¸§
            4. ç‰¹å¾æå–
```

## ğŸ¯ å‡½æ•°èŒè´£

### 1. `extract_frame_features()` - ç»Ÿä¸€å…¥å£å‡½æ•° â­

**èŒè´£**ï¼šæ‰€æœ‰å¸§ç‰¹å¾æå–çš„ç»Ÿä¸€å…¥å£

**æ”¯æŒçš„é‡‡æ ·ç­–ç•¥**ï¼š
- âœ… `uniform` - å‡åŒ€é‡‡æ ·
- âœ… `headtail` - å¤´å°¾é‡‡æ ·  
- âœ… `emotion_peak` - æ™ºèƒ½é‡‡æ ·ï¼ˆè‡ªåŠ¨è½¬å‘ï¼‰

**å‡½æ•°ç­¾å**ï¼š
```python
def extract_frame_features(
    self, 
    video_path: str,
    n_frms: int = 8,
    sampling: str = 'uniform',
    video_name: Optional[str] = None  # emotion_peakæ¨¡å¼éœ€è¦
) -> np.ndarray  # [T, D]
```

**å†…éƒ¨é€»è¾‘**ï¼š
```python
if sampling == 'emotion_peak' and video_name:
    # è½¬å‘åˆ°æ™ºèƒ½é‡‡æ ·å‡½æ•°
    return self.extract_frame_features_smart(video_path, video_name, n_frms=8)
else:
    # æ ‡å‡†é‡‡æ ·ï¼ˆuniform/headtailï¼‰
    raw_frame = load_video(video_path, n_frms, sampling=sampling)
    features = self.encoders['visual'](frame, raw_frame)
    return features
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
# å‡åŒ€é‡‡æ ·8å¸§
features = extractor.extract_frame_features(
    video_path='video.mp4',
    n_frms=8,
    sampling='uniform'
)

# æ™ºèƒ½é‡‡æ ·8å¸§ï¼ˆè‡ªåŠ¨è½¬å‘ï¼‰
features = extractor.extract_frame_features(
    video_path='video.mp4',
    n_frms=8,
    sampling='emotion_peak',
    video_name='samplenew3_00000070'  # å¿…éœ€
)
```

---

### 2. `extract_frame_features_smart()` - æ™ºèƒ½é‡‡æ ·å®ç° ğŸ§ 

**èŒè´£**ï¼šå®ç°åŸºäº au_info çš„æ™ºèƒ½8å¸§é‡‡æ ·

**å‡½æ•°ç­¾å**ï¼š
```python
def extract_frame_features_smart(
    self,
    video_path: str,
    video_name: str,  # å¿…éœ€ï¼Œç”¨äºæŸ¥æ‰¾au_info
    n_frms: int = 8   # å›ºå®šä¸º8
) -> np.ndarray  # [8, D]
```

**å†…éƒ¨æµç¨‹**ï¼š
```python
1. åŠ è½½ au_info
   au_info = self.load_au_info(video_name)
   
2. è®¡ç®—æ™ºèƒ½å¸§ç´¢å¼•
   frame_indices = self.calculate_smart_frame_indices(au_info, total_frames)
   # è¿”å›8ä¸ªå¸§ç´¢å¼•
   
3. æ‰‹åŠ¨åŠ è½½æŒ‡å®šå¸§
   raw_frame = self._load_specific_frames(video_path, frame_indices)
   
4. ç‰¹å¾æå–
   features = self.encoders['visual'](frame, raw_frame)
   return features  # [8, D]
```

**é‡‡æ ·ç­–ç•¥**ï¼ˆ4ç§ï¼‰ï¼š
1. **ç­–ç•¥1**ï¼šå‰åâ‰¥2å¸§ â†’ å³°å€¼+å‰2+å2+å‡åŒ€3
2. **ç­–ç•¥2**ï¼šä¸€è¾¹1å¸§ â†’ å³°å€¼+1å¸§+2å¸§+å‡åŒ€4
3. **ç­–ç•¥3**ï¼šå‰åå„1å¸§ â†’ å³°å€¼+å‰1+å1+å‡åŒ€5
4. **ç­–ç•¥4**ï¼šä¸€è¾¹0å¸§ â†’ å³°å€¼+2å¸§+å‡åŒ€5

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
# ç›´æ¥è°ƒç”¨ï¼ˆä¸æ¨èï¼Œåº”ä½¿ç”¨ç»Ÿä¸€å…¥å£ï¼‰
features = extractor.extract_frame_features_smart(
    video_path='video.mp4',
    video_name='samplenew3_00000070'
)
```

---

### 3. è¾…åŠ©å‡½æ•°

#### `load_au_info(video_name)`
åŠ è½½MER-Factoryçš„au_info

#### `calculate_smart_frame_indices(au_info, total_frames)`
æ ¹æ®au_infoè®¡ç®—8ä¸ªå¸§ç´¢å¼•

#### `_load_specific_frames(video_path, frame_indices)`
æ‰‹åŠ¨åŠ è½½æŒ‡å®šç´¢å¼•çš„å¸§

## âœ… ä¼˜åŒ–åçš„è°ƒç”¨æ–¹å¼

### ä¸»å‡½æ•°ä¸­ï¼ˆæ¨èï¼‰âœ¨

```python
# âœ… æ¨èï¼šç»Ÿä¸€ä½¿ç”¨ extract_frame_features
frame_features = extractor.extract_frame_features(
    video_path=video_path,
    n_frms=args.frame_n_frms,        # 8
    sampling=args.frame_sampling,     # 'emotion_peak'
    video_name=sample_name            # 'samplenew3_00000070'
)

# å®ƒä¼šè‡ªåŠ¨åˆ¤æ–­ï¼š
# - å¦‚æœæ˜¯ emotion_peak + video_nameæä¾› â†’ è°ƒç”¨æ™ºèƒ½é‡‡æ ·
# - å¦åˆ™ â†’ è°ƒç”¨æ ‡å‡†é‡‡æ ·
```

### æ—§çš„è°ƒç”¨æ–¹å¼ï¼ˆå·²ç®€åŒ–ï¼‰

```python
# âŒ æ—§æ–¹å¼ï¼ˆå·²ç§»é™¤ï¼‰ï¼šæ‰‹åŠ¨åˆ¤æ–­
if args.frame_sampling == 'emotion_peak':
    frame_features = extractor.extract_frame_features_smart(...)
else:
    frame_features = extractor.extract_frame_features(...)
```

## ğŸ“ è®¾è®¡ä¼˜åŠ¿

### ä¼˜åŠ¿1ï¼šç»Ÿä¸€æ¥å£
æ‰€æœ‰é‡‡æ ·ç­–ç•¥é€šè¿‡åŒä¸€ä¸ªå‡½æ•° `extract_frame_features` è°ƒç”¨ï¼Œç®€åŒ–ä½¿ç”¨ã€‚

### ä¼˜åŠ¿2ï¼šè‡ªåŠ¨è·¯ç”±
æ ¹æ® `sampling` å‚æ•°è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„å®ç°ï¼š
- `uniform/headtail` â†’ æ ‡å‡†å®ç°
- `emotion_peak` â†’ æ™ºèƒ½å®ç°

### ä¼˜åŠ¿3ï¼šå‘åå…¼å®¹
ç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹ï¼Œåªéœ€æ·»åŠ  `video_name` å‚æ•°å³å¯å¯ç”¨æ™ºèƒ½é‡‡æ ·ã€‚

### ä¼˜åŠ¿4ï¼šæ¸…æ™°èŒè´£
- `extract_frame_features` - å¯¹å¤–æ¥å£
- `extract_frame_features_smart` - å†…éƒ¨å®ç°

## ğŸ”„ å®Œæ•´è°ƒç”¨æµç¨‹

```
1. ç”¨æˆ·è°ƒç”¨
   extract_frame_features(
       video_path='video.mp4',
       n_frms=8,
       sampling='emotion_peak',
       video_name='samplenew3_00000070'
   )

2. å‡½æ•°å†…éƒ¨åˆ¤æ–­
   if sampling == 'emotion_peak' and video_name:
       â†“ è½¬å‘åˆ°æ™ºèƒ½é‡‡æ ·
   
3. æ™ºèƒ½é‡‡æ ·å¤„ç†
   extract_frame_features_smart()
       â†“ åŠ è½½ au_info
       â†“ è®¡ç®—å¸§ç´¢å¼• [0, 8, 9, 10, 11, 12, 18, 31]
       â†“ åŠ è½½æŒ‡å®šå¸§
       â†“ æå–ç‰¹å¾
   
4. è¿”å›ç»“æœ
   features: np.ndarray [8, 768]
```

## ğŸ“Œ æ€»ç»“

| å‡½æ•° | è§’è‰² | ä½•æ—¶ä½¿ç”¨ | æ˜¯å¦ç›´æ¥è°ƒç”¨ |
|------|------|----------|------------|
| `extract_frame_features` | **ç»Ÿä¸€å…¥å£** | æ‰€æœ‰æƒ…å†µ | âœ… æ¨è |
| `extract_frame_features_smart` | **å†…éƒ¨å®ç°** | emotion_peak | âŒ ä¸æ¨è |

**æœ€ä½³å®è·µ**ï¼š
```python
# âœ… æ€»æ˜¯ä½¿ç”¨ç»Ÿä¸€å…¥å£
features = extractor.extract_frame_features(
    video_path, n_frms, sampling, video_name
)

# âŒ é¿å…ç›´æ¥è°ƒç”¨å†…éƒ¨å®ç°
features = extractor.extract_frame_features_smart(...)  # ä¸æ¨è
```

---

**ä½œè€…**: AffectGPT Team  
**æ—¥æœŸ**: 2025-11-11  
**ç‰ˆæœ¬**: 3.0 (ç»Ÿä¸€æ¥å£ä¼˜åŒ–)
