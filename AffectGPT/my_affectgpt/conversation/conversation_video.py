import re
import copy
import dataclasses
from enum import auto, Enum
from typing import List, Tuple, Any
from PIL import Image
import numpy as np

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer
from transformers import StoppingCriteria, StoppingCriteriaList
from my_affectgpt.common.registry import registry
from my_affectgpt.processors import Blip2ImageEvalProcessor
from my_affectgpt.processors.video_processor import ToTHWC, ToUint8, load_video, load_face
from my_affectgpt.models.ImageBind.data import load_audio, transform_audio
from my_affectgpt.datasets.builders.image_text_pair_builder import *
from my_affectgpt.models.au_agent import AUAgent
import config

class SeparatorStyle(Enum):
    """Different separator style."""
    SINGLE = auto()
    TWO = auto()

@dataclasses.dataclass
class Conversation:
    """A class that keeps all conversation history."""
    system: str
    roles: List[str]
    messages: List[List[str]]
    offset: int
    sep_style: SeparatorStyle = SeparatorStyle.SINGLE
    sep: str = "###"
    sep2: str = None

    skip_next: bool = False
    conv_id: Any = None

    def get_prompt(self):
        if self.sep_style == SeparatorStyle.SINGLE:
            ret = self.system + self.sep
            for role, message in self.messages:
                if message:
                    ret += role + ": " + message + self.sep
                else:
                    ret += role + ":"
            return ret
        elif self.sep_style == SeparatorStyle.TWO:
            seps = [self.sep, self.sep2]
            ret = self.system + seps[0]
            for i, (role, message) in enumerate(self.messages):
                if message:
                    ret += role + ": " + message + seps[i % 2]
                else:
                    ret += role + ":"
            return ret
        else:
            raise ValueError(f"Invalid style: {self.sep_style}")

    def append_message(self, role, message):
        self.messages.append([role, message])

    def copy(self):
        return Conversation(
            system=self.system,
            roles=self.roles,
            messages=[[x, y] for x, y in self.messages],
            offset=self.offset,
            sep_style=self.sep_style,
            sep=self.sep,
            sep2=self.sep2,
            conv_id=self.conv_id)

    def dict(self):
        return {
            "system": self.system,
            "roles": self.roles,
            "messages": self.messages,
            "offset": self.offset,
            "sep": self.sep,
            "sep2": self.sep2,
            "conv_id": self.conv_id,
        }


class StoppingCriteriaSub(StoppingCriteria):
    def __init__(self, stops=[], encounters=1):
        super().__init__()
        self.stops = stops

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):
        for stop in self.stops:
            if torch.all((stop == input_ids[0][-len(stop):])).item():
                return True
        return False


default_conversation = Conversation(
    system="",
    roles=("Human", "Assistant"),
    messages=[],
    offset=0,
    sep_style=SeparatorStyle.SINGLE,
    sep="###",
)


######################################################
# ============== (only for inference) ============== #
######################################################
class Chat:
    def __init__(self, model, model_cfg, device='cuda:0'):
        self.device = device
        self.model = model
        self.tokenizer = model.llama_tokenizer
       
        # é‡‡ç”¨æ›´åŠ é€šç”¨çš„æ ¼å¼ï¼Œå•ç‹¬çš„ â€œ#â€ å’Œåœ¨æ–‡æœ¬ä¸­çš„â€œa#â€ é‡‡ç”¨çš„ç¼–ç ç¬¦å·æ˜¯ä¸åŒçš„
        id_tre_jin = self.tokenizer('###', add_special_tokens=False)['input_ids'][0] # 835
        id_two_jin = self.tokenizer('a##', add_special_tokens=False)['input_ids'][1] # 2277
        id_one_jin = self.tokenizer('a#', add_special_tokens=False)['input_ids'][1]  # 29937
        stop_words_ids = [torch.tensor([self.tokenizer.eos_token_id]).to(self.device),
                          torch.tensor([id_tre_jin]).to(self.device),
                          torch.tensor([id_two_jin, id_one_jin]).to(self.device),
                          torch.tensor([id_one_jin, id_two_jin]).to(self.device)] # three id for "### / # ## "
        self.stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])
        # è®¾ç½®å…¶ä»–å‚æ•°
        self.num_video_query_token = model_cfg.num_video_query_token
        self.num_audio_query_token = model_cfg.num_audio_query_token
        self.num_multi_query_token = model_cfg.num_multi_query_token
        self.num_image_query_token = model_cfg.num_image_query_token
        self.num_au_query_token = getattr(model_cfg, 'num_au_query_token', 8)  # AU query tokenæ•°é‡
        
        # åˆå§‹åŒ–AU Agentï¼ˆå¦‚æœé…ç½®ä¸­å¯ç”¨ï¼‰
        self.use_au_agent = getattr(model_cfg, 'use_au_agent', False)
        if self.use_au_agent:
            au_base_model = getattr(model_cfg, 'au_agent_base_model', '/home/project/Dataset/Emotion/tools/transformer/LLM/Qwen2.5-7B-Instruct')
            au_lora_weights = getattr(model_cfg, 'au_agent_lora_weights', '/home/project/AffectGPT/AffectGPT/output/au_agent_qwen2.5_7b_lora')
            print(f"[Chat] Loading AU Agent from {au_lora_weights}")
            self.au_agent = AUAgent(
                base_model_path=au_base_model,
                lora_weights_path=au_lora_weights,
                device=device,
                use_lora=True
            )
        else:
            self.au_agent = None

    def to_token_ids(self, text, max_length):
        input_ids = self.tokenizer(text,
                                   return_tensors="pt",
                                   padding="longest",
                                   max_length=max_length,
                                   truncation=True,
                                   add_special_tokens=False).input_ids[0]
        return input_ids

    def replace_token_for_multimodal(self, prompt):
        replace_token = config.DEFAULT_FRAME_PATCH_TOKEN * self.num_video_query_token
        prompt = prompt.replace(config.DEFAULT_FRAME_PATCH_TOKEN, replace_token)
        replace_token = config.DEFAULT_FACE_PATCH_TOKEN * self.num_video_query_token
        prompt = prompt.replace(config.DEFAULT_FACE_PATCH_TOKEN, replace_token)
        replace_token = config.DEFAULT_AUDIO_PATCH_TOKEN * self.num_audio_query_token
        prompt = prompt.replace(config.DEFAULT_AUDIO_PATCH_TOKEN, replace_token)
        replace_token = config.DEFAULT_MULTI_PATCH_TOKEN * self.num_multi_query_token
        prompt = prompt.replace(config.DEFAULT_MULTI_PATCH_TOKEN, replace_token)
        replace_token = config.DEFAULT_IMAGE_PATCH_TOKEN * self.num_image_query_token
        prompt = prompt.replace(config.DEFAULT_IMAGE_PATCH_TOKEN, replace_token)
        # ğŸ¯ Lexicalä¿¡æ¯ç›´æ¥ä½œä¸ºæ–‡æœ¬åµŒå…¥ï¼Œä¸éœ€è¦tokenæ›¿æ¢
        return prompt
   
    def postprocess_audio(self, sample_data):
        if 'audio' not in sample_data or sample_data['audio'] is None:
            return None, None
        
        audio = sample_data['audio'].unsqueeze(0).to(self.device)
        raw_audio = sample_data['raw_audio'].unsqueeze(0).to(self.device)
        audio_hiddens, audio_llms = self.model.encode_audio_merge(audio, raw_audio)
        return audio_hiddens, audio_llms

    def postprocess_face(self, sample_data):
        if 'face' not in sample_data or sample_data['face'] is None:
            return None, None
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºé¢„æå–ç‰¹å¾
        is_preextracted = sample_data.get('face_preextracted', False)
        
        face = sample_data['face'].unsqueeze(0).to(self.device)
        raw_face = sample_data['raw_face'].unsqueeze(0).to(self.device)
        
        # ä¼ é€’is_preextractedæ ‡å¿—
        face_hiddens, face_llms = self.model.encode_video_merge(face, raw_face, is_preextracted=is_preextracted)
        return face_hiddens, face_llms
    
    def postprocess_frame(self, sample_data):
        if 'frame' not in sample_data or sample_data['frame'] is None:
            return None, None
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºé¢„æå–ç‰¹å¾
        is_preextracted = sample_data.get('frame_preextracted', False)
        
        video = sample_data['frame'].unsqueeze(0).to(self.device)
        raw_video = sample_data['raw_frame'].unsqueeze(0).to(self.device)
        
        # ä¼ é€’is_preextractedæ ‡å¿—
        frame_hiddens, frame_llms = self.model.encode_video_merge(video, raw_video, is_preextracted=is_preextracted)
        return frame_hiddens, frame_llms

    def postprocess_image(self, sample_data):
        if 'image' not in sample_data or sample_data['image'] is None:
            return None, None
        
        image = sample_data['image'].unsqueeze(0).to(self.device)
        raw_image = sample_data['raw_image'].unsqueeze(0).to(self.device)
        image_hiddens, image_llms = self.model.encode_image_merge(image, raw_image)
        return image_hiddens, image_llms


    def postprocess_multi(self, video_hiddens, audio_hiddens):
        if video_hiddens is None or audio_hiddens is None:
            return None, None

        multi_hiddens, multi_llms = self.model.encode_multi_merge(video_hiddens, audio_hiddens)
        return multi_hiddens, multi_llms

    def postprocess_au(self, sample_data):
        """AUæ¨¡æ€å¤„ç†"""
        if sample_data is None or 'au' not in sample_data or sample_data['au'] is None:
            return None, None
        
        # å¦‚æœä½¿ç”¨AU Agentï¼Œç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°
        if self.use_au_agent and self.au_agent is not None:
            # è§£æsample_data['au']ç»“æ„
            au_values = None
            au_description = None
            
            if isinstance(sample_data['au'], dict):
                # MER-Factoryæ ¼å¼ï¼š{'au_values': {...}, 'au_description': "..."}
                if 'au_values' in sample_data['au'] or 'active_aus' in sample_data['au']:
                    au_values = sample_data['au'].get('au_values') or sample_data['au'].get('active_aus')
                    au_description = sample_data['au'].get('au_description', None)
                else:
                    # ç›´æ¥æ˜¯AUå€¼å­—å…¸ï¼š{'AU01_r': 0.98, ...}
                    au_values = sample_data['au']
            elif hasattr(sample_data['au'], 'values'):
                au_values = sample_data['au']['values']
                au_description = sample_data['au'].get('description', None)
            else:
                # å›é€€åˆ°é¢„æå–ç‰¹å¾æ¨¡å¼
                au = sample_data['au'].unsqueeze(0).to(self.device)
                au_hiddens, au_llms = self.model.encode_au_merge(au, is_preextracted=True)
                return au_hiddens, au_llms
            
            # ä½¿ç”¨AU Agentç”Ÿæˆæè¿°
            try:
                facial_description = self.au_agent.generate_description(
                    au_values=au_values,
                    au_description=au_description,  # å¦‚æœMER-Factoryæä¾›äº†åˆ™ä½¿ç”¨
                    temperature=0.7
                )
                
                # å°†æè¿°è½¬æ¢ä¸ºtext tokensï¼ˆç±»ä¼¼textæ¨¡æ€å¤„ç†ï¼‰
                au_text = f"<au>\n{facial_description}\n</au>"
                au_llms = self.to_token_ids(au_text, max_length=512).to(self.device)
                
                # AUä½œä¸ºtextå¤„ç†ï¼Œä¸éœ€è¦ç‰¹å¾å‘é‡
                return None, au_llms
                
            except Exception as e:
                print(f"âš ï¸ AU Agentç”Ÿæˆå¤±è´¥: {e}ï¼Œå›é€€åˆ°é¢„æå–ç‰¹å¾æ¨¡å¼")
                # å›é€€
                if torch.is_tensor(sample_data['au']):
                    au = sample_data['au'].unsqueeze(0).to(self.device)
                    au_hiddens, au_llms = self.model.encode_au_merge(au, is_preextracted=True)
                    return au_hiddens, au_llms
                else:
                    return None, None
        else:
            # é¢„æå–ç‰¹å¾æ¨¡å¼æˆ–CLIPå®æ—¶ç¼–ç æ¨¡å¼
            # æ£€æŸ¥AUæ•°æ®ç±»å‹
            if sample_data['au'] is None:
                print(f"âš ï¸ AUæ•°æ®ä¸ºNoneï¼Œè·³è¿‡AUå¤„ç†")
                return None, None
            
            if isinstance(sample_data['au'], dict):
                print(f"âš ï¸ AUæ•°æ®æ˜¯å­—å…¸æ ¼å¼ï¼Œä½†æœªå¯ç”¨AU Agentã€‚è¯·æ£€æŸ¥é…ç½®ï¼š")
                print(f"   1. è®­ç»ƒé…ç½®æ–‡ä»¶ä¸­æ˜¯å¦æ­£ç¡®è®¾ç½®äº†AUæ¨¡å¼")
                print(f"   2. æ¨ç†æ—¶æ˜¯å¦è®¾ç½®äº†use_au_clip_realtime=True")
                print(f"   è·³è¿‡AUå¤„ç†")
                return None, None
            
            if not torch.is_tensor(sample_data['au']):
                print(f"âš ï¸ AUæ•°æ®ç±»å‹é”™è¯¯: {type(sample_data['au'])}ï¼Œåº”è¯¥æ˜¯torch.Tensor")
                print(f"   è·³è¿‡AUå¤„ç†")
                return None, None
            
            # æ­£å¸¸å¤„ç†å¼ é‡AUç‰¹å¾
            au = sample_data['au'].unsqueeze(0).to(self.device)
            au_hiddens, au_llms = self.model.encode_au_merge(au, is_preextracted=True)
            return au_hiddens, au_llms

    
    # æ•´ä½“è¿‡ç¨‹å°±æ˜¯åœ¨æ¨¡æ‹Ÿinferenceè¿‡ç¨‹ => å°è¯•å®Œå…¨æŒ‰ç…§ training çš„æ–¹å¼è¿›è¡Œè¯»å†™
    def answer_sample(self, prompt, img_list, num_beams=1, temperature=1.0, do_sample=True,  top_p=0.9,
                    max_new_tokens=1000, min_length=1, max_length=2000, repetition_penalty=1.0, length_penalty=1.0):
        
        
        IMAGE_PATCH_TOKEN_ID = self.tokenizer.get_vocab()[config.DEFAULT_IMAGE_PATCH_TOKEN]
        AUDIO_PATCH_TOKEN_ID = self.tokenizer.get_vocab()[config.DEFAULT_AUDIO_PATCH_TOKEN]
        FRAME_PATCH_TOKEN_ID = self.tokenizer.get_vocab()[config.DEFAULT_FRAME_PATCH_TOKEN]
        FACE_PATCH_TOKEN_ID  = self.tokenizer.get_vocab()[config.DEFAULT_FACE_PATCH_TOKEN]
        MULTI_PATCH_TOKEN_ID = self.tokenizer.get_vocab()[config.DEFAULT_MULTI_PATCH_TOKEN]
        # ğŸ¯ Lexicalä¿¡æ¯ç›´æ¥ä½œä¸ºæ–‡æœ¬åµŒå…¥ï¼Œä¸éœ€è¦PATCH_TOKEN

        ###### step1: => (input_id, attention_mask) 
        ## replace and add
        prompt = self.replace_token_for_multimodal(prompt)
        input_id = self.to_token_ids(prompt, max_length)
        print (prompt)
        
        ## length limits
        current_max_len = len(input_id) + max_new_tokens
        if current_max_len - max_length > 0:
            print('Warning: The number of tokens in current conversation exceeds the max length. '
                'The model will not see the contexts outside the range.')
        begin_idx = max(0, current_max_len - max_length)
        input_id = input_id[begin_idx:]
        attention_mask=input_id.ne(self.tokenizer.pad_token_id).to(self.device)

        ###### step2: (input_ids) => (inputs_embeds)
        temp_input_id = copy.deepcopy(input_id).to(self.device)
        temp_input_id[temp_input_id == FRAME_PATCH_TOKEN_ID] = 0
        temp_input_id[temp_input_id == FACE_PATCH_TOKEN_ID]  = 0
        temp_input_id[temp_input_id == AUDIO_PATCH_TOKEN_ID] = 0
        temp_input_id[temp_input_id == MULTI_PATCH_TOKEN_ID] = 0
        temp_input_id[temp_input_id == IMAGE_PATCH_TOKEN_ID] = 0
        # ğŸ¯ AUä¸å†ä½¿ç”¨tokenå ä½ç¬¦ï¼Œæ”¹ä¸ºcaptionæ–‡æœ¬ç›´æ¥åµŒå…¥prompt
        cur_input_embeds = self.model.llama_model.model.model.embed_tokens(temp_input_id)
        cur_input_ids = input_id
        
        # replace <ImageHere>, <AudioHere>, <FrameHere>, <FaceHere> with features
        # ğŸ¯ AUä¸å†ä½œä¸ºç‰¹å¾æ¨¡æ€ï¼Œæ”¹ä¸ºcaptionæ–‡æœ¬ç›´æ¥åµŒå…¥prompt
        cur_idx = 0
        for (patch_token_id, query_token_number, embeds) in [(FRAME_PATCH_TOKEN_ID, self.num_video_query_token, img_list['frame']),
                                                            (FACE_PATCH_TOKEN_ID,  self.num_video_query_token, img_list['face']),
                                                            (AUDIO_PATCH_TOKEN_ID, self.num_audio_query_token, img_list['audio']),
                                                            (MULTI_PATCH_TOKEN_ID, self.num_multi_query_token, img_list['multi']),
                                                            (IMAGE_PATCH_TOKEN_ID, self.num_image_query_token, img_list['image']),
                                                            ]:
            if (cur_input_ids == patch_token_id).sum() != 0:
                assert embeds is not None, f'Some input info is missing.'
                cur_features = embeds[cur_idx]
                if (cur_input_ids == patch_token_id).sum() != query_token_number:
                    raise ValueError("The number of audio patch tokens should be the same as the number of audio patches.")
                masked_indices = torch.where(cur_input_ids == patch_token_id)[0]
                mask_index_start = masked_indices[0]
                if (masked_indices != torch.arange(mask_index_start, mask_index_start+query_token_number, device=masked_indices.device, dtype=masked_indices.dtype)).any():
                    raise ValueError("The image patch tokens should be consecutive.")
                cur_input_embeds = torch.cat((cur_input_embeds[:mask_index_start], 
                                            cur_features, 
                                            cur_input_embeds[mask_index_start+query_token_number:]), dim=0)
                    
        cur_input_embeds = cur_input_embeds.unsqueeze(0) 
        attention_mask = attention_mask.unsqueeze(0) 
        ###### step3: (inputs_embeds, attention_masks) => response
        outputs = self.model.llama_model.generate(
            inputs_embeds=cur_input_embeds,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            stopping_criteria=self.stopping_criteria,
            num_beams=num_beams,
            do_sample=do_sample,
            min_length=min_length,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            length_penalty=length_penalty,
            temperature=temperature,
        )

        ###### step4: convert to batch samples 
        # maybe <bos> aaa <stop token> bbb <eos>
        response = self.tokenizer.decode(outputs[0], add_special_tokens=False)
        if response.find(self.tokenizer.bos_token) != -1:
            response = response.split(self.tokenizer.bos_token)[1]
        if response.find(self.tokenizer.eos_token) != -1:
            response = response.split(self.tokenizer.eos_token)[0]
        response = response.rsplit('###', 1)[0] # split from stop tokens '###'
        response = response.split('Assistant:')[-1].strip()
        return response
