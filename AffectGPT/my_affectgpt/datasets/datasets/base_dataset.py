import os
import tqdm
import copy
import random
import pandas as pd
from typing import Dict, Optional, Sequence, Iterable
import json

import torch
from torch.utils.data import Dataset, ConcatDataset
from my_affectgpt.models.tokenizer import load_tokenizer_from_LLM

import torch
from PIL import Image
import numpy as np

import transformers
from my_affectgpt.processors.video_processor import load_video, load_face
from my_affectgpt.models.ImageBind.data import load_audio, transform_audio
import config

class BaseDataset():
    def __init__(self, vis_processor=None, txt_processor=None, img_processor=None, model_cfg=None, dataset_cfg=None,
                vis_root=None, ann_path=None, wav_root=None, face_root=None, img_root=None):
        
        ####################################
        ## part1: common ones
        self.vis_root = vis_root
        self.img_root = img_root
        self.wav_root = wav_root
        self.ann_path = ann_path
        self.face_root = face_root
        self.vis_processor = vis_processor
        self.txt_processor = txt_processor
        self.img_processor = img_processor
        self.model_cfg = model_cfg
        self.dataset_cfg = dataset_cfg

        self.image_caption_prompt_candidates = ["Describe this image in detail.",
                                                "Take a look at this image and describe what you notice.",
                                                "Please provide a detailed description of the picture.",
                                                "Could you describe the contents of this image for me?"]

        self.audio_caption_prompt_candidates = ["Describe this audio in detail.",
                                                "Listen to this audio and describe what you hear.",
                                                "Please provide a detailed description of this audio.",
                                                "Could you describe the contents of this audio for me?"]

        ####################################
        ## part2: (model_cfg, dataset_cfg) specific ones
        if model_cfg is None or dataset_cfg is None: return
        
        self.max_length = model_cfg.max_length
        self.num_video_query_token = model_cfg.num_video_query_token
        self.num_audio_query_token = model_cfg.num_audio_query_token
        self.num_multi_query_token = model_cfg.num_multi_query_token
        self.num_image_query_token = model_cfg.num_image_query_token
        self.num_au_query_token = getattr(model_cfg, 'num_au_query_token', 8)  # AU query tokenæ•°é‡ï¼Œé»˜è®¤8

        ## æ§åˆ¶è§†é¢‘é‡‡æ ·çš„å¸§æ•°
        self.n_frms = model_cfg.vis_processor.train.n_frms
        
        # Frameé‡‡æ ·é…ç½® - å¯ä»¥é€šè¿‡dataset_cfgè¦†ç›–
        self.frame_n_frms = getattr(dataset_cfg, 'frame_n_frms', self.n_frms)  # Frameå¸§æ•°ï¼Œé»˜è®¤ä¸n_frmsç›¸åŒ
        self.frame_sampling = getattr(dataset_cfg, 'frame_sampling', 'uniform')  # Frameé‡‡æ ·ç­–ç•¥ï¼Œé»˜è®¤uniform
        
        # MER-Factoryè¾“å‡ºè·¯å¾„ - ç”¨äºemotion_peakæ™ºèƒ½é‡‡æ ·å’ŒAUå®æ—¶å¤„ç†
        self.mer_factory_output = getattr(dataset_cfg, 'mer_factory_output', None)
        
        # CLIPæ¨¡å‹ï¼ˆç”¨äºAUå®æ—¶ç¼–ç ï¼‰ - æ‡’åŠ è½½
        self._clip_model = None
        self._clip_preprocess = None
        
        # é¢„æå–ç‰¹å¾é…ç½® - ä»dataset_cfgè·å–
        self.use_preextracted_features = getattr(dataset_cfg, 'use_preextracted_features', False)
        self.preextracted_root = getattr(dataset_cfg, 'preextracted_root', None)
        self.visual_encoder = getattr(dataset_cfg, 'visual_encoder', 'CLIP_VIT_LARGE')
        self.acoustic_encoder = getattr(dataset_cfg, 'acoustic_encoder', 'HUBERT_LARGE')
        self.clips_per_video = getattr(dataset_cfg, 'clips_per_video', 8)
        
        # ğŸ¯ å®æ—¶ç‰¹å¾æå–é…ç½® - ä»dataset_cfgè·å–
        self.use_realtime_extraction = getattr(dataset_cfg, 'use_realtime_extraction', False)
        self.extraction_server_host = getattr(dataset_cfg, 'extraction_server_host', 'localhost')
        self.extraction_server_port = getattr(dataset_cfg, 'extraction_server_port', 12345)
        self.feature_client = None
        
        # åˆå§‹åŒ–å®æ—¶ç‰¹å¾æå–å®¢æˆ·ç«¯
        if self.use_realtime_extraction:
            try:
                from simple_feature_client import SimpleFeatureClient
                self.feature_client = SimpleFeatureClient(
                    server_host=self.extraction_server_host,
                    server_port=self.extraction_server_port
                )
                if self.feature_client.connect():
                    print(f'[DATASET] å®æ—¶ç‰¹å¾æå–å®¢æˆ·ç«¯å·²è¿æ¥: {self.extraction_server_host}:{self.extraction_server_port}')
                else:
                    print(f'[DATASET] å®æ—¶ç‰¹å¾æå–å®¢æˆ·ç«¯è¿æ¥å¤±è´¥ï¼Œå°†å›é€€åˆ°å®æ—¶å¤„ç†æ¨¡å¼')
                    self.feature_client = None
                    self.use_realtime_extraction = False
            except ImportError as e:
                print(f'[DATASET] æ— æ³•å¯¼å…¥å®æ—¶ç‰¹å¾æå–å®¢æˆ·ç«¯: {e}')
                self.feature_client = None
                self.use_realtime_extraction = False
        
        print(f'====== Frame Sampling Config ======')
        print(f'Frame frames: {self.frame_n_frms}, Frame sampling: {self.frame_sampling}')
        print(f'Face frames: {self.n_frms}, Face sampling: uniform')
        
        if self.use_realtime_extraction:
            print(f'[DATASET] å®æ—¶ç‰¹å¾æå–æ¨¡å¼å·²å¯ç”¨')
            print(f'Client status: {"Connected" if self.feature_client else "Failed"}')
        elif self.use_preextracted_features:
            print(f'====== Preextracted Features Config ======')
            print(f'ğŸ¯ Preextracted mode: ENABLED')
            print(f'Root: {self.preextracted_root}')
            print(f'Visual encoder: {self.visual_encoder}')
            print(f'Acoustic encoder: {self.acoustic_encoder}')
            print(f'Clips per video: {self.clips_per_video}')
        else:
            print(f'ğŸ”„ Real-time mode: ENABLED')

        # è¿™é‡Œtokençš„è®¾ç½®å’Œ affectgpt.py ä¸­çš„ä¸€è‡´ (æ‰€ä»¥è¿™éƒ¨åˆ†è°ƒç”¨æ”¹æˆå…¨å±€è°ƒç”¨äº†)
        self.tokenizer = load_tokenizer_from_LLM(model_cfg.llama_model)
        self.IMAGE_PATCH_TOKEN_ID = self.tokenizer.get_vocab()[config.DEFAULT_IMAGE_PATCH_TOKEN]
        self.AUDIO_PATCH_TOKEN_ID = self.tokenizer.get_vocab()[config.DEFAULT_AUDIO_PATCH_TOKEN]
        self.FRAME_PATCH_TOKEN_ID = self.tokenizer.get_vocab()[config.DEFAULT_FRAME_PATCH_TOKEN]
        self.FACE_PATCH_TOKEN_ID  = self.tokenizer.get_vocab()[config.DEFAULT_FACE_PATCH_TOKEN]
        self.MULTI_PATCH_TOKEN_ID = self.tokenizer.get_vocab()[config.DEFAULT_MULTI_PATCH_TOKEN]
        
        # è®©æ¨¡å‹åªè¯»å–ä¸€å®šæ¯”ä¾‹çš„æ–‡ä»¶
        if 'ratio' in dataset_cfg and dataset_cfg.ratio < 1:
            self.annotation = self.func_random_sample_subset(self.annotation, ratio=dataset_cfg.ratio)
            print(f'after sampled sample number: {len(self.annotation)}')

        ####################################
        ## part3: debug
        sample1 = self.__getitem__(random.randint(0, len(self)-1))
        sample2 = self.__getitem__(random.randint(0, len(self)-1))
        sample3 = self.__getitem__(random.randint(0, len(self)-1))
        self.func_visualize_samples(sample1)
        self.func_visualize_samples(sample2)
        self.func_visualize_samples(sample3)
        samples = [sample1, sample2, sample3]
        self.collater(samples)

        ## debug2: for all datasets (whether contains errors)
        # print ('Debug: whether all data are readable?')
        # for index in tqdm.tqdm(range(len(self))):
        #     sample = self.__getitem__(index)
        #     self.func_visualize_samples(sample)
        #     # print (sample['raw_audio'].shape)

        ## debug3: short version, only length
        print ('training sample number: ', len(self))
        ####################################

    def __len__(self):
        return len(self.annotation)
    
    def func_visualize_samples(self, sample):
        text_input = copy.deepcopy(sample['text_input'])
        input_convert = self.tokenizer.decode(text_input)
        print (input_convert)

        label = copy.deepcopy(sample['label'])
        label[label==config.IGNORE_INDEX] = self.tokenizer.bos_token_id
        output_convert = self.tokenizer.decode(label)
        print (output_convert)
    
    # to_token_ids: å¼€å¤´ä¸å¢åŠ ç‰¹æ®Šç¬¦å·ï¼Œè£å‰ªè¾“å…¥ä¿è¯ä¸è¶…è¿‡ max_length
    def to_token_ids(self, text, max_length):
        input_ids = self.tokenizer(text, return_tensors="pt", padding="longest", max_length=max_length, 
                                truncation=True, add_special_tokens=False).input_ids[0]
        return input_ids
    
    def _load_au_result_from_mer_factory(self, video_name: str) -> Optional[Dict]:
        """
        ä»MER-Factoryè¾“å‡ºçš„JSONæ–‡ä»¶åŠ è½½AU result
        
        Args:
            video_name: è§†é¢‘åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰
        
        Returns:
            AU resultå­—å…¸: {'active_aus': {...}, 'au_description': "..."}
            å¦‚æœåŠ è½½å¤±è´¥è¿”å›None
        """
        if not self.mer_factory_output:
            return None
        
        # MER-Factory JSONè·¯å¾„
        # è·¯å¾„ç»“æ„: {mer_factory_output}/{dataset}/{video_name}/{video_name}_au_analysis.json
        # ä¾‹å¦‚: /home/project/MER-Factory/output/MER2023/sample_00000905/sample_00000905_au_analysis.json
        dataset_name = getattr(self, 'dataset', '')
        if dataset_name:
            au_json_path = os.path.join(self.mer_factory_output, dataset_name, video_name, f'{video_name}_au_analysis.json')
        else:
            # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰datasetå±æ€§ï¼Œç›´æ¥ä½¿ç”¨mer_factory_output
            au_json_path = os.path.join(self.mer_factory_output, video_name, f'{video_name}_au_analysis.json')
        
        if not os.path.exists(au_json_path):
            return None
        
        try:
            with open(au_json_path, 'r', encoding='utf-8') as f:
                au_data = json.load(f)
            
            # è·å–per_frame_au_descriptions
            per_frame_data = au_data.get('per_frame_au_descriptions', [])
            if not per_frame_data:
                return None
            
            # é€‰æ‹©å¸§ç­–ç•¥
            frame_sampling = getattr(self, 'frame_sampling', 'uniform')
            
            if frame_sampling == 'emotion_peak':
                # ä½¿ç”¨å³°å€¼å¸§ï¼ˆå¦‚æœæœ‰ï¼‰
                au_info = au_data.get('au_info', {})
                peak_frames = au_info.get('peak_frames', [])
                
                if peak_frames and len(peak_frames) > 0:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªå³°å€¼å¸§
                    peak_index = peak_frames[0]['peak_index']
                    if peak_index < len(per_frame_data):
                        frame_data = per_frame_data[peak_index]
                    else:
                        frame_data = per_frame_data[0]
                else:
                    # æ²¡æœ‰å³°å€¼å¸§ï¼Œä½¿ç”¨ç¬¬ä¸€å¸§
                    frame_data = per_frame_data[0]
            else:
                # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€å¸§
                frame_data = per_frame_data[0]
            
            # è¿”å›AU result
            return {
                'active_aus': frame_data.get('active_aus', {}),
                'au_description': frame_data.get('au_description', '')
            }
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½AU resultå¤±è´¥ {au_json_path}: {e}")
            return None
    
    
    def _load_au_clip_features_from_json(self, video_name: str) -> Optional[torch.Tensor]:
        """
        ä»MER-Factory JSONåŠ è½½summary_descriptionå¹¶å®æ—¶CLIPç¼–ç 
        ç”¨äºæ¨ç†æ—¶ä¸åŠ è½½AU Agentï¼Œç›´æ¥ä½¿ç”¨é¢„ç”Ÿæˆçš„æè¿°
        
        Args:
            video_name: è§†é¢‘åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰
        
        Returns:
            CLIPç¼–ç åçš„AUç‰¹å¾ [N, 512]ï¼Œå¤±è´¥è¿”å›None
        """
        if not self.mer_factory_output:
            # åªåœ¨ç¬¬ä¸€æ¬¡æç¤º
            if not hasattr(BaseDataset, '_warned_no_mer_factory'):
                print(f"âš ï¸ [AU CLIP] mer_factory_outputæœªé…ç½®")
                BaseDataset._warned_no_mer_factory = True
            return None
        
        # MER-Factory JSONè·¯å¾„
        # è·¯å¾„ç»“æ„: {mer_factory_output}/{dataset}/{video_name}/{video_name}_au_analysis.json
        dataset_name = getattr(self, 'dataset', '')
        if dataset_name:
            au_json_path = os.path.join(self.mer_factory_output, dataset_name, video_name, f'{video_name}_au_analysis.json')
        else:
            # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰datasetå±æ€§ï¼Œç›´æ¥ä½¿ç”¨mer_factory_output
            au_json_path = os.path.join(self.mer_factory_output, video_name, f'{video_name}_au_analysis.json')
        
        if not os.path.exists(au_json_path):
            # åªåœ¨å‰å‡ æ¬¡æç¤ºï¼Œé¿å…åˆ·å±
            if not hasattr(BaseDataset, '_json_not_found_count'):
                BaseDataset._json_not_found_count = 0
            if BaseDataset._json_not_found_count < 3:
                print(f"âš ï¸ [AU CLIP] JSONæ–‡ä»¶ä¸å­˜åœ¨: {au_json_path}")
                BaseDataset._json_not_found_count += 1
                if BaseDataset._json_not_found_count == 3:
                    print(f"â„¹ï¸ [AU CLIP] åç»­ç¼ºå¤±æ–‡ä»¶å°†ä¸å†æç¤º...")
            return None
        
        try:
            import clip
            
            with open(au_json_path, 'r', encoding='utf-8') as f:
                au_data = json.load(f)
            
            # ä¼˜å…ˆä½¿ç”¨summary_descriptionï¼ˆçº¯å‡€çš„assistantæè¿°ï¼‰
            summary_description = au_data.get('summary_description', {})
            
            # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰summary_descriptionï¼Œå°è¯•fine_grained_descriptions
            if not summary_description:
                fine_grained = au_data.get('fine_grained_descriptions', {})
                if not fine_grained:
                    # åªåœ¨å‰å‡ æ¬¡æç¤º
                    if not hasattr(BaseDataset, '_no_description_count'):
                        BaseDataset._no_description_count = 0
                    if BaseDataset._no_description_count < 2:
                        print(f"âš ï¸ [AU CLIP] JSONä¸­æ—¢æ²¡æœ‰summary_descriptionä¹Ÿæ²¡æœ‰fine_grained_descriptions")
                        BaseDataset._no_description_count += 1
                    return None
                summary_description = fine_grained
            
            if not summary_description:
                return None
            
            # å‡†å¤‡æ–‡æœ¬åˆ—è¡¨ï¼ˆæŒ‰å¸§å·æ’åºï¼‰
            frame_indices = sorted(summary_description.keys(), key=int)
            texts = [summary_description[idx] for idx in frame_indices]
            
            if len(texts) == 0:
                return None
            
            # åŠ è½½CLIPæ¨¡å‹ï¼ˆå¦‚æœè¿˜æ²¡åŠ è½½ï¼‰
            # ä½¿ç”¨ç±»çº§åˆ«å˜é‡ï¼Œç¡®ä¿åªåŠ è½½ä¸€æ¬¡
            if not hasattr(BaseDataset, '_clip_model_loaded'):
                device = "cuda" if torch.cuda.is_available() else "cpu"
                print(f"ğŸ“¥ [AU CLIP] åŠ è½½CLIPæ¨¡å‹ (ViT-B/32) åˆ° {device}...")
                self._clip_model, _ = clip.load("ViT-B/32", device=device)
                self._clip_device = device
                BaseDataset._clip_model_loaded = True
                print(f"âœ… [AU CLIP] CLIPæ¨¡å‹åŠ è½½å®Œæˆ")
            elif not hasattr(self, '_clip_model') or self._clip_model is None:
                # å…¶ä»–å®ä¾‹ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹
                device = "cuda" if torch.cuda.is_available() else "cpu"
                self._clip_model, _ = clip.load("ViT-B/32", device=device)
                self._clip_device = device
            
            # ä½¿ç”¨CLIPç¼–ç 
            text_tokens = clip.tokenize(texts, truncate=True).to(self._clip_device)
            
            with torch.no_grad():
                text_features = self._clip_model.encode_text(text_tokens)  # [N, 512]
                # å½’ä¸€åŒ–
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                # ä¿æŒåŸå§‹float32ç²¾åº¦ï¼Œè®©æ¨¡å‹å±‚è‡ªåŠ¨è½¬æ¢
                # è¿™æ ·å¯ä»¥å…¼å®¹ä¸åŒçš„æ¨¡å‹ç²¾åº¦è®¾ç½®
            
            # ä¸å†è¾“å‡ºæ¯ä¸ªæ ·æœ¬çš„æˆåŠŸä¿¡æ¯ï¼Œå‡å°‘æ—¥å¿—
            return text_features
            
        except ImportError:
            print(f"âš ï¸ [AU CLIP] CLIPåº“æœªå®‰è£…ï¼Œæ— æ³•ç¼–ç AUæè¿°")
            print(f"   è¯·è¿è¡Œ: pip install git+https://github.com/openai/CLIP.git")
            return None
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å¹¶ç¼–ç AUæè¿°å¤±è´¥ {au_json_path}: {e}")
            return None


    def func_map_valence_to_emotion(self, valence):
        if valence > 0:
            return 'positive'
        elif valence < 0:
            return 'negative'
        else:
            return 'neutral'
        

    def get_cur_label_type(self, label_type_candidates, label_type):
        if label_type == 'hybird':
            index = random.randint(0, len(label_type_candidates) -1)
            return label_type_candidates[index]
        else:
            assert label_type in label_type_candidates, f'error label type: {label_type} not in {label_type_candidates}'
            return label_type
        
    
    def func_random_prompts(self, candidates):
        index = random.randint(0, len(candidates) - 1)
        prompt = candidates[index]
        return prompt
    
    
    # éšæœºé‡‡æ ·ä¸€ä¸ª annotations
    def func_random_sample_subset(self, annotations, ratio=0.1):
        annotations_subset = random.sample(annotations, int(len(annotations)*ratio))
        return annotations_subset


    ###########################################################
    ## æ•°æ®è¯»å–éƒ¨åˆ†æ“ä½œ
    ###########################################################
    # all types: {audio, frame, face, image}
    def get_needed_data(self, face_or_frame):
        if face_or_frame == 'faceframe': # (face, frame, audio, text)
            needed_data = ['audio', 'frame', 'face']
        elif face_or_frame == 'face': # (face, audio, text)
            needed_data = ['audio', 'face']
        elif face_or_frame == 'frame': # (frame, audio, text)
            needed_data = ['audio', 'frame']
        elif face_or_frame == 'audioonly': # (audio)
            needed_data = ['audio']
        elif face_or_frame == 'textonly':  # (text)
            needed_data = []
        elif face_or_frame == 'faceonly':  # (face)
            needed_data = ['face']
        elif face_or_frame == 'frameonly': # (frame)
            needed_data = ['frame']
        elif face_or_frame == 'multiface_text': # (multi, text)
            needed_data = ['face', 'audio', 'multi']
        elif face_or_frame == 'multiface_audio_face_text': # (multi, face, audio, text)
            needed_data = ['face', 'audio', 'multi']
        elif face_or_frame == 'image': # (image)
            needed_data = ['image']
        elif face_or_frame == 'multiframe_audio_frame_text': # (multi, face, audio, text)
            needed_data = ['frame', 'audio', 'multi']
        elif face_or_frame == 'multiface_audio_face_frame_text': # (multi, face, audio, text)
            needed_data = ['frame', 'face', 'audio', 'multi']
        elif face_or_frame == 'multiface_audio_face_frame_au_text': # (multi, face, audio, au, text)
            needed_data = ['frame', 'face', 'audio', 'au', 'multi']
        elif face_or_frame == 'audio_text': # (audio, text)
            needed_data = ['audio']
        elif face_or_frame == 'face_text': # (face, text)
            needed_data = ['face']
        elif face_or_frame == 'frame_text': # (frame, text)
            needed_data = ['frame']
        return needed_data
    

    def read_frame_face_audio_text(self, video_path=None, face_npy=None, audio_path=None, image_path=None, sample_name=None):

        sample_data = {}

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é¢„æå–ç‰¹å¾æ¨¡å¼ (æ¨ç†æ—¶å¯èƒ½ä¸å­˜åœ¨è¿™äº›å±æ€§)
        use_preextracted = getattr(self, 'use_preextracted_features', False)
        preextracted_root = getattr(self, 'preextracted_root', None)

        # step1: read (raw_frame, frame) - å¯é…ç½®çš„Frameé‡‡æ ·ç­–ç•¥
        frame, raw_frame = None, None
        if 'frame' in self.needed_data:
            # ğŸ¯ æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦ä½¿ç”¨å®æ—¶ç‰¹å¾æå–æœåŠ¡
            if hasattr(self, 'use_realtime_extraction') and self.use_realtime_extraction:
                # å®æ—¶ç‰¹å¾æå–æ¨¡å¼ - é€šè¿‡æœåŠ¡æå–ç‰¹å¾ï¼ˆä¿æŒæ•°æ®å¢å¼ºï¼‰
                if hasattr(self, 'feature_client') and self.feature_client:
                    realtime_features = self.feature_client.extract_features(
                        sample_name=sample_name,
                        modalities=['frame'],
                        video_path=video_path,
                        n_frms=getattr(self, 'frame_n_frms', self.n_frms),
                        frame_sampling=getattr(self, 'frame_sampling', 'uniform')
                    )
                    if realtime_features and 'frame' in realtime_features:
                        frame_features = realtime_features['frame']  # [T, D] - ç¼–ç å™¨è¾“å‡ºç‰¹å¾
                        frame = torch.from_numpy(frame_features).float()
                        raw_frame = frame  # åˆ†å¸ƒå¼æ¨¡å¼ä¸‹ä½¿ç”¨ç›¸åŒæ•°æ®
                        sample_data['frame_preextracted'] = True  # æ ‡è®°ä¸ºå·²æå–ç‰¹å¾ï¼ˆç¼–ç å™¨è¾“å‡ºï¼‰
                        pass  # ç‰¹å¾æå–æˆåŠŸ
                    else:
                        print(f"âš ï¸ å®æ—¶Frameç‰¹å¾æå–å¤±è´¥: {sample_name}")
            elif use_preextracted and preextracted_root and sample_name:
                # é¢„æå–ç‰¹å¾æ¨¡å¼ - ç›´æ¥åŠ è½½.npyç‰¹å¾æ–‡ä»¶
                frame_n_frms = getattr(self, 'frame_n_frms', 1)
                frame_sampling = getattr(self, 'frame_sampling', 'uniform')
                visual_encoder = getattr(self, 'visual_encoder', 'CLIP_VIT_LARGE')
                
                frame_feat_dir = f'frame_{visual_encoder}_{frame_sampling}_{frame_n_frms}frms'
                frame_feat_path = os.path.join(preextracted_root, frame_feat_dir, f'{sample_name}.npy')
                
                if os.path.exists(frame_feat_path):
                    frame_features = np.load(frame_feat_path)  # [T, D]
                    frame = torch.from_numpy(frame_features).float()  # è½¬æ¢ä¸ºtensor
                    raw_frame = frame  # é¢„æå–æ¨¡å¼ä¸‹raw_frameä¸frameç›¸åŒ
                else:
                    # é¢„æå–ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨å®æ—¶å¤„ç†æ¨¡å¼
                    pass
            else:
                # å®æ—¶å¤„ç†æ¨¡å¼ - åŸæœ‰é€»è¾‘
                if video_path is not None:
                    frame_n_frms = getattr(self, 'frame_n_frms', self.n_frms)  # é»˜è®¤ä½¿ç”¨n_frms
                    frame_sampling = getattr(self, 'frame_sampling', 'uniform')  # é»˜è®¤ä½¿ç”¨uniformé‡‡æ ·
                    mer_factory_output = getattr(self, 'mer_factory_output', None)  # MER-Factoryè¾“å‡ºè·¯å¾„
                    
                    # æå–video_nameï¼ˆä¸å«æ‰©å±•åï¼‰
                    video_name = None
                    if sample_name:
                        video_name = sample_name
                    elif video_path:
                        video_name = os.path.splitext(os.path.basename(video_path))[0]
                    
                    raw_frame, msg = load_video(
                        video_path=video_path,
                        n_frms=frame_n_frms,
                        height=224,
                        width=224,
                        sampling=frame_sampling,
                        return_msg=True,
                        video_name=video_name,  # ä¼ é€’video_nameç”¨äºæ™ºèƒ½é‡‡æ ·
                        mer_factory_output=mer_factory_output  # ä¼ é€’MER-Factoryè·¯å¾„
                    )
                    frame = self.vis_processor.transform(raw_frame) # [3, frame_n_frms, 224, 224]
        # åªæœ‰å½“frameç‰¹å¾æœ‰æ•ˆæ—¶æ‰æ·»åŠ åˆ°æ ·æœ¬ä¸­
        if frame is not None:
            sample_data['frame'] = frame
            sample_data['raw_frame'] = raw_frame
        else:
            # Frameç‰¹å¾æ— æ•ˆï¼Œå¦‚æœéœ€è¦Frameæ¨¡æ€åˆ™è·³è¿‡æ­¤æ ·æœ¬
            if 'frame' in self.needed_data:
                print(f"âš ï¸ Frameç‰¹å¾æ— æ•ˆï¼Œè·³è¿‡æ ·æœ¬: {sample_name}")
                return None  # è¿”å›Noneè¡¨ç¤ºæ­¤æ ·æœ¬æ— æ•ˆï¼Œéœ€è¦é‡æ–°é€‰æ‹©
            # ç¡®ä¿frameç›¸å…³çš„æ ‡å¿—ä¹Ÿä¸è®¾ç½®
            if 'frame_preextracted' in sample_data:
                del sample_data['frame_preextracted']

        # step2: read (raw_face, face)
        face, raw_face = None, None
        if 'face' in self.needed_data:
            # ğŸ¯ æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦ä½¿ç”¨å®æ—¶ç‰¹å¾æå–æœåŠ¡
            if hasattr(self, 'use_realtime_extraction') and self.use_realtime_extraction:
                # å®æ—¶ç‰¹å¾æå–æ¨¡å¼ - é€šè¿‡æœåŠ¡æå–ç‰¹å¾ï¼ˆä¿æŒæ•°æ®å¢å¼ºï¼‰
                if hasattr(self, 'feature_client') and self.feature_client:
                    realtime_features = self.feature_client.extract_features(
                        sample_name=sample_name,
                        modalities=['face'],
                        face_path=face_npy,
                        n_frms=self.n_frms
                    )
                    if realtime_features and 'face' in realtime_features:
                        face_features = realtime_features['face']  # [T, D] - ç¼–ç å™¨è¾“å‡ºç‰¹å¾
                        face = torch.from_numpy(face_features).float()
                        raw_face = face  # åˆ†å¸ƒå¼æ¨¡å¼ä¸‹ä½¿ç”¨ç›¸åŒæ•°æ®
                        sample_data['face_preextracted'] = True  # æ ‡è®°ä¸ºå·²æå–ç‰¹å¾ï¼ˆç¼–ç å™¨è¾“å‡ºï¼‰
                        pass  # ç‰¹å¾æå–æˆåŠŸ
                    else:
                        print(f"âš ï¸ å®æ—¶Faceç‰¹å¾æå–å¤±è´¥: {sample_name}")
            elif use_preextracted and preextracted_root and sample_name:
                # é¢„æå–ç‰¹å¾æ¨¡å¼ - ç›´æ¥åŠ è½½.npyç‰¹å¾æ–‡ä»¶
                visual_encoder = getattr(self, 'visual_encoder', 'CLIP_VIT_LARGE')
                n_frms = getattr(self, 'n_frms', 8)
                face_feat_dir = f'face_{visual_encoder}_{n_frms}frms'
                face_feat_path = os.path.join(preextracted_root, face_feat_dir, f'{sample_name}.npy')
                
                if os.path.exists(face_feat_path):
                    face_features = np.load(face_feat_path)  # [T, D]
                    face = torch.from_numpy(face_features).float()  # è½¬æ¢ä¸ºtensor
                    raw_face = face  # é¢„æå–æ¨¡å¼ä¸‹raw_faceä¸faceç›¸åŒ
                    sample_data['face_preextracted'] = True  # æ ‡è®°ä¸ºé¢„æå–ç‰¹å¾
                else:
                    # é¢„æå–ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨å®æ—¶å¤„ç†æ¨¡å¼
                    pass
            else:
                # å®æ—¶å¤„ç†æ¨¡å¼ - åŸæœ‰é€»è¾‘
                if face_npy is not None:
                    raw_face, msg = load_face(
                        face_npy=face_npy,
                        n_frms = self.n_frms,
                        height = 224,
                        width  = 224,
                        sampling ="uniform",
                        return_msg=True
                    )
                    face = self.vis_processor.transform(raw_face) # [3, 8, 224, 224]
        # åªæœ‰å½“faceç‰¹å¾æœ‰æ•ˆæ—¶æ‰æ·»åŠ åˆ°æ ·æœ¬ä¸­
        if face is not None:
            sample_data['face'] = face
            sample_data['raw_face'] = raw_face
        else:
            # Faceç‰¹å¾æ— æ•ˆï¼Œä¸æ·»åŠ åˆ°æ ·æœ¬ä¸­
            print(f"âš ï¸ Faceç‰¹å¾æ— æ•ˆï¼Œè·³è¿‡Faceæ¨¡æ€: {sample_name}")
            # ç¡®ä¿faceç›¸å…³çš„æ ‡å¿—ä¹Ÿä¸è®¾ç½®
            if 'face_preextracted' in sample_data:
                del sample_data['face_preextracted']

        # step3: read audio [éœ€è¦é’ˆå¯¹æ²¡æœ‰ audio track çš„ video è¿›è¡Œé¢å¤–å¤„ç†]
        audio, raw_audio = None, None
        if 'audio' in self.needed_data:
            # ğŸ¯ æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦ä½¿ç”¨å®æ—¶ç‰¹å¾æå–æœåŠ¡
            if hasattr(self, 'use_realtime_extraction') and self.use_realtime_extraction:
                # å®æ—¶ç‰¹å¾æå–æ¨¡å¼ - é€šè¿‡æœåŠ¡æå–ç‰¹å¾ï¼ˆä¿æŒæ•°æ®å¢å¼ºï¼‰
                if hasattr(self, 'feature_client') and self.feature_client:
                    realtime_features = self.feature_client.extract_features(
                        sample_name=sample_name,
                        modalities=['audio'],
                        audio_path=audio_path,
                        clips_per_video=self.clips_per_video
                    )
                    if realtime_features and 'audio' in realtime_features:
                        audio_features = realtime_features['audio']  # [T, D] - ç¼–ç å™¨è¾“å‡ºç‰¹å¾
                        audio = torch.from_numpy(audio_features).float()
                        raw_audio = audio  # åˆ†å¸ƒå¼æ¨¡å¼ä¸‹ä½¿ç”¨ç›¸åŒæ•°æ®
                        sample_data['audio_preextracted'] = True  # æ ‡è®°ä¸ºå·²æå–ç‰¹å¾ï¼ˆç¼–ç å™¨è¾“å‡ºï¼‰
                        pass  # ç‰¹å¾æå–æˆåŠŸ
                    else:
                        print(f"âš ï¸ å®æ—¶Audioç‰¹å¾æå–å¤±è´¥: {sample_name}")
            elif use_preextracted and preextracted_root and sample_name:
                # é¢„æå–ç‰¹å¾æ¨¡å¼ - ç›´æ¥åŠ è½½.npyç‰¹å¾æ–‡ä»¶
                acoustic_encoder = getattr(self, 'acoustic_encoder', 'HUBERT_LARGE')
                clips_per_video = getattr(self, 'clips_per_video', 8)
                audio_feat_dir = f'audio_{acoustic_encoder}_{clips_per_video}clips'
                audio_feat_path = os.path.join(preextracted_root, audio_feat_dir, f'{sample_name}.npy')
                
                if os.path.exists(audio_feat_path):
                    audio_features = np.load(audio_feat_path)  # [T, D]
                    audio = torch.from_numpy(audio_features).float()  # è½¬æ¢ä¸ºtensor
                    raw_audio = audio  # é¢„æå–æ¨¡å¼ä¸‹raw_audioä¸audioç›¸åŒ
                else:
                    # é¢„æå–ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨å®æ—¶å¤„ç†æ¨¡å¼
                    pass
            else:
                # å®æ—¶å¤„ç†æ¨¡å¼ - åŸæœ‰é€»è¾‘
                if audio_path is not None:
                    raw_audio = load_audio([audio_path], "cpu", clips_per_video=8)[0] # [8, 1, 16000*2s]
                    audio = transform_audio(raw_audio, "cpu") # [8, 1, 128, 204]
        # åªæœ‰å½“audioç‰¹å¾æœ‰æ•ˆæ—¶æ‰æ·»åŠ åˆ°æ ·æœ¬ä¸­
        if audio is not None:
            sample_data['audio'] = audio
            sample_data['raw_audio'] = raw_audio
        else:
            # Audioç‰¹å¾æ— æ•ˆï¼Œä¸æ·»åŠ åˆ°æ ·æœ¬ä¸­
            print(f"âš ï¸ Audioç‰¹å¾æ— æ•ˆï¼Œè·³è¿‡Audioæ¨¡æ€: {sample_name}")
            # ç¡®ä¿audioç›¸å…³çš„æ ‡å¿—ä¹Ÿä¸è®¾ç½®
            if 'audio_preextracted' in sample_data:
                del sample_data['audio_preextracted']
        
        # step4: read multi features (Face+Audioèåˆç‰¹å¾)
        multi, raw_multi = None, None
        if 'multi' in self.needed_data:
            if use_preextracted and preextracted_root and sample_name:
                # é¢„æå–Multiç‰¹å¾æ¨¡å¼ - ç›´æ¥åŠ è½½èåˆåçš„ç‰¹å¾
                visual_encoder = getattr(self, 'visual_encoder', 'CLIP_VIT_LARGE')
                acoustic_encoder = getattr(self, 'acoustic_encoder', 'HUBERT_LARGE')
                # ä¼˜å…ˆå°è¯•å®Œæ•´ç‰ˆæœ¬ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•ç®€åŒ–ç‰ˆæœ¬ï¼ˆå‘åå…¼å®¹ï¼‰
                multi_feat_dir_complete = f'multi_{visual_encoder}_{acoustic_encoder}_complete'
                multi_feat_dir_simple = f'multi_{visual_encoder}_{acoustic_encoder}_simple'
                
                # æ£€æŸ¥å®Œæ•´ç‰ˆæœ¬ç›®å½•æ˜¯å¦å­˜åœ¨
                multi_feat_path_complete = os.path.join(preextracted_root, multi_feat_dir_complete, f'{sample_name}.npy')
                multi_feat_path_simple = os.path.join(preextracted_root, multi_feat_dir_simple, f'{sample_name}.npy')
                
                if os.path.exists(multi_feat_path_complete):
                    multi_feat_path = multi_feat_path_complete
                elif os.path.exists(multi_feat_path_simple):
                    multi_feat_path = multi_feat_path_simple
                else:
                    multi_feat_path = multi_feat_path_complete  # é»˜è®¤ä½¿ç”¨å®Œæ•´ç‰ˆæœ¬è·¯å¾„
                
                if os.path.exists(multi_feat_path):
                    multi_features = np.load(multi_feat_path)  # [D]
                    multi = torch.from_numpy(multi_features).float()  # è½¬æ¢ä¸ºtensor
                    raw_multi = multi  # é¢„æå–æ¨¡å¼ä¸‹raw_multiä¸multiç›¸åŒ
                else:
                    # é¢„æå–ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨ï¼ŒMultiç‰¹å¾å°†åœ¨æ¨¡å‹ä¸­åŠ¨æ€èåˆ
                    pass
            # æ³¨æ„ï¼šå®æ—¶æ¨¡å¼ä¸‹Multiç‰¹å¾æ˜¯åœ¨æ¨¡å‹ä¸­åŠ¨æ€èåˆçš„ï¼Œä¸åœ¨è¿™é‡Œå¤„ç†
        sample_data['multi'] = multi
        sample_data['raw_multi'] = raw_multi
        
        # step5: read AU result (ä»MER-Factory JSONåŠ è½½)
        au = None
        if 'au' in self.needed_data:
            # æ¨¡å¼1: é¢„æå–CLIPç‰¹å¾æ¨¡å¼ï¼ˆè®­ç»ƒæ¨èï¼‰
            if use_preextracted and preextracted_root and sample_name:
                au_feat_dir = 'au_CLIP_VITB32_8frms'  # AUç‰¹å¾ç›®å½•
                au_feat_path = os.path.join(preextracted_root, au_feat_dir, f'{sample_name}.npy')
                
                if os.path.exists(au_feat_path):
                    au_features = np.load(au_feat_path)  # [T, 512] CLIP text encoderè¾“å‡º
                    au = torch.from_numpy(au_features).float()  # è½¬æ¢ä¸ºtensor
                else:
                    # åªåœ¨å‰å‡ æ¬¡æç¤º
                    if not hasattr(BaseDataset, '_au_feat_missing_count'):
                        BaseDataset._au_feat_missing_count = 0
                    if BaseDataset._au_feat_missing_count < 2:
                        print(f"âš ï¸ AUç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {au_feat_path}")
                        BaseDataset._au_feat_missing_count += 1
                        if BaseDataset._au_feat_missing_count == 2:
                            print(f"â„¹ï¸ [AU] åç»­ç¼ºå¤±ç‰¹å¾æ–‡ä»¶å°†ä¸å†æç¤º...")
            
            # æ¨¡å¼2: ä»JSONå®æ—¶CLIPç¼–ç æ¨¡å¼ï¼ˆæ¨ç†æ¨èï¼Œä¸ä½¿ç”¨AU Agentï¼‰
            elif getattr(self, 'use_au_clip_realtime', False):
                # ä»video_pathæˆ–sample_nameæå–video_name
                video_name = None
                if sample_name:
                    video_name = sample_name
                elif video_path:
                    video_name = os.path.splitext(os.path.basename(video_path))[0]
                
                if video_name and self.mer_factory_output:
                    # ä»JSONåŠ è½½summary_descriptionå¹¶CLIPç¼–ç 
                    au = self._load_au_clip_features_from_json(video_name)
                    # å¤±è´¥æç¤ºå·²åœ¨_load_au_clip_features_from_jsonä¸­å¤„ç†
                else:
                    if 'au' in self.needed_data:
                        # åªåœ¨ç¬¬ä¸€æ¬¡æç¤º
                        if not hasattr(BaseDataset, '_warned_au_config'):
                            print(f"âš ï¸ AUåŠ è½½éœ€è¦video_nameå’Œmer_factory_outputé…ç½®")
                            BaseDataset._warned_au_config = True
            
            # æ¨¡å¼3: AU Agentæ¨¡å¼ï¼ˆéœ€è¦åŠ è½½AU Agentæ¨¡å‹ï¼Œæ˜¾å­˜å ç”¨å¤§ï¼‰
            else:
                # ä»video_pathæˆ–sample_nameæå–video_name
                video_name = None
                if sample_name:
                    video_name = sample_name
                elif video_path:
                    video_name = os.path.splitext(os.path.basename(video_path))[0]
                
                if video_name and self.mer_factory_output:
                    # ä»MER-Factory JSONåŠ è½½AU result
                    au = self._load_au_result_from_mer_factory(video_name)
                    # å¤±è´¥æ—¶åªåœ¨å‰å‡ æ¬¡æç¤º
                    if au is None:
                        if not hasattr(BaseDataset, '_au_result_fail_count'):
                            BaseDataset._au_result_fail_count = 0
                        if BaseDataset._au_result_fail_count < 2:
                            print(f"âš ï¸ AU resultåŠ è½½å¤±è´¥: {video_name}")
                            BaseDataset._au_result_fail_count += 1
                else:
                    if 'au' in self.needed_data:
                        # åªåœ¨ç¬¬ä¸€æ¬¡æç¤º
                        if not hasattr(BaseDataset, '_warned_au_agent_config'):
                            print(f"âš ï¸ AU Agentæ¨¡å¼éœ€è¦video_nameå’Œmer_factory_outputé…ç½®")
                            BaseDataset._warned_au_agent_config = True
        
        sample_data['au'] = au
        
        # è®¾ç½®é¢„æå–æ ‡å¿—
        # åœ¨åˆ†å¸ƒå¼å®æ—¶æå–æ¨¡å¼ä¸‹ï¼Œç‰¹å¾æ¥è‡ªæœåŠ¡ç«¯ï¼Œä¹Ÿç®—ä½œ"é¢„æå–"
        sample_data['frame_preextracted'] = (use_preextracted and 'frame' in self.needed_data) or (self.use_realtime_extraction and 'frame' in self.needed_data)
        sample_data['face_preextracted'] = (use_preextracted and 'face' in self.needed_data) or (self.use_realtime_extraction and 'face' in self.needed_data)
        sample_data['audio_preextracted'] = (use_preextracted and 'audio' in self.needed_data) or (self.use_realtime_extraction and 'audio' in self.needed_data)
        sample_data['multi_preextracted'] = use_preextracted and 'multi' in self.needed_data
        sample_data['au_preextracted'] = use_preextracted and 'au' in self.needed_data
        
        # step4: read image
        image, raw_image = None, None
        if image_path is not None and 'image' in self.needed_data:
            ###### æ”¯æŒä¸¤ç§ç±»å‹çš„ image_path è¾“å…¥ ######
            if not isinstance(image_path, Image.Image):
                raw_image = Image.open(image_path)
            else:
                raw_image = image_path
            ##########################################
            ## image process
            image = self.img_processor(raw_image.convert("RGB")) # [3, 224, 224] è¿™æ˜¯ vis processor é»˜è®¤ä¸‹çš„å¤„ç†ï¼Œæ­£å¸¸æƒ…å†µå…¶å®ä¹Ÿä¸éœ€è¦è¿™ä¸ªå†…å®¹
            image = image.unsqueeze(dim=1) # [3, 1, 224, 224]
            raw_image = torch.from_numpy(np.array(raw_image.resize((224, 224)))) # [H, W, C] => å¯èƒ½å› ä¸ºllavaä¸­çš„å›¾ç‰‡æœ‰äº›å¹¶ä¸æ˜¯ä¸€æ ·å¤§å°çš„ï¼Œä½¿å¾—è½¬æ¢è¿‡ç¨‹ä¸­æœ‰äº›
            raw_image = raw_image.permute(2, 0, 1).unsqueeze(dim=1).float() # (C, T=1, H, W)
        sample_data['image'] = image
        sample_data['raw_image'] = raw_image
        # print (sample_data)

        return sample_data


    ###########################################################
    ## QA è·å–
    ###########################################################
    ## å»ºç«‹ä¸€ä¸ª qa è¯»å–å™¨ï¼Œç”¨äºåç»­ç»Ÿä¸€åŒ–çš„å¤„ç†
    def func_get_qa_description(self, sample, question_only=False):
        question = "Please infer the person's emotional state and provide your reasoning process."

        if question_only:
            return question
        else:
            return {
                'question': question, 
                'answer':sample['description'],
                }
    
    def func_get_qa_ovlabel(self, sample, question_only=False):
        question = "Please recognize all possible emotional states of the character."

        if question_only:
            return question
        else:
            return {
                'question': question,
                'answer':  f"The character's emotional state is {sample['ovlabel']}."
                }
    
    def func_get_qa_onehot_w_candidates(self, sample, question_only=False):
        question = f"Please select the label that can best describe the person's emotional state from the provided candidate labels: {self.candidate_labels}."

        if question_only:
            return question
        else:
            return {
                'question': question,
                'answer':   f"The most likely label is {sample['onehot']}."
                }

    def func_get_qa_onehot_wo_candidates(self, sample, question_only=False):
        question = "Please recognize the character's most likely emotional state."

        if question_only:
            return question
        else:
            return {
                'question': question,
                'answer':  f"The character's emotional state is {sample['onehot']}."
                }

    def func_get_qa_valence(self, sample, question_only=False):
        question = f"Please identify the overall positive or negative emotional polarity of the main characters. " \
                 + f"The output should be a ï¬‚oating-point number ranging from {self.minval} to {self.maxval}. " \
                 + f"Here, {self.minval} indicates extremely negative emotions, 0 indicates neutral emotions, and {self.maxval} indicates extremely positive emotions. " \
                 + f"Please provide your judgment as a ï¬‚oating-point number."
        
        if question_only:
            return question
        else:
            return {
                'question': question,
                'answer': 'The valence score is %.2f.' %(sample['valence']),
                }

    def func_get_qa_sentiment(self, sample, question_only=False):
        question = "Please select the most likely sentiment label that can best describe the person's emotional state: positive, negative, neutral."
        
        if question_only:
            return question
        else:
            return {
                'question': question,
                'answer':  f"The character's sentiment state is {sample['sentiment']}.",
                }

    def func_get_qa_direct(self, sample):
        return {
            'question': sample['question'],
            'answer':   sample['answer'],
            }
    
    def func_get_qa_caption(self, sample, modality):
        if modality == 'image':
            return {
            'question': self.func_random_prompts(self.image_caption_prompt_candidates),
            'answer':   sample['caption'],
            }
        elif modality == 'audio':
            return {
            'question': self.func_random_prompts(self.audio_caption_prompt_candidates),
            'answer':   sample['caption'],
            }
    
    def func_get_qa_preference(self, sample):

        a1 = sample['preference']['a1']
        a2 = sample['preference']['a2']
        p  = sample['preference']['p']

        question = f"We provide two descriptions. a1: {a1} \t\t\t a2: {a2} Please select the one that best matches the video content."
        
        assert p in ['a1', 'a2', 'same']
        if p in ['a1', 'a2']:
            answer = f"The best one is {p}."
        else:
            answer = f'These two sentences describe the content of the video with the same accuracy.'

        return {
            'question': question,
            'answer':   answer,
            }

    # this (q, a) is used to determinate the reward value
    def func_get_description_reward(self, sample):
        reason = sample['description']
        reward = sample['reward']

        question = f"We have provided a description: {reason} \t\t\t Please evaluate and decide whether to accept or reject this description based on its alignment with the video content."

        assert reward in ['accept', 'reject']
        answer = f'{reward} this sentence.'

        return {
            'question': question,
            'answer':   answer,
        }

    ## è·å– <question, answer> ç”¨äºåç»­è®­ç»ƒ
    def get_qa_pairs(self, dataset, label_type, sample):
        
        '''
        self.  -> æ•°æ®é›†å…¨å±€çš„å†…å®¹
        sample -> æ ·æœ¬å±€éƒ¨çš„å†…å®¹
        '''
        # EMERFine æŒ‡çš„æ˜¯ (training set) é‚£ 332 samplesï¼ŒåŒæ—¶åŒ…å« ovlabel/description
        if dataset in ['EMERCoarse', 'EMERFine']:
            candidates = {
                'description': self.func_get_qa_description(sample),
                'ovlabel':     self.func_get_qa_ovlabel(sample),
            }
        
        elif dataset in ['EMERCoarseFilter']:
            candidates = {
                'description': self.func_get_qa_description(sample),
                'ovlabel':     self.func_get_qa_ovlabel(sample),
                'sentiment':   self.func_get_qa_sentiment(sample),
                'valence':     self.func_get_qa_valence(sample),
            }
        
        elif dataset in ['MERCaptionPlus', 'OVMERD']:
            candidates = {
                'description': self.func_get_qa_description(sample),
                'ovlabel':     self.func_get_qa_ovlabel(sample),
            }
        
        elif dataset in ['Preference']: # å¸¦ preference ä¼˜åŒ–
            candidates = {
                'description': self.func_get_qa_description(sample),
                'ovlabel':     self.func_get_qa_ovlabel(sample),
                'sentiment':   self.func_get_qa_sentiment(sample),
                'valence':     self.func_get_qa_valence(sample),
                'preference':  self.func_get_qa_preference(sample),
            }

        elif dataset in ['Preference2', 'Preference4']: # ä¸å¸¦ preference ä¼˜åŒ–
            candidates = {
                'description': self.func_get_qa_description(sample),
                'ovlabel':     self.func_get_qa_ovlabel(sample),
                'sentiment':   self.func_get_qa_sentiment(sample),
                'valence':     self.func_get_qa_valence(sample),
            }
        
        elif dataset in ['Preference3']: # ä¸å¸¦ preference ä¼˜åŒ–
            candidates = {
                'reward': self.func_get_description_reward(sample),
            }
        
        ## case1: Zebang's labels
        elif dataset in ['MERRCoarse', 'MERRFine', 'MAFW']:
            candidates = {
                'description': self.func_get_qa_description(sample),
            }

        ## case2: onehot labels
        elif dataset in ['MER2023', 'MER2024', 'MELD', 'IEMOCAPFour']:
            candidates = {
                'onehot_w_candidates':  self.func_get_qa_onehot_w_candidates(sample),
                'onehot_wo_candidates': self.func_get_qa_onehot_wo_candidates(sample),
            }

        ## case3: valence scores
        elif dataset in ['CMUMOSI', 'CMUMOSEI', 'SIMS', 'SIMSv2']:
            candidates = {
                'valence':   self.func_get_qa_valence(sample),
                'sentiment': self.func_get_qa_sentiment(sample),
            }

        ## case4: instruction dataset
        elif dataset in ['VideoChat', 'LLaVA', 'EmoVIT']:
            candidates = {
                'qa':  self.func_get_qa_direct(sample),
            }

        elif dataset in ['MiniGPT4']:
            candidates = {
                'caption': self.func_get_qa_caption(sample, 'image'),
            }

        elif dataset in ['WavCaps', 'TextrolSpeech', 'PromptSpeech']:
            candidates = {
                'caption': self.func_get_qa_caption(sample, 'audio'),
            }

        return candidates[label_type] # åŒ…å« question, answer ä¸¤éƒ¨åˆ†å†…å®¹


    def get_prompt_for_multimodal(self, face_or_frame, subtitle, user_message):

        # step5: get prompts for differet cases [å¯èƒ½å­˜åœ¨ä¸‰ç§æ•°æ®åŠ è½½æƒ…å†µï¼Œä»è€Œèƒ½å¤Ÿæ‰©å±•è‡³4ç§æ¨¡æ€è¾“å…¥]
        if face_or_frame == 'faceframe': # (face, frame, audio, text)
            assert subtitle is not None
            prompt = f"###Human: The audio content is as follows: <Audio><AudioHere></Audio>. " \
                    + f"Meanwhile, we uniformly sample raw frames from the video: <Video><FrameHere></Video>. "  \
                    + f"Additionally, we uniformly sample raw frames from the video and extract faces from these frames: <Video><FaceHere></Video>. "  \
                    + f"The subtitle of this video is: <Subtitle>{subtitle}</Subtitle>. " \
                    + f"Now, please answer my question based on all the provided information. {user_message} ###Assistant: "
        elif face_or_frame == 'face': # (face, audio, text)
            assert subtitle is not None
            prompt = f"###Human: The audio content is as follows: <Audio><AudioHere></Audio>. " \
                    + f"Meanwhile, we uniformly sample raw frames from the video and extract faces from these frames: <Video><FaceHere></Video>. "  \
                    + f"The subtitle of this video is: <Subtitle>{subtitle}</Subtitle>. " \
                    + f"Now, please answer my question based on all the provided information. {user_message} ###Assistant: "
        elif face_or_frame == 'frame': # (frame, audio, text)
            assert subtitle is not None
            prompt = f"###Human: The audio content is as follows: <Audio><AudioHere></Audio>. " \
                    + f"Meanwhile, we uniformly sample raw frames from the video: <Video><FrameHere></Video>. "  \
                    + f"The subtitle of this video is: <Subtitle>{subtitle}</Subtitle>. " \
                    + f"Now, please answer my question based on all the provided information. {user_message} ###Assistant: "
        elif face_or_frame == 'audioonly': # (audio)
            prompt = f"###Human: The audio content is as follows: <Audio><AudioHere></Audio>. " \
                    + f"Now, please answer my question based on all the provided information. {user_message} ###Assistant: "
        elif face_or_frame == 'textonly':  # (text)
            assert subtitle is not None
            prompt = f"###Human: The subtitle of this video is: <Subtitle>{subtitle}</Subtitle>. " \
                    + f"Now, please answer my question based on all the provided information. {user_message} ###Assistant: "
        elif face_or_frame == 'faceonly':  # (face)
            prompt = f"###Human: We uniformly sample raw frames from the video and extract faces from these frames: <Video><FaceHere></Video>. " \
                    + f"Now, please answer my question based on all the provided information. {user_message} ###Assistant: "
        elif face_or_frame == 'frameonly': # (frame)
            prompt = f"###Human: We uniformly sample raw frames from the video: <Video><FrameHere></Video>. " \
                    + f"Now, please answer my question based on all the provided information. {user_message} ###Assistant: "
        elif face_or_frame == 'image': # (image)
            prompt = f"###Human: The image content is as follows: <Image><ImageHere></Image>. " \
                    + f"Now, please answer my question based on all the provided information. {user_message} ###Assistant: "
        
        ## è¿™éƒ¨åˆ†æ˜¯ä¸ºäº†å’Œå…¶ä»– MLLM è¿›è¡Œå…¬å¹³æ¯”è¾ƒï¼Œæ‰€è¿›è¡Œçš„ ablation study éƒ¨åˆ†
        elif face_or_frame == 'audio_text': # (audio, text)
            assert subtitle is not None
            prompt =  f"The audio content is as follows: <Audio><AudioHere></Audio>. " \
                    + f"The subtitle of this video is: <Subtitle>{subtitle}</Subtitle>. " \
                    + f"Now, please answer my question based on all the provided information. {user_message} ###Assistant: "
        elif face_or_frame == 'face_text': # (audio, text)
            assert subtitle is not None
            prompt =  f"We uniformly sample raw frames from the video and extract faces from these frames: <Video><FaceHere></Video>. "  \
                    + f"The subtitle of this video is: <Subtitle>{subtitle}</Subtitle>. " \
                    + f"Now, please answer my question based on all the provided information. {user_message} ###Assistant: "
        elif face_or_frame == 'frame_text': # (audio, text)
            assert subtitle is not None
            prompt =  f"we uniformly sample raw frames from the video: <Video><FrameHere></Video>. "  \
                    + f"The subtitle of this video is: <Subtitle>{subtitle}</Subtitle>. " \
                    + f"Now, please answer my question based on all the provided information. {user_message} ###Assistant: "
            
        ## åé¢éƒ½æ˜¯å¢åŠ  <Multi> token åçš„ç»“æœ    
        elif face_or_frame == 'multiface_text': # (multi, text)
            assert subtitle is not None
            prompt = f"###Human: The audio and video merged info is: <Multi><MultiHere></Multi>. " \
                    + f"The subtitle of this video is: <Subtitle>{subtitle}</Subtitle>. " \
                    + f"Now, please answer my question based on all the provided information. {user_message} ###Assistant: "
        elif face_or_frame == 'multiface_audio_face_text': # (multi, face, audio, text)
            assert subtitle is not None
            prompt = f"###Human: The audio and video merged info is: <Multi><MultiHere></Multi>. " \
                    + f"The audio content is as follows: <Audio><AudioHere></Audio>. " \
                    + f"Meanwhile, we uniformly sample raw frames from the video and extract faces from these frames: <Video><FaceHere></Video>. "  \
                    + f"The subtitle of this video is: <Subtitle>{subtitle}</Subtitle>. " \
                    + f"Now, please answer my question based on all the provided information. {user_message} ###Assistant: "
        elif face_or_frame == 'multiframe_audio_frame_text': # (multi, face, audio, text)
            assert subtitle is not None
            prompt = f"###Human: The audio and video merged info is: <Multi><MultiHere></Multi>. " \
                    + f"The audio content is as follows: <Audio><AudioHere></Audio>. " \
                    + f"Meanwhile, we uniformly sample raw frames from the video: <Video><FrameHere></Video>. "  \
                    + f"The subtitle of this video is: <Subtitle>{subtitle}</Subtitle>. " \
                    + f"Now, please answer my question based on all the provided information. {user_message} ###Assistant: "
        elif face_or_frame == 'multiface_audio_face_frame_text': # (multi, frame, face, audio, text)
            assert subtitle is not None
            prompt = f"###Human: The audio and video merged info is: <Multi><MultiHere></Multi>. " \
                    + f"The audio content is as follows: <Audio><AudioHere></Audio>. " \
                    + f"Meanwhile, we uniformly sample raw frames from the video and extract faces from these frames: <Video><FaceHere></Video>. "  \
                    + f"Meanwhile, we uniformly sample raw frames from the video: <Video><FrameHere></Video>. "  \
                    + f"The subtitle of this video is: <Subtitle>{subtitle}</Subtitle>. " \
                    + f"Now, please answer my question based on all the provided information. {user_message} ###Assistant: "
        elif face_or_frame == 'multiface_audio_face_frame_au_text': # (multi, frame, face, audio, au, text)
            assert subtitle is not None
            prompt = f"###Human: The audio and video merged info is: <Multi><MultiHere></Multi>. " \
                    + f"The audio content is as follows: <Audio><AudioHere></Audio>. " \
                    + f"Meanwhile, we uniformly sample raw frames from the video and extract faces from these frames: <Video><FaceHere></Video>. "  \
                    + f"Meanwhile, we uniformly sample raw frames from the video: <Video><FrameHere></Video>. "  \
                    + f"The AU (Action Unit) features are: <AU><AUHere></AU>. " \
                    + f"The subtitle of this video is: <Subtitle>{subtitle}</Subtitle>. " \
                    + f"Now, please answer my question based on all the provided information. {user_message} ###Assistant: "
        return prompt
    
    def _load_clip_for_au(self):
        """æ‡’åŠ è½½CLIPæ¨¡å‹ç”¨äºAUå®æ—¶ç¼–ç """
        # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿å±æ€§å­˜åœ¨
        if not hasattr(self, '_clip_model'):
            self._clip_model = None
            self._clip_preprocess = None
        
        if self._clip_model is None:
            try:
                import clip
                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                self._clip_model, self._clip_preprocess = clip.load('ViT-B/32', device=device)
                self._clip_model.eval()
            except Exception as e:
                print(f'[DATASET] æ— æ³•åŠ è½½CLIPæ¨¡å‹: {e}')
                self._clip_model = False  # æ ‡è®°åŠ è½½å¤±è´¥
        return self._clip_model if self._clip_model else None
    
    def _extract_au_features_realtime(self, video_name):
        """å®æ—¶ä»MER-Factoryè¾“å‡ºæå–AUç‰¹å¾å¹¶ç”¨CLIPç¼–ç 
        
        Args:
            video_name: è§†é¢‘åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰
        
        Returns:
            au_features: [N, 512] CLIPç¼–ç çš„AUæè¿°ç‰¹å¾ï¼ŒNä¸ºå¸§æ•°
        """
        if not self.mer_factory_output or not video_name:
            return None
        
        # æ‡’åŠ è½½CLIPæ¨¡å‹
        clip_model = self._load_clip_for_au()
        if clip_model is None:
            return None
        
        try:
            import json
            import clip
            from pathlib import Path
            
            # æ„å»ºJSONæ–‡ä»¶è·¯å¾„
            json_path = Path(self.mer_factory_output) / video_name / f"{video_name}_au_analysis.json"
            
            if not json_path.exists():
                print(f"âš ï¸ AUåˆ†ææ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
                return None
            
            # åŠ è½½JSONæ•°æ®
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            fine_grained_descriptions = data.get('fine_grained_descriptions', {})
            
            if not fine_grained_descriptions:
                print(f"âš ï¸ AU JSONä¸­æ²¡æœ‰fine_grained_descriptions: {video_name}")
                return None
            
            # å‡†å¤‡æ–‡æœ¬åˆ—è¡¨
            frame_indices = sorted(fine_grained_descriptions.keys(), key=int)
            texts = [fine_grained_descriptions[idx] for idx in frame_indices]
            
            # ä½¿ç”¨CLIPç¼–ç 
            device = next(clip_model.parameters()).device
            text_tokens = clip.tokenize(texts, truncate=True).to(device)
            
            with torch.no_grad():
                text_features = clip_model.encode_text(text_tokens)  # [N, 512]
                # å½’ä¸€åŒ–ç‰¹å¾å‘é‡
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                text_features = text_features.float()  # ä¿æŒä¸ºtensor
            
            return text_features
        
        except Exception as e:
            print(f"âŒ AUå®æ—¶æå–å¤±è´¥ ({video_name}): {e}")
            return None
    
    ## æ›¿æ¢ <FaceHere> / <FrameHere> / <AudioHere> / <ImageHere> / <MultiToken>
    def replace_token_for_multimodal(self, prompt):

        replace_token = config.DEFAULT_FRAME_PATCH_TOKEN * self.num_video_query_token
        prompt = prompt.replace(config.DEFAULT_FRAME_PATCH_TOKEN, replace_token)
        replace_token = config.DEFAULT_FACE_PATCH_TOKEN * self.num_video_query_token
        prompt = prompt.replace(config.DEFAULT_FACE_PATCH_TOKEN, replace_token)
        replace_token = config.DEFAULT_AUDIO_PATCH_TOKEN * self.num_audio_query_token
        prompt = prompt.replace(config.DEFAULT_AUDIO_PATCH_TOKEN, replace_token)
        replace_token = config.DEFAULT_MULTI_PATCH_TOKEN * self.num_multi_query_token
        prompt = prompt.replace(config.DEFAULT_MULTI_PATCH_TOKEN, replace_token)
        replace_token = config.DEFAULT_IMAGE_PATCH_TOKEN * self.num_image_query_token
        prompt = prompt.replace(config.DEFAULT_IMAGE_PATCH_TOKEN, replace_token)
        replace_token = config.DEFAULT_AU_PATCH_TOKEN * self.num_au_query_token
        prompt = prompt.replace(config.DEFAULT_AU_PATCH_TOKEN, replace_token)
        return prompt


    ####################################################################################
    ## è¯»å–ä¸€ä¸ªæ ·æœ¬ (read one sample)
    ####################################################################################
    def __getitem__(self, index):
        num_retries = 10 # skip error or too long videos
        for _ in range(num_retries):
            try:
                sample = self.annotation[index]
                cur_label_type = self.get_cur_label_type(self.label_type_candidates, self.label_type)
                # print ('cur_label_type: ', cur_label_type)

                # step1: read needed data
                video_path, image_path, audio_path, face_npy = None, None, None, None
                if hasattr(self, '_get_video_path'): video_path = self._get_video_path(sample)
                if hasattr(self, '_get_image_path'): image_path = self._get_image_path(sample)
                if hasattr(self, '_get_audio_path'): audio_path = self._get_audio_path(sample)
                if hasattr(self, '_get_face_path'):  face_npy   = self._get_face_path(sample)
                # print (video_path, image_path, audio_path, face_npy)
                sample_name = sample.get('name', None)  # è·å–æ ·æœ¬åç§°ç”¨äºé¢„æå–ç‰¹å¾
                sample_data = self.read_frame_face_audio_text(video_path, face_npy, audio_path, image_path, sample_name)

                # step2: read (question, answer)
                # => å¦‚æœ sample ä¸­ç¼ºå°‘ qa å¯¹åº”å†…å®¹çš„ä¿¡æ¯ï¼Œç»“æœæ˜¯ä¼šæŠ¥é”™çš„
                qa_pair = self.get_qa_pairs(self.dataset, cur_label_type, sample)
                # print (qa_pair)

                # step4: generate (text_input, label)
                if 'subtitle' not in sample: sample['subtitle'] = None
                prompt = self.get_prompt_for_multimodal(self.face_or_frame, sample['subtitle'], qa_pair['question']) # get prompt
                prompt = self.replace_token_for_multimodal(prompt) # replace specific tokens
                # print (prompt)

                ## tokenizer [æ¯éƒ¨åˆ†å†…å®¹ä¸èƒ½è¶…è¿‡ self.max_length, ä¸”ä¸¤éƒ¨åˆ†å†…å®¹çš„å’Œä¹Ÿä¸èƒ½è¶…è¿‡ self.max_length]
                prompt_id = self.to_token_ids(prompt, self.max_length) # => é¿å… GPU OOM
                
                target = qa_pair['answer'] + '###'
                # print (target)
                target_id = self.to_token_ids(target, self.max_length)

                text_input = torch.cat([prompt_id, target_id])
                label = torch.cat([torch.ones([len(prompt_id)], dtype=text_input.dtype) * -100, target_id])
                assert len(text_input) == len(label)
                if len(text_input) > self.max_length:
                    raise RuntimeError("too long text_input")
            except Exception as error:
                print(f'Error: {error}')
                print(f"Failed to load data {self.dataset} {sample['name']}. We will randomly sample an example as a replacement.")
                index = random.randint(0, len(self) - 1)
                continue
            break
        else:
            raise RuntimeError(f"Failed to fetch video after {num_retries} retries.")
        # æ„å»ºè¿”å›å­—å…¸ï¼ŒåªåŒ…å«æœ‰æ•ˆçš„æ¨¡æ€
        result = {}
        
        # Faceæ¨¡æ€
        if 'face' in sample_data:
            result["face"] = sample_data['face']           # [c=3, frame=8, 224, 224] [è¿™ä¸ªç»è¿‡äº†transformerå˜æ¢]
            result["raw_face"] = sample_data['raw_face']   # [c=3, frame=8, 224, 224]

        # Frameæ¨¡æ€
        if 'frame' in sample_data:
            result["frame"] = sample_data['frame']         # [c=3, frame=8, 224, 224] [è¿™ä¸ªç»è¿‡äº†transformerå˜æ¢]
            result["raw_frame"] = sample_data['raw_frame'] # [c=3, frame=8, 224, 224]

        # Audioæ¨¡æ€
        if 'audio' in sample_data:
            result["audio"] = sample_data['audio']          # [frame=8, c=1, 128, 204]
            result["raw_audio"] = sample_data['raw_audio']  # [frame=8, c=1, 16000*2é‡‡æ ·ç‚¹]

        # Imageæ¨¡æ€
        if 'image' in sample_data:
            result["image"] = sample_data['image']
            result["raw_image"] = sample_data['raw_image']
            
        # Multiæ¨¡æ€
        result["multi"] = sample_data.get('multi', None)
        result["raw_multi"] = sample_data.get('raw_multi', None)
        
        # AUæ¨¡æ€
        result["au"] = sample_data.get('au', None)
        result["raw_au"] = sample_data.get('raw_au', None)
        
        # å…¶ä»–å¿…è¦å­—æ®µ
        result["label"] = label
        result["text_input"] = text_input
        result['dataset'] = self.dataset.lower()
        result['face_or_frame'] = self.face_or_frame
        
        # ä¼ é€’é¢„æå–æ ‡å¿—
        result['frame_preextracted'] = sample_data.get('frame_preextracted', False)
        result['face_preextracted'] = sample_data.get('face_preextracted', False)
        result['audio_preextracted'] = sample_data.get('audio_preextracted', False)
        result['multi_preextracted'] = sample_data.get('multi_preextracted', False)
        result['au_preextracted'] = sample_data.get('au_preextracted', False)
        
        return result

        
    ####################################################################################
    ## batch çº§åˆ«æ•°æ®åˆå¹¶
    ####################################################################################
    def collater(self, instances):
        '''
        llama token ids:
            <unk>: 0
            bos|<s>: 1
            eos|pad|</s>: 2
            <ImageHere>: 32000
            <AudioHere>: 32001

        data_dict:  input_ids:[###Human: <Image> <ImageHere>*32 /<Image> xxx  ...   ###Assistant: xxx###Human: xxx###Assistant: xxx###]
                    labels:   [-100..., -100, ....,                                 ...           xxx###-100...,        ...     xxx###]

        data_dict:  input_ids:[<bos>###Human: <Image> <ImageHere>*32 /<Image> xxx  ...   ###Assistant: xxx###Human: xxx###Assistant: xxx###, <eos>,    ...]
                    labels:   [-100..., -100, ....,                                 ...                xxx###-100...,        ...     xxx###, -100, ...]
                    images:   [bs=3, c=3, 224, 224]
        '''
        labels = []
        input_ids = []
        for instance in instances:
            label = instance['label']
            input_id = instance['text_input']
            label    = torch.cat([torch.ones([1], dtype=input_id.dtype) * config.IGNORE_INDEX, label,
                                  torch.ones([1], dtype=input_id.dtype) * self.tokenizer.eos_token_id]) # (-100  xxx <eos>)
            input_id = torch.cat([torch.ones([1], dtype=input_id.dtype) * self.tokenizer.bos_token_id, input_id,
                                  torch.ones([1], dtype=input_id.dtype) * self.tokenizer.eos_token_id]) # (<bos> xxx <eos>)
            labels.append(label)
            input_ids.append(input_id)

        # pad bacth input into the same length 
        # => input_ids: <bos> xxx <eos> <pad>
        # => label    : -100  xxx <eos> -100
        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, 
                                                    batch_first=True, 
                                                    padding_value=self.tokenizer.pad_token_id)
        labels    = torch.nn.utils.rnn.pad_sequence(labels,    
                                                    batch_first=True, 
                                                    padding_value=config.IGNORE_INDEX)
        batch = dict(
            labels=labels,
            input_ids=input_ids,
            attention_masks=input_ids.ne(self.tokenizer.pad_token_id),
        )

        # åé¢è·Ÿç€çš„æ˜¯ dataset ä¸­æ‰€æœ‰æ•°æ®ç±»å‹
        # => åªæœ‰ç¬¦åˆçº¦æŸï¼Œæ‰æŠŠè¿™éƒ¨åˆ†æ•°æ®å­˜å‚¨åœ¨ batch é‡Œé¢ï¼Œå¦‚æœæœ‰é—®é¢˜ï¼Œç›´æ¥å°±ä¸å­˜å‚¨
        for sample_type in ['face', 'raw_face', 'frame', 'raw_frame', 'audio', 'raw_audio', 'image', 'raw_image', 'multi', 'raw_multi', 'au', 'raw_au']:
            batch_type = sample_type + 's'

            if sample_type in instances[0]:
                datas = [instance[sample_type] for instance in instances]
                if all(x is not None and x.shape == datas[0].shape for x in datas):
                    batch[batch_type] = torch.stack(datas)
        
        batch['dataset'] = instances[0]['dataset']
        batch['face_or_frame'] = instances[0]['face_or_frame']
        
        # ä¼ é€’é¢„æå–æ ‡å¿—
        batch['frame_preextracted'] = instances[0].get('frame_preextracted', False)
        batch['face_preextracted'] = instances[0].get('face_preextracted', False)
        batch['audio_preextracted'] = instances[0].get('audio_preextracted', False)
        batch['multi_preextracted'] = instances[0].get('multi_preextracted', False)
        batch['au_preextracted'] = instances[0].get('au_preextracted', False)  # æ·»åŠ AUé¢„æå–æ ‡å¿—
        
        return batch
    

class ConcatDataset(ConcatDataset):
    def __init__(self, datasets: Iterable[Dataset]) -> None:
        super().__init__(datasets)

    def collater(self, samples):
        
        all_keys = set()
        for s in samples:
            all_keys.update(s)

        shared_keys = all_keys
        for s in samples:
            shared_keys = shared_keys & set(s.keys())

        samples_shared_keys = []
        for s in samples:
            samples_shared_keys.append({k: s[k] for k in s.keys() if k in shared_keys})

        return self.datasets[0].collater(samples_shared_keys)
